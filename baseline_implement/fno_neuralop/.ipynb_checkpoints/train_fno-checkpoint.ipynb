{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Neural Operators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=10\n",
      "Loading test dataloader at resolution (64, 128) with 50 samples and batch-size=10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from neuralop.models import FNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.datasets import load_spherical_swe\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %%\n",
    "# Loading the Navier-Stokes dataset in 128x128 resolution\n",
    "train_loader, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[(32, 64), (64, 128)], n_tests=[50, 50], test_batch_sizes=[10, 10])\n",
    "\n",
    "model = FNO(n_modes=(32, 32), in_channels=3, out_channels=3, hidden_channels=32, projection_channels=64, factorization='dense').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 4472227 parameters.\n",
      "\n",
      "### MODEL ###\n",
      " FNO(\n",
      "  (fno_blocks): FNOBlocks(\n",
      "    (convs): SpectralConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-3): 4 x ComplexDenseTensor(shape=torch.Size([32, 32, 32, 17]), rank=None)\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-3): 4 x Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (lifting): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (projection): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0008\n",
      "    lr: 0.0008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000002A6B7081220>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.LpLoss object at 0x000002A6B6EA3250>\n",
      "\n",
      " * Test: {'l2': <neuralop.losses.data_losses.LpLoss object at 0x000002A6B6EA3250>}\n",
      "=======Loss: 12.0193815 ======\n",
      "=======Loss: 11.996341 ======\n",
      "=======Loss: 11.980951 ======\n",
      "=======Loss: 11.95977 ======\n",
      "=======Loss: 11.940369 ======\n",
      "=======Loss: 11.8913 ======\n",
      "=======Loss: 11.896233 ======\n",
      "=======Loss: 11.815715 ======\n",
      "=======Loss: 11.780931 ======\n",
      "=======Loss: 11.706423 ======\n",
      "=======Loss: 11.704708 ======\n",
      "=======Loss: 11.584332 ======\n",
      "=======Loss: 11.468179 ======\n",
      "=======Loss: 11.36289 ======\n",
      "=======Loss: 11.237289 ======\n",
      "=======Loss: 11.016521 ======\n",
      "=======Loss: 10.686823 ======\n",
      "=======Loss: 10.443628 ======\n",
      "=======Loss: 10.229388 ======\n",
      "=======Loss: 10.133108 ======\n",
      "=======Loss: 9.776703 ======\n",
      "=======Loss: 9.820781 ======\n",
      "=======Loss: 9.625933 ======\n",
      "=======Loss: 9.543762 ======\n",
      "=======Loss: 9.290232 ======\n",
      "=======Loss: 8.9477215 ======\n",
      "=======Loss: 8.990873 ======\n",
      "=======Loss: 8.787583 ======\n",
      "=======Loss: 8.760733 ======\n",
      "=======Loss: 8.629427 ======\n",
      "=======Loss: 8.388933 ======\n",
      "=======Loss: 8.720783 ======\n",
      "=======Loss: 8.478415 ======\n",
      "=======Loss: 8.364742 ======\n",
      "=======Loss: 8.322588 ======\n",
      "=======Loss: 8.142548 ======\n",
      "=======Loss: 8.179679 ======\n",
      "=======Loss: 8.244717 ======\n",
      "=======Loss: 8.073704 ======\n",
      "=======Loss: 7.952634 ======\n",
      "=======Loss: 8.0119 ======\n",
      "=======Loss: 7.8157573 ======\n",
      "=======Loss: 7.535881 ======\n",
      "=======Loss: 7.810026 ======\n",
      "=======Loss: 7.574976 ======\n",
      "=======Loss: 7.723078 ======\n",
      "=======Loss: 7.210456 ======\n",
      "=======Loss: 7.3492064 ======\n",
      "=======Loss: 7.250039 ======\n",
      "=======Loss: 7.3469224 ======\n",
      "=======Loss: 6.930559 ======\n",
      "=======Loss: 7.114154 ======\n",
      "=======Loss: 6.92752 ======\n",
      "=======Loss: 6.9926105 ======\n",
      "=======Loss: 6.9331055 ======\n",
      "=======Loss: 6.715616 ======\n",
      "=======Loss: 6.5325994 ======\n",
      "=======Loss: 6.2253213 ======\n",
      "=======Loss: 6.4598613 ======\n",
      "=======Loss: 6.3834343 ======\n",
      "=======Loss: 6.446931 ======\n",
      "=======Loss: 6.409748 ======\n",
      "=======Loss: 6.28303 ======\n",
      "=======Loss: 6.1498675 ======\n",
      "=======Loss: 5.9980187 ======\n",
      "=======Loss: 6.086567 ======\n",
      "=======Loss: 5.889039 ======\n",
      "=======Loss: 5.8537807 ======\n",
      "=======Loss: 6.049307 ======\n",
      "=======Loss: 6.0155454 ======\n",
      "=======Loss: 5.701398 ======\n",
      "=======Loss: 5.9523845 ======\n",
      "=======Loss: 6.125395 ======\n",
      "=======Loss: 6.1612687 ======\n",
      "=======Loss: 5.7118015 ======\n",
      "=======Loss: 6.3550763 ======\n",
      "=======Loss: 5.72146 ======\n",
      "=======Loss: 6.3430986 ======\n",
      "=======Loss: 6.1484103 ======\n",
      "=======Loss: 5.934209 ======\n",
      "=======Loss: 6.029248 ======\n",
      "=======Loss: 5.7044706 ======\n",
      "=======Loss: 5.8527517 ======\n",
      "=======Loss: 5.676254 ======\n",
      "=======Loss: 5.6341715 ======\n",
      "=======Loss: 5.99137 ======\n",
      "=======Loss: 5.5049744 ======\n",
      "=======Loss: 5.465933 ======\n",
      "=======Loss: 5.483449 ======\n",
      "=======Loss: 5.77936 ======\n",
      "=======Loss: 5.775454 ======\n",
      "=======Loss: 5.8855495 ======\n",
      "=======Loss: 5.4410276 ======\n",
      "=======Loss: 5.712386 ======\n",
      "=======Loss: 5.4392834 ======\n",
      "=======Loss: 5.5642195 ======\n",
      "=======Loss: 5.690999 ======\n",
      "=======Loss: 5.706892 ======\n",
      "=======Loss: 5.5800242 ======\n",
      "=======Loss: 5.3398046 ======\n",
      "=======Loss: 5.638277 ======\n",
      "=======Loss: 5.639906 ======\n",
      "=======Loss: 5.738108 ======\n",
      "=======Loss: 5.339804 ======\n",
      "=======Loss: 5.3294606 ======\n",
      "=======Loss: 5.617422 ======\n",
      "=======Loss: 5.8143263 ======\n",
      "=======Loss: 5.668555 ======\n",
      "=======Loss: 5.718007 ======\n",
      "=======Loss: 5.6000476 ======\n",
      "=======Loss: 5.574563 ======\n",
      "=======Loss: 5.7450056 ======\n",
      "=======Loss: 6.0107946 ======\n",
      "=======Loss: 5.6638365 ======\n",
      "=======Loss: 5.8216267 ======\n",
      "=======Loss: 5.5575876 ======\n",
      "=======Loss: 5.442645 ======\n",
      "=======Loss: 5.580043 ======\n",
      "=======Loss: 5.541029 ======\n",
      "=======Loss: 5.687394 ======\n",
      "=======Loss: 5.541828 ======\n",
      "=======Loss: 5.4628577 ======\n",
      "=======Loss: 5.5408707 ======\n",
      "=======Loss: 5.3831472 ======\n",
      "=======Loss: 5.5734453 ======\n",
      "=======Loss: 5.2978106 ======\n",
      "=======Loss: 5.748935 ======\n",
      "=======Loss: 5.5227966 ======\n",
      "=======Loss: 5.9481854 ======\n",
      "=======Loss: 5.258292 ======\n",
      "=======Loss: 5.5518584 ======\n",
      "=======Loss: 5.5703053 ======\n",
      "=======Loss: 5.3224854 ======\n",
      "=======Loss: 5.2533145 ======\n",
      "=======Loss: 5.816576 ======\n",
      "=======Loss: 5.669743 ======\n",
      "=======Loss: 5.6700487 ======\n",
      "=======Loss: 5.3541574 ======\n",
      "=======Loss: 5.4527535 ======\n",
      "=======Loss: 5.813541 ======\n",
      "=======Loss: 5.713778 ======\n",
      "=======Loss: 5.7466474 ======\n",
      "=======Loss: 5.55752 ======\n",
      "=======Loss: 5.608166 ======\n",
      "=======Loss: 5.303288 ======\n",
      "=======Loss: 5.725653 ======\n",
      "=======Loss: 5.701831 ======\n",
      "=======Loss: 5.282413 ======\n",
      "=======Loss: 5.5719585 ======\n",
      "=======Loss: 5.4001894 ======\n",
      "=======Loss: 5.430995 ======\n",
      "=======Loss: 5.6081614 ======\n",
      "=======Loss: 5.570634 ======\n",
      "=======Loss: 5.6244726 ======\n",
      "=======Loss: 5.2853928 ======\n",
      "=======Loss: 5.329282 ======\n",
      "=======Loss: 5.3963833 ======\n",
      "=======Loss: 5.448451 ======\n",
      "=======Loss: 5.6785116 ======\n",
      "=======Loss: 5.662979 ======\n",
      "=======Loss: 5.305214 ======\n",
      "=======Loss: 5.555656 ======\n",
      "=======Loss: 5.5102863 ======\n",
      "=======Loss: 5.395076 ======\n",
      "=======Loss: 5.4253197 ======\n",
      "=======Loss: 5.716605 ======\n",
      "=======Loss: 5.6045322 ======\n",
      "=======Loss: 5.404753 ======\n",
      "=======Loss: 5.578763 ======\n",
      "=======Loss: 5.136981 ======\n",
      "=======Loss: 5.600355 ======\n",
      "=======Loss: 5.41452 ======\n",
      "=======Loss: 5.752962 ======\n",
      "=======Loss: 5.33917 ======\n",
      "=======Loss: 5.49284 ======\n",
      "=======Loss: 5.544114 ======\n",
      "=======Loss: 5.416121 ======\n",
      "=======Loss: 5.2877655 ======\n",
      "=======Loss: 5.600515 ======\n",
      "=======Loss: 5.4400225 ======\n",
      "=======Loss: 5.225601 ======\n",
      "=======Loss: 5.4557266 ======\n",
      "=======Loss: 5.6132073 ======\n",
      "=======Loss: 5.2470264 ======\n",
      "=======Loss: 5.3275833 ======\n",
      "=======Loss: 5.674742 ======\n",
      "=======Loss: 5.624915 ======\n",
      "=======Loss: 5.446911 ======\n",
      "=======Loss: 5.2915173 ======\n",
      "=======Loss: 5.4520855 ======\n",
      "=======Loss: 5.535698 ======\n",
      "=======Loss: 5.499407 ======\n",
      "=======Loss: 5.330349 ======\n",
      "=======Loss: 5.5716143 ======\n",
      "=======Loss: 5.475461 ======\n",
      "=======Loss: 5.6573253 ======\n",
      "=======Loss: 5.4867744 ======\n",
      "=======Loss: 5.5146685 ======\n",
      "=======Loss: 5.390277 ======\n",
      "=======Loss: 5.6023073 ======\n",
      "=======Loss: 5.4089947 ======\n",
      "=======Loss: 5.3778057 ======\n",
      "=======Loss: 5.5035143 ======\n",
      "=======Loss: 5.258804 ======\n",
      "=======Loss: 5.447006 ======\n",
      "=======Loss: 5.448102 ======\n",
      "=======Loss: 5.298804 ======\n",
      "=======Loss: 5.329792 ======\n",
      "=======Loss: 5.4315586 ======\n",
      "=======Loss: 5.3312488 ======\n",
      "=======Loss: 5.6433 ======\n",
      "=======Loss: 5.5694466 ======\n",
      "=======Loss: 5.370945 ======\n",
      "=======Loss: 5.442659 ======\n",
      "=======Loss: 5.1390657 ======\n",
      "=======Loss: 5.5459647 ======\n",
      "=======Loss: 5.506115 ======\n",
      "=======Loss: 5.160638 ======\n",
      "=======Loss: 5.428944 ======\n",
      "=======Loss: 5.628275 ======\n",
      "=======Loss: 5.599516 ======\n",
      "=======Loss: 5.2593355 ======\n",
      "=======Loss: 5.246997 ======\n",
      "=======Loss: 5.460514 ======\n",
      "=======Loss: 5.278542 ======\n",
      "=======Loss: 5.712196 ======\n",
      "=======Loss: 5.5721097 ======\n",
      "=======Loss: 5.331136 ======\n",
      "=======Loss: 5.6817474 ======\n",
      "=======Loss: 5.3069773 ======\n",
      "=======Loss: 5.45929 ======\n",
      "=======Loss: 5.4578767 ======\n",
      "=======Loss: 5.4735937 ======\n",
      "=======Loss: 5.665929 ======\n",
      "=======Loss: 5.0719843 ======\n",
      "=======Loss: 5.359024 ======\n",
      "=======Loss: 5.403403 ======\n",
      "=======Loss: 5.30304 ======\n",
      "=======Loss: 5.6071663 ======\n",
      "=======Loss: 5.6380053 ======\n",
      "=======Loss: 5.298219 ======\n",
      "=======Loss: 5.49844 ======\n",
      "=======Loss: 5.1754866 ======\n",
      "=======Loss: 5.192511 ======\n",
      "=======Loss: 5.2114816 ======\n",
      "=======Loss: 5.5591493 ======\n",
      "=======Loss: 5.417897 ======\n",
      "=======Loss: 5.3212996 ======\n",
      "=======Loss: 5.455817 ======\n",
      "=======Loss: 5.221817 ======\n",
      "=======Loss: 5.5260997 ======\n",
      "=======Loss: 5.708371 ======\n",
      "=======Loss: 5.2752924 ======\n",
      "=======Loss: 5.5255976 ======\n",
      "=======Loss: 5.4935427 ======\n",
      "=======Loss: 5.327866 ======\n",
      "=======Loss: 5.3265667 ======\n",
      "=======Loss: 5.3371654 ======\n",
      "=======Loss: 5.3138847 ======\n",
      "=======Loss: 5.191207 ======\n",
      "=======Loss: 5.631953 ======\n",
      "=======Loss: 5.360152 ======\n",
      "=======Loss: 5.750256 ======\n",
      "=======Loss: 5.2670994 ======\n",
      "=======Loss: 5.343337 ======\n",
      "=======Loss: 5.344546 ======\n",
      "=======Loss: 5.411175 ======\n",
      "=======Loss: 5.3294954 ======\n",
      "=======Loss: 5.104509 ======\n",
      "=======Loss: 5.328401 ======\n",
      "=======Loss: 5.1874757 ======\n",
      "=======Loss: 5.0625577 ======\n",
      "=======Loss: 5.029196 ======\n",
      "=======Loss: 5.1575284 ======\n",
      "=======Loss: 5.097005 ======\n",
      "=======Loss: 4.9458866 ======\n",
      "=======Loss: 5.2400236 ======\n",
      "=======Loss: 5.39873 ======\n",
      "=======Loss: 5.0785165 ======\n",
      "=======Loss: 5.328218 ======\n",
      "=======Loss: 5.0105486 ======\n",
      "=======Loss: 4.9638405 ======\n",
      "=======Loss: 5.2907414 ======\n",
      "=======Loss: 4.7935452 ======\n",
      "=======Loss: 4.6319313 ======\n",
      "=======Loss: 4.6569896 ======\n",
      "=======Loss: 4.9455028 ======\n",
      "=======Loss: 5.5343466 ======\n",
      "=======Loss: 4.6588936 ======\n",
      "=======Loss: 5.001023 ======\n",
      "=======Loss: 4.7740126 ======\n",
      "=======Loss: 4.8690085 ======\n",
      "=======Loss: 4.9496794 ======\n",
      "=======Loss: 5.4726467 ======\n",
      "=======Loss: 4.72584 ======\n",
      "=======Loss: 4.889534 ======\n",
      "=======Loss: 4.8987355 ======\n",
      "=======Loss: 5.437727 ======\n",
      "=======Loss: 5.44197 ======\n",
      "=======Loss: 5.772448 ======\n",
      "=======Loss: 4.777645 ======\n",
      "=======Loss: 4.5934467 ======\n",
      "=======Loss: 5.0877447 ======\n",
      "=======Loss: 4.7689056 ======\n",
      "=======Loss: 4.989134 ======\n",
      "=======Loss: 4.625262 ======\n",
      "=======Loss: 4.3988414 ======\n",
      "=======Loss: 4.519952 ======\n",
      "=======Loss: 4.863603 ======\n",
      "=======Loss: 4.276692 ======\n",
      "=======Loss: 4.4254665 ======\n",
      "=======Loss: 5.0366592 ======\n",
      "=======Loss: 4.600762 ======\n",
      "=======Loss: 4.377784 ======\n",
      "=======Loss: 4.9666624 ======\n",
      "=======Loss: 5.0311155 ======\n",
      "=======Loss: 4.7746754 ======\n",
      "=======Loss: 4.4312906 ======\n",
      "=======Loss: 5.4621744 ======\n",
      "=======Loss: 4.6581736 ======\n",
      "=======Loss: 5.1398654 ======\n",
      "=======Loss: 4.233838 ======\n",
      "=======Loss: 4.4122305 ======\n",
      "=======Loss: 4.754546 ======\n",
      "=======Loss: 4.3171425 ======\n",
      "=======Loss: 4.71861 ======\n",
      "=======Loss: 4.692216 ======\n",
      "=======Loss: 4.502808 ======\n",
      "=======Loss: 4.0871544 ======\n",
      "=======Loss: 4.2342997 ======\n",
      "=======Loss: 4.611121 ======\n",
      "=======Loss: 4.355024 ======\n",
      "=======Loss: 4.7038236 ======\n",
      "=======Loss: 4.969247 ======\n",
      "=======Loss: 4.342558 ======\n",
      "=======Loss: 4.601537 ======\n",
      "=======Loss: 4.5769815 ======\n",
      "=======Loss: 4.4640813 ======\n",
      "=======Loss: 4.4108233 ======\n",
      "=======Loss: 3.9904346 ======\n",
      "=======Loss: 4.352171 ======\n",
      "=======Loss: 4.358243 ======\n",
      "=======Loss: 3.9107363 ======\n",
      "=======Loss: 4.170436 ======\n",
      "=======Loss: 3.9861393 ======\n",
      "=======Loss: 4.1182427 ======\n",
      "=======Loss: 4.0646853 ======\n",
      "=======Loss: 4.431606 ======\n",
      "=======Loss: 4.740699 ======\n",
      "=======Loss: 3.8769455 ======\n",
      "=======Loss: 3.8810196 ======\n",
      "=======Loss: 4.2058744 ======\n",
      "=======Loss: 3.8932352 ======\n",
      "=======Loss: 4.018729 ======\n",
      "=======Loss: 4.144248 ======\n",
      "=======Loss: 3.8587525 ======\n",
      "=======Loss: 4.016354 ======\n",
      "=======Loss: 4.183686 ======\n",
      "=======Loss: 4.225524 ======\n",
      "=======Loss: 4.353647 ======\n",
      "=======Loss: 3.718021 ======\n",
      "=======Loss: 4.101276 ======\n",
      "=======Loss: 4.068472 ======\n",
      "=======Loss: 4.112861 ======\n",
      "=======Loss: 4.4666057 ======\n",
      "=======Loss: 3.9962845 ======\n",
      "=======Loss: 3.961332 ======\n",
      "=======Loss: 3.8939223 ======\n",
      "=======Loss: 3.5671732 ======\n",
      "=======Loss: 4.250974 ======\n",
      "=======Loss: 3.77344 ======\n",
      "=======Loss: 3.7643037 ======\n",
      "=======Loss: 4.110584 ======\n",
      "=======Loss: 4.014469 ======\n",
      "=======Loss: 3.7320385 ======\n",
      "=======Loss: 3.784038 ======\n",
      "=======Loss: 4.201744 ======\n",
      "=======Loss: 3.3953152 ======\n",
      "=======Loss: 3.938968 ======\n",
      "=======Loss: 3.8222802 ======\n",
      "=======Loss: 3.742365 ======\n",
      "=======Loss: 3.5844026 ======\n",
      "=======Loss: 3.848186 ======\n",
      "=======Loss: 3.9796886 ======\n",
      "=======Loss: 3.5628633 ======\n",
      "=======Loss: 3.6591678 ======\n",
      "=======Loss: 3.7246003 ======\n",
      "=======Loss: 4.0530186 ======\n",
      "=======Loss: 3.6203914 ======\n",
      "=======Loss: 3.6912942 ======\n",
      "=======Loss: 3.7719696 ======\n",
      "=======Loss: 3.7560606 ======\n",
      "=======Loss: 3.475603 ======\n",
      "=======Loss: 3.5914145 ======\n",
      "=======Loss: 3.5059223 ======\n",
      "=======Loss: 3.4241152 ======\n",
      "=======Loss: 3.7139053 ======\n",
      "=======Loss: 4.015707 ======\n",
      "=======Loss: 3.7567186 ======\n",
      "=======Loss: 3.7564812 ======\n",
      "=======Loss: 3.5800757 ======\n",
      "=======Loss: 3.7358894 ======\n",
      "=======Loss: 3.6884532 ======\n",
      "=======Loss: 3.4456456 ======\n",
      "=======Loss: 3.76908 ======\n",
      "=======Loss: 3.5638444 ======\n",
      "=======Loss: 3.6683593 ======\n",
      "=======Loss: 3.5015974 ======\n",
      "=======Loss: 3.731256 ======\n",
      "=======Loss: 3.2625065 ======\n",
      "=======Loss: 3.6148021 ======\n",
      "=======Loss: 3.7691283 ======\n",
      "=======Loss: 3.5521493 ======\n",
      "=======Loss: 3.438675 ======\n",
      "=======Loss: 3.6023786 ======\n",
      "=======Loss: 3.2814233 ======\n",
      "=======Loss: 3.387372 ======\n",
      "=======Loss: 3.6804478 ======\n",
      "=======Loss: 3.3140454 ======\n",
      "=======Loss: 3.652821 ======\n",
      "=======Loss: 3.5862248 ======\n",
      "=======Loss: 3.0038242 ======\n",
      "=======Loss: 3.3810706 ======\n",
      "=======Loss: 3.3616872 ======\n",
      "=======Loss: 3.6265967 ======\n",
      "=======Loss: 3.2064633 ======\n",
      "=======Loss: 3.2995539 ======\n",
      "=======Loss: 3.4381747 ======\n",
      "=======Loss: 3.3313498 ======\n",
      "=======Loss: 3.3536918 ======\n",
      "=======Loss: 3.4540973 ======\n",
      "=======Loss: 3.432559 ======\n",
      "=======Loss: 3.2634766 ======\n",
      "=======Loss: 3.436586 ======\n",
      "=======Loss: 3.0659013 ======\n",
      "=======Loss: 3.4675417 ======\n",
      "=======Loss: 3.5077338 ======\n",
      "=======Loss: 3.6050394 ======\n",
      "=======Loss: 3.3246617 ======\n",
      "=======Loss: 3.4783099 ======\n",
      "=======Loss: 3.9194717 ======\n",
      "=======Loss: 3.5112922 ======\n",
      "=======Loss: 3.4941099 ======\n",
      "=======Loss: 3.1371398 ======\n",
      "=======Loss: 3.2346358 ======\n",
      "=======Loss: 3.360907 ======\n",
      "=======Loss: 3.579791 ======\n",
      "=======Loss: 3.3079858 ======\n",
      "=======Loss: 3.0497632 ======\n",
      "=======Loss: 3.206061 ======\n",
      "=======Loss: 3.0452676 ======\n",
      "=======Loss: 3.5422735 ======\n",
      "=======Loss: 3.4828358 ======\n",
      "=======Loss: 3.360671 ======\n",
      "=======Loss: 3.1696181 ======\n",
      "=======Loss: 3.385352 ======\n",
      "=======Loss: 3.6957922 ======\n",
      "=======Loss: 3.3855877 ======\n",
      "=======Loss: 3.371317 ======\n",
      "=======Loss: 3.35841 ======\n",
      "=======Loss: 3.436998 ======\n",
      "=======Loss: 3.0867505 ======\n",
      "=======Loss: 3.2766576 ======\n",
      "=======Loss: 3.4826257 ======\n",
      "=======Loss: 3.2051113 ======\n",
      "=======Loss: 3.1439722 ======\n",
      "=======Loss: 3.1598332 ======\n",
      "=======Loss: 3.531036 ======\n",
      "=======Loss: 3.253529 ======\n",
      "=======Loss: 3.0311422 ======\n",
      "=======Loss: 3.4727087 ======\n",
      "=======Loss: 3.6539383 ======\n",
      "=======Loss: 3.2317367 ======\n",
      "=======Loss: 3.3199039 ======\n",
      "=======Loss: 3.2348294 ======\n",
      "=======Loss: 3.1218543 ======\n",
      "=======Loss: 2.8586001 ======\n",
      "=======Loss: 3.025616 ======\n",
      "=======Loss: 3.201492 ======\n",
      "=======Loss: 3.1210945 ======\n",
      "=======Loss: 3.0179925 ======\n",
      "=======Loss: 3.2628965 ======\n",
      "=======Loss: 3.101984 ======\n",
      "=======Loss: 3.5530493 ======\n",
      "=======Loss: 3.0555568 ======\n",
      "=======Loss: 3.0089061 ======\n",
      "=======Loss: 3.373748 ======\n",
      "=======Loss: 2.886254 ======\n",
      "=======Loss: 2.8829308 ======\n",
      "=======Loss: 3.1885605 ======\n",
      "=======Loss: 3.04678 ======\n",
      "=======Loss: 2.9972732 ======\n",
      "=======Loss: 3.0905302 ======\n",
      "=======Loss: 3.2200394 ======\n",
      "=======Loss: 2.9620113 ======\n",
      "=======Loss: 3.300239 ======\n",
      "=======Loss: 3.3081527 ======\n",
      "=======Loss: 3.1101737 ======\n",
      "=======Loss: 3.2510448 ======\n",
      "=======Loss: 3.3131912 ======\n",
      "=======Loss: 2.8487935 ======\n",
      "=======Loss: 3.2475548 ======\n",
      "=======Loss: 3.1745455 ======\n",
      "=======Loss: 3.0838733 ======\n",
      "=======Loss: 3.0677547 ======\n",
      "=======Loss: 3.1468844 ======\n",
      "=======Loss: 3.0772433 ======\n",
      "=======Loss: 3.5131302 ======\n",
      "=======Loss: 3.0101328 ======\n",
      "=======Loss: 3.368956 ======\n",
      "=======Loss: 3.1389194 ======\n",
      "=======Loss: 2.9846745 ======\n",
      "=======Loss: 2.9419656 ======\n",
      "=======Loss: 3.2462392 ======\n",
      "=======Loss: 3.0612042 ======\n",
      "=======Loss: 2.9887285 ======\n",
      "=======Loss: 3.0562887 ======\n",
      "=======Loss: 2.9709167 ======\n",
      "=======Loss: 3.242564 ======\n",
      "=======Loss: 3.1560187 ======\n",
      "=======Loss: 3.2590027 ======\n",
      "=======Loss: 3.1523457 ======\n",
      "=======Loss: 3.0403712 ======\n",
      "=======Loss: 2.9085243 ======\n",
      "=======Loss: 2.7968056 ======\n",
      "=======Loss: 2.935177 ======\n",
      "=======Loss: 2.8318877 ======\n",
      "=======Loss: 2.9299836 ======\n",
      "=======Loss: 2.955744 ======\n",
      "=======Loss: 2.958944 ======\n",
      "=======Loss: 2.7228663 ======\n",
      "=======Loss: 2.93714 ======\n",
      "=======Loss: 2.809266 ======\n",
      "=======Loss: 2.9281473 ======\n",
      "=======Loss: 3.022387 ======\n",
      "=======Loss: 2.812757 ======\n",
      "=======Loss: 2.9195902 ======\n",
      "=======Loss: 2.9298806 ======\n",
      "=======Loss: 3.1370416 ======\n",
      "=======Loss: 2.879864 ======\n",
      "=======Loss: 2.962994 ======\n",
      "=======Loss: 2.7042627 ======\n",
      "=======Loss: 3.1637602 ======\n",
      "=======Loss: 2.9983761 ======\n",
      "=======Loss: 2.9152374 ======\n",
      "=======Loss: 3.0229094 ======\n",
      "=======Loss: 2.71705 ======\n",
      "=======Loss: 2.95596 ======\n",
      "=======Loss: 2.7313795 ======\n",
      "=======Loss: 2.827476 ======\n",
      "=======Loss: 3.0781398 ======\n",
      "=======Loss: 2.7610383 ======\n",
      "=======Loss: 2.700027 ======\n",
      "=======Loss: 2.9896662 ======\n",
      "=======Loss: 2.831524 ======\n",
      "=======Loss: 2.8811994 ======\n",
      "=======Loss: 2.7001648 ======\n",
      "=======Loss: 2.7509704 ======\n",
      "=======Loss: 2.8480623 ======\n",
      "=======Loss: 2.9941669 ======\n",
      "=======Loss: 2.79883 ======\n",
      "=======Loss: 2.666327 ======\n",
      "=======Loss: 2.8815238 ======\n",
      "=======Loss: 2.9437118 ======\n",
      "=======Loss: 3.2076068 ======\n",
      "=======Loss: 2.596293 ======\n",
      "=======Loss: 2.7114458 ======\n",
      "=======Loss: 2.937952 ======\n",
      "=======Loss: 2.3025725 ======\n",
      "=======Loss: 2.7438116 ======\n",
      "=======Loss: 2.892308 ======\n",
      "=======Loss: 2.643578 ======\n",
      "=======Loss: 2.7565887 ======\n",
      "=======Loss: 2.6814282 ======\n",
      "=======Loss: 2.8809757 ======\n",
      "=======Loss: 2.957934 ======\n",
      "=======Loss: 2.6120377 ======\n",
      "=======Loss: 2.751843 ======\n",
      "=======Loss: 2.7984881 ======\n",
      "=======Loss: 2.6918814 ======\n",
      "=======Loss: 2.9980056 ======\n",
      "=======Loss: 2.6397943 ======\n",
      "=======Loss: 2.596758 ======\n",
      "=======Loss: 2.5024335 ======\n",
      "=======Loss: 2.67491 ======\n",
      "=======Loss: 2.8309066 ======\n",
      "=======Loss: 2.4594352 ======\n",
      "=======Loss: 2.6699154 ======\n",
      "=======Loss: 2.73989 ======\n",
      "=======Loss: 3.0186005 ======\n",
      "=======Loss: 2.719188 ======\n",
      "=======Loss: 2.733302 ======\n",
      "=======Loss: 3.0436158 ======\n",
      "=======Loss: 2.616373 ======\n",
      "=======Loss: 2.8559747 ======\n",
      "=======Loss: 2.9317546 ======\n",
      "=======Loss: 2.6560757 ======\n",
      "=======Loss: 2.7563837 ======\n",
      "=======Loss: 2.8756185 ======\n",
      "=======Loss: 2.7096694 ======\n",
      "=======Loss: 2.808238 ======\n",
      "=======Loss: 2.6314108 ======\n",
      "=======Loss: 3.10595 ======\n",
      "=======Loss: 3.3232727 ======\n",
      "=======Loss: 2.7294078 ======\n",
      "=======Loss: 2.7218902 ======\n",
      "=======Loss: 2.7660384 ======\n",
      "=======Loss: 2.7237127 ======\n",
      "=======Loss: 2.567909 ======\n",
      "=======Loss: 2.5558105 ======\n",
      "=======Loss: 2.75352 ======\n",
      "=======Loss: 2.860188 ======\n",
      "=======Loss: 2.4942095 ======\n",
      "=======Loss: 3.050504 ======\n",
      "=======Loss: 2.728996 ======\n",
      "=======Loss: 2.6649623 ======\n",
      "=======Loss: 2.6703408 ======\n",
      "=======Loss: 2.5732446 ======\n",
      "=======Loss: 2.5787845 ======\n",
      "=======Loss: 2.8502164 ======\n",
      "=======Loss: 2.6436787 ======\n",
      "=======Loss: 2.844458 ======\n",
      "=======Loss: 2.4132023 ======\n",
      "=======Loss: 2.6068506 ======\n",
      "=======Loss: 2.5394888 ======\n",
      "=======Loss: 2.909618 ======\n",
      "=======Loss: 2.577486 ======\n",
      "=======Loss: 2.7436733 ======\n",
      "=======Loss: 2.6908853 ======\n",
      "=======Loss: 2.76235 ======\n",
      "=======Loss: 2.5923977 ======\n",
      "=======Loss: 2.4103537 ======\n",
      "=======Loss: 2.5699382 ======\n",
      "=======Loss: 2.3838553 ======\n",
      "=======Loss: 2.5320733 ======\n",
      "=======Loss: 2.700887 ======\n",
      "=======Loss: 2.5038435 ======\n",
      "=======Loss: 2.4135861 ======\n",
      "=======Loss: 2.5250278 ======\n",
      "=======Loss: 2.711937 ======\n",
      "=======Loss: 2.925907 ======\n",
      "=======Loss: 2.5981596 ======\n",
      "=======Loss: 2.5031502 ======\n",
      "=======Loss: 2.5342157 ======\n",
      "=======Loss: 2.996903 ======\n",
      "=======Loss: 2.415157 ======\n",
      "=======Loss: 2.4390721 ======\n",
      "=======Loss: 2.6636252 ======\n",
      "=======Loss: 2.825539 ======\n",
      "=======Loss: 2.7390263 ======\n",
      "=======Loss: 3.1565287 ======\n",
      "=======Loss: 2.786387 ======\n",
      "=======Loss: 2.6226206 ======\n",
      "=======Loss: 2.8411074 ======\n",
      "=======Loss: 2.7981448 ======\n",
      "=======Loss: 2.5438693 ======\n",
      "=======Loss: 2.4861531 ======\n",
      "=======Loss: 2.3473363 ======\n",
      "=======Loss: 2.8320327 ======\n",
      "=======Loss: 2.3733325 ======\n",
      "=======Loss: 2.7100072 ======\n",
      "=======Loss: 2.4973981 ======\n",
      "=======Loss: 2.3458114 ======\n",
      "=======Loss: 2.4729264 ======\n",
      "=======Loss: 2.753789 ======\n",
      "=======Loss: 2.5494504 ======\n",
      "=======Loss: 2.4077115 ======\n",
      "=======Loss: 2.594202 ======\n",
      "=======Loss: 2.3871145 ======\n",
      "=======Loss: 2.5318544 ======\n",
      "=======Loss: 2.440761 ======\n",
      "=======Loss: 2.550273 ======\n",
      "=======Loss: 2.484508 ======\n",
      "=======Loss: 2.5844045 ======\n",
      "=======Loss: 2.4037704 ======\n",
      "=======Loss: 2.4130292 ======\n",
      "=======Loss: 2.650788 ======\n",
      "=======Loss: 2.5146217 ======\n",
      "=======Loss: 2.599176 ======\n",
      "=======Loss: 2.361396 ======\n",
      "=======Loss: 2.4080307 ======\n",
      "=======Loss: 2.5411518 ======\n",
      "=======Loss: 2.8737674 ======\n",
      "=======Loss: 2.453459 ======\n",
      "=======Loss: 2.3004122 ======\n",
      "=======Loss: 2.5889611 ======\n",
      "=======Loss: 2.5525224 ======\n",
      "=======Loss: 2.380259 ======\n",
      "=======Loss: 2.5551455 ======\n",
      "=======Loss: 2.8440485 ======\n",
      "=======Loss: 2.5307732 ======\n",
      "=======Loss: 2.3780653 ======\n",
      "=======Loss: 2.440916 ======\n",
      "=======Loss: 2.6586628 ======\n",
      "=======Loss: 2.4835613 ======\n",
      "=======Loss: 2.4779932 ======\n",
      "=======Loss: 2.533969 ======\n",
      "=======Loss: 2.447259 ======\n",
      "=======Loss: 2.6318095 ======\n",
      "=======Loss: 2.4834652 ======\n",
      "=======Loss: 2.2209926 ======\n",
      "=======Loss: 2.7847555 ======\n",
      "=======Loss: 2.664215 ======\n",
      "=======Loss: 2.3365068 ======\n",
      "=======Loss: 2.7722294 ======\n",
      "=======Loss: 2.4896312 ======\n",
      "=======Loss: 2.3498092 ======\n",
      "=======Loss: 2.5370517 ======\n",
      "=======Loss: 2.5058093 ======\n",
      "=======Loss: 2.6836667 ======\n",
      "=======Loss: 2.3010404 ======\n",
      "=======Loss: 2.323686 ======\n",
      "=======Loss: 2.4554405 ======\n",
      "=======Loss: 2.5107014 ======\n",
      "=======Loss: 2.5314307 ======\n",
      "=======Loss: 2.5575364 ======\n",
      "=======Loss: 2.2759204 ======\n",
      "=======Loss: 2.357428 ======\n",
      "=======Loss: 2.6875167 ======\n",
      "=======Loss: 2.6805239 ======\n",
      "=======Loss: 2.4269896 ======\n",
      "=======Loss: 2.6173053 ======\n",
      "=======Loss: 2.5152984 ======\n",
      "=======Loss: 2.2718077 ======\n",
      "=======Loss: 2.381062 ======\n",
      "=======Loss: 2.5533516 ======\n",
      "=======Loss: 2.762765 ======\n",
      "=======Loss: 2.2473454 ======\n",
      "=======Loss: 2.5651815 ======\n",
      "=======Loss: 2.4677885 ======\n",
      "=======Loss: 2.6193588 ======\n",
      "=======Loss: 2.2019448 ======\n",
      "=======Loss: 2.5232916 ======\n",
      "=======Loss: 2.7437124 ======\n",
      "=======Loss: 2.3116674 ======\n",
      "=======Loss: 2.3307443 ======\n",
      "=======Loss: 2.4201837 ======\n",
      "=======Loss: 2.4035957 ======\n",
      "=======Loss: 2.290966 ======\n",
      "=======Loss: 2.3226733 ======\n",
      "=======Loss: 2.2641273 ======\n",
      "=======Loss: 2.3460317 ======\n",
      "=======Loss: 2.268587 ======\n",
      "=======Loss: 2.3345933 ======\n",
      "=======Loss: 2.2322543 ======\n",
      "=======Loss: 2.3160808 ======\n",
      "=======Loss: 2.5976872 ======\n",
      "=======Loss: 2.4675388 ======\n",
      "=======Loss: 2.3271744 ======\n",
      "=======Loss: 2.445538 ======\n",
      "=======Loss: 2.687252 ======\n",
      "=======Loss: 2.7291255 ======\n",
      "=======Loss: 2.3437176 ======\n",
      "=======Loss: 2.2811015 ======\n",
      "=======Loss: 2.4703832 ======\n",
      "=======Loss: 2.6509883 ======\n",
      "=======Loss: 2.675314 ======\n",
      "=======Loss: 2.3046675 ======\n",
      "=======Loss: 2.4107635 ======\n",
      "=======Loss: 2.2524614 ======\n",
      "=======Loss: 2.311745 ======\n",
      "=======Loss: 2.3384042 ======\n",
      "=======Loss: 2.3928103 ======\n",
      "=======Loss: 2.313862 ======\n",
      "=======Loss: 2.5052342 ======\n",
      "=======Loss: 2.4658875 ======\n",
      "=======Loss: 2.4145408 ======\n",
      "=======Loss: 2.4729578 ======\n",
      "=======Loss: 2.1142583 ======\n",
      "=======Loss: 2.4385722 ======\n",
      "=======Loss: 2.3421516 ======\n",
      "=======Loss: 2.1382635 ======\n",
      "=======Loss: 2.5551405 ======\n",
      "=======Loss: 2.1774654 ======\n",
      "=======Loss: 2.0758312 ======\n",
      "=======Loss: 2.4943428 ======\n",
      "=======Loss: 2.1453962 ======\n",
      "=======Loss: 2.3187485 ======\n",
      "=======Loss: 2.292454 ======\n",
      "=======Loss: 2.1210864 ======\n",
      "=======Loss: 2.3403764 ======\n",
      "=======Loss: 2.446612 ======\n",
      "=======Loss: 2.311795 ======\n",
      "=======Loss: 2.3099773 ======\n",
      "=======Loss: 2.5737185 ======\n",
      "=======Loss: 2.3905697 ======\n",
      "=======Loss: 2.2281482 ======\n",
      "=======Loss: 2.5361686 ======\n",
      "=======Loss: 2.3692617 ======\n",
      "=======Loss: 2.2803087 ======\n",
      "=======Loss: 2.2293823 ======\n",
      "=======Loss: 2.3129764 ======\n",
      "=======Loss: 2.5135634 ======\n",
      "=======Loss: 2.4049726 ======\n",
      "=======Loss: 2.2373586 ======\n",
      "=======Loss: 2.493309 ======\n",
      "=======Loss: 2.2066207 ======\n",
      "=======Loss: 1.907547 ======\n",
      "=======Loss: 2.1443326 ======\n",
      "=======Loss: 2.3458054 ======\n",
      "=======Loss: 2.609702 ======\n",
      "=======Loss: 2.3494413 ======\n",
      "=======Loss: 2.4205391 ======\n",
      "=======Loss: 2.1948457 ======\n",
      "=======Loss: 2.2582245 ======\n",
      "=======Loss: 2.3000753 ======\n",
      "=======Loss: 2.3051019 ======\n",
      "=======Loss: 2.2565417 ======\n",
      "=======Loss: 1.9043114 ======\n",
      "=======Loss: 2.193824 ======\n",
      "=======Loss: 2.2851944 ======\n",
      "=======Loss: 2.198193 ======\n",
      "=======Loss: 2.5084512 ======\n",
      "=======Loss: 2.1039236 ======\n",
      "=======Loss: 2.094389 ======\n",
      "=======Loss: 2.2328134 ======\n",
      "=======Loss: 2.5341675 ======\n",
      "=======Loss: 2.093556 ======\n",
      "=======Loss: 2.300265 ======\n",
      "=======Loss: 2.2821877 ======\n",
      "=======Loss: 2.2627935 ======\n",
      "=======Loss: 2.1469312 ======\n",
      "=======Loss: 2.4847698 ======\n",
      "=======Loss: 2.0756702 ======\n",
      "=======Loss: 2.323625 ======\n",
      "=======Loss: 2.2531862 ======\n",
      "=======Loss: 2.4875016 ======\n",
      "=======Loss: 2.1788726 ======\n",
      "=======Loss: 2.3748584 ======\n",
      "=======Loss: 2.3799162 ======\n",
      "=======Loss: 2.254036 ======\n",
      "=======Loss: 2.3509269 ======\n",
      "=======Loss: 2.0299 ======\n",
      "=======Loss: 2.1916065 ======\n",
      "=======Loss: 2.4648125 ======\n",
      "=======Loss: 2.2742095 ======\n",
      "=======Loss: 2.1794353 ======\n",
      "=======Loss: 2.6047726 ======\n",
      "=======Loss: 2.5952835 ======\n",
      "=======Loss: 2.0907884 ======\n",
      "=======Loss: 2.3571339 ======\n",
      "=======Loss: 1.9998783 ======\n",
      "=======Loss: 2.1979177 ======\n",
      "=======Loss: 2.3348908 ======\n",
      "=======Loss: 2.4147768 ======\n",
      "=======Loss: 2.3304482 ======\n",
      "=======Loss: 2.1911883 ======\n",
      "=======Loss: 2.1908922 ======\n",
      "=======Loss: 2.1113505 ======\n",
      "=======Loss: 2.2330613 ======\n",
      "=======Loss: 2.498951 ======\n",
      "=======Loss: 2.0665224 ======\n",
      "=======Loss: 2.0504596 ======\n",
      "=======Loss: 2.487369 ======\n",
      "=======Loss: 2.0519714 ======\n",
      "=======Loss: 2.4024782 ======\n",
      "=======Loss: 2.3347085 ======\n",
      "=======Loss: 1.937698 ======\n",
      "=======Loss: 2.5961356 ======\n",
      "=======Loss: 2.259755 ======\n",
      "=======Loss: 2.2647417 ======\n",
      "=======Loss: 2.46978 ======\n",
      "=======Loss: 2.083771 ======\n",
      "=======Loss: 2.1987846 ======\n",
      "=======Loss: 1.9888976 ======\n",
      "=======Loss: 2.2533684 ======\n",
      "=======Loss: 2.2945113 ======\n",
      "=======Loss: 2.4684868 ======\n",
      "=======Loss: 2.2133625 ======\n",
      "=======Loss: 2.4518142 ======\n",
      "=======Loss: 2.4721103 ======\n",
      "=======Loss: 2.2754204 ======\n",
      "=======Loss: 2.2100086 ======\n",
      "=======Loss: 2.3224058 ======\n",
      "=======Loss: 1.9214671 ======\n",
      "=======Loss: 2.077002 ======\n",
      "=======Loss: 2.29598 ======\n",
      "=======Loss: 2.4077296 ======\n",
      "=======Loss: 1.9749634 ======\n",
      "=======Loss: 2.0389028 ======\n",
      "=======Loss: 2.03189 ======\n",
      "=======Loss: 2.082708 ======\n",
      "=======Loss: 2.3507888 ======\n",
      "=======Loss: 2.460868 ======\n",
      "=======Loss: 2.1455412 ======\n",
      "=======Loss: 2.32581 ======\n",
      "=======Loss: 1.8987992 ======\n",
      "=======Loss: 2.2621841 ======\n",
      "=======Loss: 2.4249673 ======\n",
      "=======Loss: 1.9874129 ======\n",
      "=======Loss: 2.2793913 ======\n",
      "=======Loss: 2.0169582 ======\n",
      "=======Loss: 2.332726 ======\n",
      "=======Loss: 2.0225842 ======\n",
      "=======Loss: 2.0514474 ======\n",
      "=======Loss: 2.2863421 ======\n",
      "=======Loss: 2.2262716 ======\n",
      "=======Loss: 2.2669082 ======\n",
      "=======Loss: 2.0410626 ======\n",
      "=======Loss: 2.2565122 ======\n",
      "=======Loss: 2.0246863 ======\n",
      "=======Loss: 1.9374021 ======\n",
      "=======Loss: 2.1355734 ======\n",
      "=======Loss: 2.119285 ======\n",
      "=======Loss: 2.0295546 ======\n",
      "=======Loss: 1.8699003 ======\n",
      "=======Loss: 2.138213 ======\n",
      "=======Loss: 2.254262 ======\n",
      "=======Loss: 1.9853079 ======\n",
      "=======Loss: 1.9589322 ======\n",
      "=======Loss: 2.2510896 ======\n",
      "=======Loss: 1.8874749 ======\n",
      "=======Loss: 2.0756843 ======\n",
      "=======Loss: 1.9835548 ======\n",
      "=======Loss: 1.9669268 ======\n",
      "=======Loss: 2.3472383 ======\n",
      "=======Loss: 2.1166725 ======\n",
      "=======Loss: 2.1757336 ======\n",
      "=======Loss: 2.2052934 ======\n",
      "=======Loss: 1.9808484 ======\n",
      "=======Loss: 2.1882753 ======\n",
      "=======Loss: 2.0054069 ======\n",
      "=======Loss: 2.1628795 ======\n",
      "=======Loss: 2.0439184 ======\n",
      "=======Loss: 2.052483 ======\n",
      "=======Loss: 1.825927 ======\n",
      "=======Loss: 2.2423081 ======\n",
      "=======Loss: 2.3696222 ======\n",
      "=======Loss: 2.0462372 ======\n",
      "=======Loss: 2.102451 ======\n",
      "=======Loss: 2.0717125 ======\n",
      "=======Loss: 2.2022905 ======\n",
      "=======Loss: 2.3011253 ======\n",
      "=======Loss: 2.1230001 ======\n",
      "=======Loss: 1.9279368 ======\n",
      "=======Loss: 2.1097078 ======\n",
      "=======Loss: 2.0794575 ======\n",
      "=======Loss: 1.9969678 ======\n",
      "=======Loss: 1.9968188 ======\n",
      "=======Loss: 2.3297129 ======\n",
      "=======Loss: 2.1156673 ======\n",
      "=======Loss: 2.066363 ======\n",
      "=======Loss: 1.9047339 ======\n",
      "=======Loss: 2.0726585 ======\n",
      "=======Loss: 2.1944475 ======\n",
      "=======Loss: 2.0249932 ======\n",
      "=======Loss: 1.9099429 ======\n",
      "=======Loss: 2.0742764 ======\n",
      "=======Loss: 2.12578 ======\n",
      "=======Loss: 1.855315 ======\n",
      "=======Loss: 2.1516035 ======\n",
      "=======Loss: 1.9271971 ======\n",
      "=======Loss: 2.386045 ======\n",
      "=======Loss: 2.354867 ======\n",
      "=======Loss: 1.9435209 ======\n",
      "=======Loss: 2.3252184 ======\n",
      "=======Loss: 1.9665083 ======\n",
      "=======Loss: 1.91596 ======\n",
      "=======Loss: 2.1279697 ======\n",
      "=======Loss: 1.9591174 ======\n",
      "=======Loss: 1.9254584 ======\n",
      "=======Loss: 1.8634708 ======\n",
      "=======Loss: 1.8845168 ======\n",
      "=======Loss: 2.026493 ======\n",
      "=======Loss: 1.9413701 ======\n",
      "=======Loss: 1.9664018 ======\n",
      "=======Loss: 2.0743186 ======\n",
      "=======Loss: 1.9452088 ======\n",
      "=======Loss: 2.3610473 ======\n",
      "=======Loss: 1.8482189 ======\n",
      "=======Loss: 1.8822793 ======\n",
      "=======Loss: 2.1986082 ======\n",
      "=======Loss: 2.002792 ======\n",
      "=======Loss: 2.2662613 ======\n",
      "=======Loss: 2.1979327 ======\n",
      "=======Loss: 2.0050116 ======\n",
      "=======Loss: 2.110463 ======\n",
      "=======Loss: 2.080562 ======\n",
      "=======Loss: 1.8973626 ======\n",
      "=======Loss: 1.8878204 ======\n",
      "=======Loss: 1.9694823 ======\n",
      "=======Loss: 1.8657564 ======\n",
      "=======Loss: 1.9748521 ======\n",
      "=======Loss: 2.2253468 ======\n",
      "=======Loss: 2.1534214 ======\n",
      "=======Loss: 2.0260954 ======\n",
      "=======Loss: 2.0532398 ======\n",
      "=======Loss: 1.9166862 ======\n",
      "=======Loss: 1.9962375 ======\n",
      "=======Loss: 2.1929107 ======\n",
      "=======Loss: 1.9727583 ======\n",
      "=======Loss: 2.007029 ======\n",
      "=======Loss: 2.0636241 ======\n",
      "=======Loss: 1.7846186 ======\n",
      "=======Loss: 1.9890577 ======\n",
      "=======Loss: 2.3026588 ======\n",
      "=======Loss: 1.9496999 ======\n",
      "=======Loss: 1.995712 ======\n",
      "=======Loss: 2.174858 ======\n",
      "=======Loss: 2.1454403 ======\n",
      "=======Loss: 1.8592595 ======\n",
      "=======Loss: 2.2399037 ======\n",
      "=======Loss: 2.3388443 ======\n",
      "=======Loss: 1.7900447 ======\n",
      "=======Loss: 2.0721974 ======\n",
      "=======Loss: 2.0335484 ======\n",
      "=======Loss: 2.0327642 ======\n",
      "=======Loss: 2.0919552 ======\n",
      "=======Loss: 1.9768915 ======\n",
      "=======Loss: 1.9728034 ======\n",
      "=======Loss: 1.8974842 ======\n",
      "=======Loss: 1.8590289 ======\n",
      "=======Loss: 2.0533047 ======\n",
      "=======Loss: 2.009278 ======\n",
      "=======Loss: 1.878001 ======\n",
      "=======Loss: 1.9021761 ======\n",
      "=======Loss: 2.0156918 ======\n",
      "=======Loss: 1.8879005 ======\n",
      "=======Loss: 1.9551239 ======\n",
      "=======Loss: 1.9335753 ======\n",
      "=======Loss: 1.8714539 ======\n",
      "=======Loss: 2.0063758 ======\n",
      "=======Loss: 1.9687164 ======\n",
      "=======Loss: 1.9772265 ======\n",
      "=======Loss: 2.2310662 ======\n",
      "=======Loss: 2.0880146 ======\n",
      "=======Loss: 2.073744 ======\n",
      "=======Loss: 2.0005703 ======\n",
      "=======Loss: 2.1626205 ======\n",
      "=======Loss: 1.9840364 ======\n",
      "=======Loss: 2.0049906 ======\n",
      "=======Loss: 1.9810969 ======\n",
      "=======Loss: 2.0127969 ======\n",
      "=======Loss: 1.8414949 ======\n",
      "=======Loss: 1.8584012 ======\n",
      "=======Loss: 2.0769691 ======\n",
      "=======Loss: 1.9807886 ======\n",
      "=======Loss: 2.059863 ======\n",
      "=======Loss: 2.0204346 ======\n",
      "=======Loss: 1.8146856 ======\n",
      "=======Loss: 1.9330394 ======\n",
      "=======Loss: 1.8985674 ======\n",
      "=======Loss: 2.1311896 ======\n",
      "=======Loss: 1.9728378 ======\n",
      "=======Loss: 1.9890444 ======\n",
      "=======Loss: 1.9015977 ======\n",
      "=======Loss: 2.1147237 ======\n",
      "=======Loss: 1.9469876 ======\n",
      "=======Loss: 2.103132 ======\n",
      "=======Loss: 1.9171665 ======\n",
      "=======Loss: 2.050913 ======\n",
      "=======Loss: 1.9172072 ======\n",
      "=======Loss: 1.855616 ======\n",
      "=======Loss: 2.0639663 ======\n",
      "=======Loss: 1.8799806 ======\n",
      "=======Loss: 1.9686875 ======\n",
      "=======Loss: 1.9466219 ======\n",
      "=======Loss: 1.937802 ======\n",
      "=======Loss: 1.9797645 ======\n",
      "=======Loss: 1.8111676 ======\n",
      "=======Loss: 1.9631248 ======\n",
      "=======Loss: 1.9088082 ======\n",
      "=======Loss: 1.8699255 ======\n",
      "=======Loss: 1.9122915 ======\n",
      "=======Loss: 2.0318422 ======\n",
      "=======Loss: 1.8954057 ======\n",
      "=======Loss: 2.006146 ======\n",
      "=======Loss: 2.0652375 ======\n",
      "=======Loss: 2.0689511 ======\n",
      "=======Loss: 1.8649426 ======\n",
      "=======Loss: 1.8018374 ======\n",
      "=======Loss: 1.8879735 ======\n",
      "=======Loss: 1.774853 ======\n",
      "=======Loss: 1.9546508 ======\n",
      "=======Loss: 1.7202474 ======\n",
      "=======Loss: 2.0681553 ======\n",
      "=======Loss: 1.8229725 ======\n",
      "=======Loss: 1.8107986 ======\n",
      "=======Loss: 1.955166 ======\n",
      "=======Loss: 1.9214997 ======\n",
      "=======Loss: 1.9858859 ======\n",
      "=======Loss: 2.1966298 ======\n",
      "=======Loss: 2.1627493 ======\n",
      "=======Loss: 1.9525392 ======\n",
      "=======Loss: 2.0258896 ======\n",
      "=======Loss: 2.1541355 ======\n",
      "=======Loss: 1.8747052 ======\n",
      "=======Loss: 1.9542407 ======\n",
      "=======Loss: 1.8816195 ======\n",
      "=======Loss: 1.8568279 ======\n",
      "=======Loss: 2.1074743 ======\n",
      "=======Loss: 1.9536107 ======\n",
      "=======Loss: 2.066846 ======\n",
      "=======Loss: 1.9216021 ======\n",
      "=======Loss: 1.9972157 ======\n",
      "=======Loss: 1.8537202 ======\n",
      "=======Loss: 2.03958 ======\n",
      "=======Loss: 2.0529585 ======\n",
      "=======Loss: 2.200278 ======\n",
      "=======Loss: 2.0138338 ======\n",
      "=======Loss: 1.9120008 ======\n",
      "=======Loss: 1.8139815 ======\n",
      "=======Loss: 1.8388281 ======\n",
      "=======Loss: 1.959175 ======\n",
      "=======Loss: 1.792328 ======\n",
      "=======Loss: 2.0240216 ======\n",
      "=======Loss: 1.8092909 ======\n",
      "=======Loss: 1.872926 ======\n",
      "=======Loss: 1.738004 ======\n",
      "=======Loss: 1.9600365 ======\n",
      "=======Loss: 1.8143411 ======\n",
      "=======Loss: 1.9729025 ======\n",
      "=======Loss: 1.9293344 ======\n",
      "=======Loss: 2.1465535 ======\n",
      "=======Loss: 1.9387078 ======\n",
      "=======Loss: 1.9696207 ======\n",
      "=======Loss: 1.9035292 ======\n",
      "=======Loss: 2.188296 ======\n",
      "=======Loss: 1.8563509 ======\n",
      "=======Loss: 1.9149196 ======\n",
      "=======Loss: 1.8408475 ======\n",
      "=======Loss: 1.8218739 ======\n",
      "=======Loss: 1.8293635 ======\n",
      "=======Loss: 1.7713702 ======\n",
      "=======Loss: 1.7567611 ======\n",
      "=======Loss: 1.8443359 ======\n",
      "=======Loss: 1.8991603 ======\n",
      "=======Loss: 1.792986 ======\n",
      "=======Loss: 2.1361578 ======\n",
      "=======Loss: 2.12416 ======\n",
      "=======Loss: 1.6759152 ======\n",
      "=======Loss: 1.9568785 ======\n",
      "=======Loss: 1.7549479 ======\n",
      "=======Loss: 1.863529 ======\n",
      "=======Loss: 1.8245039 ======\n",
      "=======Loss: 2.233686 ======\n",
      "=======Loss: 1.964565 ======\n",
      "=======Loss: 1.9303372 ======\n",
      "=======Loss: 1.8366578 ======\n",
      "=======Loss: 1.8466251 ======\n",
      "=======Loss: 2.0572672 ======\n",
      "=======Loss: 1.9794952 ======\n",
      "=======Loss: 1.7728757 ======\n",
      "=======Loss: 1.8908838 ======\n",
      "=======Loss: 1.8580111 ======\n",
      "=======Loss: 1.9525142 ======\n",
      "=======Loss: 2.0457158 ======\n",
      "=======Loss: 1.9742167 ======\n",
      "=======Loss: 1.9114012 ======\n",
      "=======Loss: 1.9885719 ======\n",
      "=======Loss: 1.9673196 ======\n",
      "=======Loss: 2.0241637 ======\n",
      "=======Loss: 1.8864365 ======\n",
      "=======Loss: 1.9098858 ======\n",
      "=======Loss: 1.9669712 ======\n",
      "=======Loss: 1.7057011 ======\n",
      "=======Loss: 1.850584 ======\n",
      "=======Loss: 1.9242736 ======\n",
      "=======Loss: 2.0471094 ======\n",
      "=======Loss: 1.7482786 ======\n",
      "=======Loss: 1.9453764 ======\n",
      "=======Loss: 1.7899377 ======\n",
      "=======Loss: 1.7104712 ======\n",
      "=======Loss: 1.9227514 ======\n",
      "=======Loss: 1.7335365 ======\n",
      "=======Loss: 2.2049265 ======\n",
      "=======Loss: 2.017114 ======\n",
      "=======Loss: 2.040763 ======\n",
      "=======Loss: 1.7964242 ======\n",
      "=======Loss: 1.8609158 ======\n",
      "=======Loss: 2.0094628 ======\n",
      "=======Loss: 1.6914535 ======\n",
      "=======Loss: 1.7208102 ======\n",
      "=======Loss: 2.012698 ======\n",
      "=======Loss: 1.9817436 ======\n",
      "=======Loss: 1.7915254 ======\n",
      "=======Loss: 1.9382253 ======\n",
      "=======Loss: 1.937889 ======\n",
      "=======Loss: 1.8104018 ======\n",
      "=======Loss: 1.8905483 ======\n",
      "=======Loss: 1.8924706 ======\n",
      "=======Loss: 2.0977135 ======\n",
      "=======Loss: 1.8712897 ======\n",
      "=======Loss: 1.8629366 ======\n",
      "=======Loss: 1.8047321 ======\n",
      "=======Loss: 2.09416 ======\n",
      "=======Loss: 1.7106922 ======\n",
      "=======Loss: 1.8859096 ======\n",
      "=======Loss: 2.0435295 ======\n",
      "=======Loss: 1.895637 ======\n",
      "=======Loss: 1.9330764 ======\n",
      "=======Loss: 1.8156018 ======\n",
      "=======Loss: 1.8685231 ======\n",
      "=======Loss: 1.8067284 ======\n",
      "=======Loss: 1.8905671 ======\n",
      "=======Loss: 1.873837 ======\n",
      "=======Loss: 1.8964944 ======\n",
      "=======Loss: 1.8069913 ======\n",
      "=======Loss: 1.8361888 ======\n",
      "=======Loss: 1.8681518 ======\n",
      "=======Loss: 2.0356822 ======\n",
      "=======Loss: 1.7536956 ======\n",
      "=======Loss: 1.7625138 ======\n",
      "=======Loss: 1.9066043 ======\n",
      "=======Loss: 1.8738818 ======\n",
      "=======Loss: 1.8054581 ======\n",
      "=======Loss: 1.9545465 ======\n",
      "=======Loss: 1.8596812 ======\n",
      "=======Loss: 2.0797307 ======\n",
      "=======Loss: 1.8664134 ======\n",
      "=======Loss: 1.8363667 ======\n",
      "=======Loss: 1.9399974 ======\n",
      "=======Loss: 1.6909186 ======\n",
      "=======Loss: 1.752299 ======\n",
      "=======Loss: 1.9643717 ======\n",
      "=======Loss: 1.9175521 ======\n",
      "=======Loss: 1.8877716 ======\n",
      "=======Loss: 1.7429795 ======\n",
      "=======Loss: 1.8637794 ======\n",
      "=======Loss: 1.713729 ======\n",
      "=======Loss: 1.8537526 ======\n",
      "=======Loss: 1.7753311 ======\n",
      "=======Loss: 1.8859379 ======\n",
      "=======Loss: 2.0444956 ======\n",
      "=======Loss: 2.0515716 ======\n",
      "=======Loss: 1.8802576 ======\n",
      "=======Loss: 1.9581306 ======\n",
      "=======Loss: 2.2723408 ======\n",
      "=======Loss: 1.7418042 ======\n",
      "=======Loss: 2.0256948 ======\n",
      "=======Loss: 1.8215984 ======\n",
      "=======Loss: 1.9648471 ======\n",
      "=======Loss: 1.9612269 ======\n",
      "=======Loss: 1.8729606 ======\n",
      "=======Loss: 2.1996558 ======\n",
      "=======Loss: 2.0286112 ======\n",
      "=======Loss: 1.9849563 ======\n",
      "=======Loss: 1.8533618 ======\n",
      "=======Loss: 2.2115316 ======\n",
      "=======Loss: 2.2493052 ======\n",
      "=======Loss: 1.8900918 ======\n",
      "=======Loss: 1.8375599 ======\n",
      "=======Loss: 1.7284802 ======\n",
      "=======Loss: 1.8733313 ======\n",
      "=======Loss: 1.8140227 ======\n",
      "=======Loss: 1.9500389 ======\n",
      "=======Loss: 1.8114562 ======\n",
      "=======Loss: 1.7753266 ======\n",
      "=======Loss: 1.856103 ======\n",
      "=======Loss: 1.9061289 ======\n",
      "=======Loss: 2.0341718 ======\n",
      "=======Loss: 1.9495093 ======\n",
      "=======Loss: 1.9624312 ======\n",
      "=======Loss: 1.9039905 ======\n",
      "=======Loss: 1.7915924 ======\n",
      "=======Loss: 1.7870812 ======\n",
      "=======Loss: 1.7018933 ======\n",
      "=======Loss: 1.7531519 ======\n",
      "=======Loss: 1.9025605 ======\n",
      "=======Loss: 1.8351066 ======\n",
      "=======Loss: 1.857479 ======\n",
      "=======Loss: 1.8448323 ======\n",
      "=======Loss: 1.7400706 ======\n",
      "=======Loss: 1.8174499 ======\n",
      "=======Loss: 1.9530067 ======\n",
      "=======Loss: 1.9363713 ======\n",
      "=======Loss: 1.7293828 ======\n",
      "=======Loss: 1.7550297 ======\n",
      "=======Loss: 1.758146 ======\n",
      "=======Loss: 1.7460067 ======\n",
      "=======Loss: 1.7474043 ======\n",
      "=======Loss: 1.8287346 ======\n",
      "=======Loss: 1.8656273 ======\n",
      "=======Loss: 1.8514485 ======\n",
      "=======Loss: 1.8573275 ======\n",
      "=======Loss: 1.7805686 ======\n",
      "=======Loss: 2.191378 ======\n",
      "=======Loss: 1.7335658 ======\n",
      "=======Loss: 1.9954958 ======\n",
      "=======Loss: 1.791863 ======\n",
      "=======Loss: 1.7366761 ======\n",
      "=======Loss: 1.6925309 ======\n",
      "=======Loss: 1.8465561 ======\n",
      "=======Loss: 1.8328545 ======\n",
      "=======Loss: 1.7800272 ======\n",
      "=======Loss: 1.9210088 ======\n",
      "=======Loss: 1.8606559 ======\n",
      "=======Loss: 1.9253762 ======\n",
      "=======Loss: 2.0226207 ======\n",
      "=======Loss: 1.6715779 ======\n",
      "=======Loss: 1.8570108 ======\n",
      "=======Loss: 2.0017447 ======\n",
      "=======Loss: 2.029316 ======\n",
      "=======Loss: 1.835875 ======\n",
      "=======Loss: 1.9282606 ======\n",
      "=======Loss: 2.0138938 ======\n",
      "=======Loss: 2.1083539 ======\n",
      "=======Loss: 1.8400359 ======\n",
      "=======Loss: 2.0141358 ======\n",
      "=======Loss: 1.7550862 ======\n",
      "=======Loss: 1.8948926 ======\n",
      "=======Loss: 1.9160371 ======\n",
      "=======Loss: 1.8853364 ======\n",
      "=======Loss: 1.8723435 ======\n",
      "=======Loss: 1.6389691 ======\n",
      "=======Loss: 1.9190841 ======\n",
      "=======Loss: 1.8441223 ======\n",
      "=======Loss: 1.793041 ======\n",
      "=======Loss: 1.7527366 ======\n",
      "=======Loss: 1.7436062 ======\n",
      "=======Loss: 1.8282721 ======\n",
      "=======Loss: 2.0417793 ======\n",
      "=======Loss: 1.9450264 ======\n",
      "=======Loss: 1.7260889 ======\n",
      "=======Loss: 2.043334 ======\n",
      "=======Loss: 1.894577 ======\n",
      "=======Loss: 1.9202317 ======\n",
      "=======Loss: 2.1137908 ======\n",
      "=======Loss: 1.8210003 ======\n",
      "=======Loss: 1.7457615 ======\n",
      "=======Loss: 1.7837129 ======\n",
      "=======Loss: 1.8190589 ======\n",
      "=======Loss: 1.6925982 ======\n",
      "=======Loss: 2.0165892 ======\n",
      "=======Loss: 2.0660465 ======\n",
      "=======Loss: 1.9098258 ======\n",
      "=======Loss: 1.8502231 ======\n",
      "=======Loss: 1.7709365 ======\n",
      "=======Loss: 1.8129587 ======\n",
      "=======Loss: 1.9415541 ======\n",
      "=======Loss: 1.7349346 ======\n",
      "=======Loss: 1.7392519 ======\n",
      "=======Loss: 1.8159637 ======\n",
      "=======Loss: 1.7031753 ======\n",
      "=======Loss: 2.197773 ======\n",
      "=======Loss: 1.7182155 ======\n",
      "=======Loss: 1.8023586 ======\n",
      "=======Loss: 1.8694477 ======\n",
      "=======Loss: 1.7325183 ======\n",
      "=======Loss: 1.9635471 ======\n",
      "=======Loss: 1.7124279 ======\n",
      "=======Loss: 2.039998 ======\n",
      "=======Loss: 1.752447 ======\n",
      "=======Loss: 1.8494518 ======\n",
      "=======Loss: 1.7235079 ======\n",
      "=======Loss: 2.013867 ======\n",
      "=======Loss: 1.9564468 ======\n",
      "=======Loss: 1.9428148 ======\n",
      "=======Loss: 1.7204559 ======\n",
      "=======Loss: 1.8525144 ======\n",
      "=======Loss: 1.7717292 ======\n",
      "=======Loss: 1.8071916 ======\n",
      "=======Loss: 1.9351294 ======\n",
      "=======Loss: 1.76194 ======\n",
      "=======Loss: 1.6544213 ======\n",
      "=======Loss: 1.7987888 ======\n",
      "=======Loss: 2.1014113 ======\n",
      "=======Loss: 1.7009363 ======\n",
      "=======Loss: 1.9097679 ======\n",
      "=======Loss: 1.9553161 ======\n",
      "=======Loss: 1.7648104 ======\n",
      "=======Loss: 1.798016 ======\n",
      "=======Loss: 2.1669028 ======\n",
      "=======Loss: 1.8687578 ======\n",
      "=======Loss: 1.7632287 ======\n",
      "=======Loss: 1.6124191 ======\n",
      "=======Loss: 1.7549949 ======\n",
      "=======Loss: 1.9876542 ======\n",
      "=======Loss: 1.9804362 ======\n",
      "=======Loss: 1.8715751 ======\n",
      "=======Loss: 1.7932222 ======\n",
      "=======Loss: 1.6739792 ======\n",
      "=======Loss: 1.9327002 ======\n",
      "=======Loss: 1.8406117 ======\n",
      "=======Loss: 1.8614721 ======\n",
      "=======Loss: 1.9360642 ======\n",
      "=======Loss: 1.6901444 ======\n",
      "=======Loss: 1.8045235 ======\n",
      "=======Loss: 1.720233 ======\n",
      "=======Loss: 1.811654 ======\n",
      "=======Loss: 1.8917915 ======\n",
      "=======Loss: 1.951652 ======\n",
      "=======Loss: 1.8106806 ======\n",
      "=======Loss: 1.8240829 ======\n",
      "=======Loss: 2.0076349 ======\n",
      "=======Loss: 1.819167 ======\n",
      "=======Loss: 1.7475917 ======\n",
      "=======Loss: 1.8818932 ======\n",
      "=======Loss: 1.7770298 ======\n",
      "=======Loss: 1.7271185 ======\n",
      "=======Loss: 1.8742578 ======\n",
      "=======Loss: 1.8839308 ======\n",
      "=======Loss: 1.9352775 ======\n",
      "=======Loss: 1.7634319 ======\n",
      "=======Loss: 1.803215 ======\n",
      "=======Loss: 1.7781847 ======\n",
      "=======Loss: 2.0279274 ======\n",
      "=======Loss: 1.7169065 ======\n",
      "=======Loss: 1.8074341 ======\n",
      "=======Loss: 2.106246 ======\n",
      "=======Loss: 1.7368562 ======\n",
      "=======Loss: 1.6622313 ======\n",
      "=======Loss: 1.9474614 ======\n",
      "=======Loss: 1.7466462 ======\n",
      "=======Loss: 1.7964058 ======\n",
      "=======Loss: 1.8672473 ======\n",
      "=======Loss: 1.8402326 ======\n",
      "=======Loss: 1.678893 ======\n",
      "=======Loss: 1.7421985 ======\n",
      "=======Loss: 1.8392197 ======\n",
      "=======Loss: 1.8802247 ======\n",
      "=======Loss: 1.8178859 ======\n",
      "=======Loss: 2.0860634 ======\n",
      "=======Loss: 1.7328967 ======\n",
      "=======Loss: 1.8968501 ======\n",
      "=======Loss: 1.6535385 ======\n",
      "=======Loss: 1.6961253 ======\n",
      "=======Loss: 2.0176365 ======\n",
      "=======Loss: 1.6758909 ======\n",
      "=======Loss: 1.8151389 ======\n",
      "=======Loss: 1.8315399 ======\n",
      "=======Loss: 1.8993027 ======\n",
      "=======Loss: 1.823176 ======\n",
      "=======Loss: 1.7908633 ======\n",
      "=======Loss: 1.7225331 ======\n",
      "=======Loss: 1.6857736 ======\n",
      "=======Loss: 1.9441922 ======\n",
      "=======Loss: 1.615104 ======\n",
      "=======Loss: 1.8053802 ======\n",
      "=======Loss: 1.6196523 ======\n",
      "=======Loss: 1.8081331 ======\n",
      "=======Loss: 1.814715 ======\n",
      "=======Loss: 1.9466742 ======\n",
      "=======Loss: 1.6860878 ======\n",
      "=======Loss: 2.0316436 ======\n",
      "=======Loss: 1.6161127 ======\n",
      "=======Loss: 1.738839 ======\n",
      "=======Loss: 1.893703 ======\n",
      "=======Loss: 1.8129872 ======\n",
      "=======Loss: 1.9466034 ======\n",
      "=======Loss: 2.0101027 ======\n",
      "=======Loss: 1.9341245 ======\n",
      "=======Loss: 1.9531223 ======\n",
      "=======Loss: 1.9997689 ======\n",
      "=======Loss: 1.8270669 ======\n",
      "=======Loss: 1.7874091 ======\n",
      "=======Loss: 1.7718856 ======\n",
      "=======Loss: 1.8389372 ======\n",
      "=======Loss: 1.7339895 ======\n",
      "=======Loss: 1.8766692 ======\n",
      "=======Loss: 1.9674555 ======\n",
      "=======Loss: 1.9196699 ======\n",
      "=======Loss: 1.725344 ======\n",
      "=======Loss: 1.7486305 ======\n",
      "=======Loss: 1.858683 ======\n",
      "=======Loss: 1.9287276 ======\n",
      "=======Loss: 1.7628669 ======\n",
      "=======Loss: 1.7566152 ======\n",
      "=======Loss: 1.6519439 ======\n",
      "=======Loss: 1.6693761 ======\n",
      "=======Loss: 1.8153028 ======\n",
      "=======Loss: 1.8908393 ======\n",
      "=======Loss: 1.940233 ======\n",
      "=======Loss: 1.9839174 ======\n",
      "=======Loss: 1.9553584 ======\n",
      "=======Loss: 2.1013608 ======\n",
      "=======Loss: 1.8195134 ======\n",
      "=======Loss: 1.891166 ======\n",
      "=======Loss: 1.6525486 ======\n",
      "=======Loss: 1.8435582 ======\n",
      "=======Loss: 1.8688437 ======\n",
      "=======Loss: 1.894011 ======\n",
      "=======Loss: 1.8611242 ======\n",
      "=======Loss: 1.6910441 ======\n",
      "=======Loss: 1.825633 ======\n",
      "=======Loss: 1.9512622 ======\n",
      "=======Loss: 1.8827567 ======\n",
      "=======Loss: 1.7775072 ======\n",
      "=======Loss: 1.7842427 ======\n",
      "=======Loss: 1.7694905 ======\n",
      "=======Loss: 1.6857779 ======\n",
      "=======Loss: 1.8995488 ======\n",
      "=======Loss: 1.7481107 ======\n",
      "=======Loss: 1.9464164 ======\n",
      "=======Loss: 1.8520732 ======\n",
      "=======Loss: 1.7288356 ======\n",
      "=======Loss: 1.9946967 ======\n",
      "=======Loss: 1.6414719 ======\n",
      "=======Loss: 1.7877593 ======\n",
      "=======Loss: 2.0466938 ======\n",
      "=======Loss: 1.8997657 ======\n",
      "=======Loss: 1.9599121 ======\n",
      "=======Loss: 1.9266863 ======\n",
      "=======Loss: 1.7563417 ======\n",
      "=======Loss: 1.96381 ======\n",
      "=======Loss: 1.8532691 ======\n",
      "=======Loss: 1.6481085 ======\n",
      "=======Loss: 1.8594967 ======\n",
      "=======Loss: 1.9078866 ======\n",
      "=======Loss: 1.6828109 ======\n",
      "=======Loss: 1.851102 ======\n",
      "=======Loss: 1.7439295 ======\n",
      "=======Loss: 1.8435212 ======\n",
      "=======Loss: 1.8862169 ======\n",
      "=======Loss: 1.9310606 ======\n",
      "=======Loss: 1.8880651 ======\n",
      "=======Loss: 1.9736146 ======\n",
      "=======Loss: 1.8026471 ======\n",
      "=======Loss: 1.7590353 ======\n",
      "=======Loss: 1.9163289 ======\n",
      "=======Loss: 1.9859791 ======\n",
      "=======Loss: 2.1654959 ======\n",
      "=======Loss: 1.822576 ======\n",
      "=======Loss: 2.2307394 ======\n",
      "=======Loss: 1.7843076 ======\n",
      "=======Loss: 1.8853912 ======\n",
      "=======Loss: 1.6855154 ======\n",
      "=======Loss: 1.5007975 ======\n",
      "=======Loss: 1.7673008 ======\n",
      "=======Loss: 2.089914 ======\n",
      "=======Loss: 1.9035296 ======\n",
      "=======Loss: 1.8754318 ======\n",
      "=======Loss: 2.0919352 ======\n",
      "=======Loss: 1.7887322 ======\n",
      "=======Loss: 1.617826 ======\n",
      "=======Loss: 1.7937539 ======\n",
      "=======Loss: 1.8053393 ======\n",
      "=======Loss: 1.695715 ======\n",
      "=======Loss: 1.704149 ======\n",
      "=======Loss: 1.6712573 ======\n",
      "=======Loss: 1.6870159 ======\n",
      "=======Loss: 1.9960505 ======\n",
      "=======Loss: 1.8203292 ======\n",
      "=======Loss: 1.745276 ======\n",
      "=======Loss: 1.7572868 ======\n",
      "=======Loss: 1.9109387 ======\n",
      "=======Loss: 1.7414562 ======\n",
      "=======Loss: 1.6139334 ======\n",
      "=======Loss: 1.7337877 ======\n",
      "=======Loss: 1.6205776 ======\n",
      "=======Loss: 1.8505626 ======\n",
      "=======Loss: 1.765367 ======\n",
      "=======Loss: 1.6809708 ======\n",
      "=======Loss: 1.7938062 ======\n",
      "=======Loss: 1.9533434 ======\n",
      "=======Loss: 1.7892878 ======\n",
      "=======Loss: 1.7005647 ======\n",
      "=======Loss: 1.7278085 ======\n",
      "=======Loss: 2.0187852 ======\n",
      "=======Loss: 1.6775466 ======\n",
      "=======Loss: 1.8641167 ======\n",
      "=======Loss: 1.8362421 ======\n",
      "=======Loss: 1.7918499 ======\n",
      "=======Loss: 1.7864197 ======\n",
      "=======Loss: 1.7293375 ======\n",
      "=======Loss: 1.6721859 ======\n",
      "=======Loss: 1.8358877 ======\n",
      "=======Loss: 1.6577188 ======\n",
      "=======Loss: 1.8785561 ======\n",
      "=======Loss: 1.90256 ======\n",
      "=======Loss: 1.9028492 ======\n",
      "=======Loss: 1.7763704 ======\n",
      "=======Loss: 1.7824832 ======\n",
      "=======Loss: 1.7635136 ======\n",
      "=======Loss: 1.8184916 ======\n",
      "=======Loss: 1.9676495 ======\n",
      "=======Loss: 1.7824324 ======\n",
      "=======Loss: 1.6957493 ======\n",
      "=======Loss: 1.5509912 ======\n",
      "=======Loss: 1.8143715 ======\n",
      "=======Loss: 1.6971543 ======\n",
      "=======Loss: 1.8007276 ======\n",
      "=======Loss: 1.6678514 ======\n",
      "=======Loss: 1.8180532 ======\n",
      "=======Loss: 2.1171074 ======\n",
      "=======Loss: 1.7335981 ======\n",
      "=======Loss: 1.8592623 ======\n",
      "=======Loss: 1.8225286 ======\n",
      "=======Loss: 1.8044071 ======\n",
      "=======Loss: 1.6967349 ======\n",
      "=======Loss: 1.7875777 ======\n",
      "=======Loss: 1.8235395 ======\n",
      "=======Loss: 1.7179627 ======\n",
      "=======Loss: 1.6902671 ======\n",
      "=======Loss: 1.7901568 ======\n",
      "=======Loss: 1.8720974 ======\n",
      "=======Loss: 2.0147731 ======\n",
      "=======Loss: 1.8566842 ======\n",
      "=======Loss: 1.7210588 ======\n",
      "=======Loss: 1.6521666 ======\n",
      "=======Loss: 1.9216588 ======\n",
      "=======Loss: 1.7505401 ======\n",
      "=======Loss: 1.8830974 ======\n",
      "=======Loss: 1.9975772 ======\n",
      "=======Loss: 1.9650896 ======\n",
      "=======Loss: 1.7907426 ======\n",
      "=======Loss: 1.7357777 ======\n",
      "=======Loss: 1.8945103 ======\n",
      "=======Loss: 1.752206 ======\n",
      "=======Loss: 1.7029037 ======\n",
      "=======Loss: 2.09553 ======\n",
      "=======Loss: 1.7595927 ======\n",
      "=======Loss: 1.812903 ======\n",
      "=======Loss: 1.773587 ======\n",
      "=======Loss: 1.7316606 ======\n",
      "=======Loss: 1.8404396 ======\n",
      "=======Loss: 1.7385898 ======\n",
      "=======Loss: 1.888797 ======\n",
      "=======Loss: 1.6329514 ======\n",
      "=======Loss: 1.8317685 ======\n",
      "=======Loss: 1.9932096 ======\n",
      "=======Loss: 1.7489973 ======\n",
      "=======Loss: 1.9378171 ======\n",
      "=======Loss: 1.9449849 ======\n",
      "=======Loss: 1.8566308 ======\n",
      "=======Loss: 1.8310344 ======\n",
      "=======Loss: 1.9057252 ======\n",
      "=======Loss: 1.9459051 ======\n",
      "=======Loss: 1.7454982 ======\n",
      "=======Loss: 1.7269192 ======\n",
      "=======Loss: 1.699436 ======\n",
      "=======Loss: 1.7819248 ======\n",
      "=======Loss: 1.8246646 ======\n",
      "=======Loss: 1.8933139 ======\n",
      "=======Loss: 1.9040642 ======\n",
      "=======Loss: 1.8639458 ======\n",
      "=======Loss: 1.8038323 ======\n",
      "=======Loss: 1.8419032 ======\n",
      "=======Loss: 2.039454 ======\n",
      "=======Loss: 1.7007954 ======\n",
      "=======Loss: 1.9122782 ======\n",
      "=======Loss: 1.5730249 ======\n",
      "=======Loss: 1.7329526 ======\n",
      "=======Loss: 1.6358826 ======\n",
      "=======Loss: 1.7840121 ======\n",
      "=======Loss: 1.8876143 ======\n",
      "=======Loss: 1.8721315 ======\n",
      "=======Loss: 1.8320491 ======\n",
      "=======Loss: 1.5441271 ======\n",
      "=======Loss: 1.7365972 ======\n",
      "=======Loss: 1.8876302 ======\n",
      "=======Loss: 1.7668031 ======\n",
      "=======Loss: 1.9203908 ======\n",
      "=======Loss: 1.703817 ======\n",
      "=======Loss: 1.7209332 ======\n",
      "=======Loss: 1.6849461 ======\n",
      "=======Loss: 1.8527288 ======\n",
      "=======Loss: 1.71842 ======\n",
      "=======Loss: 1.8745139 ======\n",
      "=======Loss: 1.781472 ======\n",
      "=======Loss: 1.8348104 ======\n",
      "=======Loss: 2.0350199 ======\n",
      "=======Loss: 1.6474515 ======\n",
      "=======Loss: 1.9852304 ======\n",
      "=======Loss: 1.5888665 ======\n",
      "=======Loss: 1.7202243 ======\n",
      "=======Loss: 1.7616591 ======\n",
      "=======Loss: 1.7968831 ======\n",
      "=======Loss: 1.845299 ======\n",
      "=======Loss: 2.1124413 ======\n",
      "=======Loss: 1.6436498 ======\n",
      "=======Loss: 1.8158629 ======\n",
      "=======Loss: 1.6166905 ======\n",
      "=======Loss: 1.9595709 ======\n",
      "=======Loss: 1.7701908 ======\n",
      "=======Loss: 1.7254288 ======\n",
      "=======Loss: 1.878976 ======\n",
      "=======Loss: 1.8176787 ======\n",
      "=======Loss: 1.7352383 ======\n",
      "=======Loss: 1.7923174 ======\n",
      "=======Loss: 1.7831692 ======\n",
      "=======Loss: 1.8903676 ======\n",
      "=======Loss: 1.8174247 ======\n",
      "=======Loss: 1.6938231 ======\n",
      "=======Loss: 1.8669791 ======\n",
      "=======Loss: 1.720658 ======\n",
      "=======Loss: 1.9362583 ======\n",
      "=======Loss: 1.8243085 ======\n",
      "=======Loss: 1.7881637 ======\n",
      "=======Loss: 1.9152849 ======\n",
      "=======Loss: 2.1510432 ======\n",
      "=======Loss: 1.8864318 ======\n",
      "=======Loss: 1.8532829 ======\n",
      "=======Loss: 1.8264134 ======\n",
      "=======Loss: 1.8075784 ======\n",
      "=======Loss: 2.1825192 ======\n",
      "=======Loss: 1.7065735 ======\n",
      "=======Loss: 1.8033056 ======\n",
      "=======Loss: 1.893357 ======\n",
      "=======Loss: 1.5618095 ======\n",
      "=======Loss: 1.5988271 ======\n",
      "=======Loss: 1.7308387 ======\n",
      "=======Loss: 1.777333 ======\n",
      "=======Loss: 1.8506104 ======\n",
      "=======Loss: 1.67174 ======\n",
      "=======Loss: 1.9785041 ======\n",
      "=======Loss: 1.7661104 ======\n",
      "=======Loss: 1.87302 ======\n",
      "=======Loss: 1.8045092 ======\n",
      "=======Loss: 1.6760051 ======\n",
      "=======Loss: 1.7943135 ======\n",
      "=======Loss: 1.8788383 ======\n",
      "=======Loss: 1.5983272 ======\n",
      "=======Loss: 1.8297958 ======\n",
      "=======Loss: 1.6305671 ======\n",
      "=======Loss: 1.8219666 ======\n",
      "=======Loss: 1.8974725 ======\n",
      "=======Loss: 1.8564174 ======\n",
      "=======Loss: 1.6981137 ======\n",
      "=======Loss: 1.7792513 ======\n",
      "=======Loss: 1.6401691 ======\n",
      "=======Loss: 1.8020043 ======\n",
      "=======Loss: 1.7563077 ======\n",
      "=======Loss: 1.8303719 ======\n",
      "=======Loss: 1.9887702 ======\n",
      "=======Loss: 1.8091412 ======\n",
      "=======Loss: 1.8146191 ======\n",
      "=======Loss: 2.1113036 ======\n",
      "=======Loss: 1.7035395 ======\n",
      "=======Loss: 1.8581483 ======\n",
      "=======Loss: 1.9143076 ======\n",
      "=======Loss: 2.0890014 ======\n",
      "=======Loss: 1.6618466 ======\n",
      "=======Loss: 1.8965814 ======\n",
      "=======Loss: 1.7592328 ======\n",
      "=======Loss: 1.8136569 ======\n",
      "=======Loss: 1.6775551 ======\n",
      "=======Loss: 1.8751562 ======\n",
      "=======Loss: 1.7521883 ======\n",
      "=======Loss: 1.6549505 ======\n",
      "=======Loss: 1.6167792 ======\n",
      "=======Loss: 1.9682661 ======\n",
      "=======Loss: 1.6576049 ======\n",
      "=======Loss: 1.7724352 ======\n",
      "=======Loss: 1.8623693 ======\n",
      "=======Loss: 1.8300071 ======\n",
      "=======Loss: 1.7860417 ======\n",
      "=======Loss: 1.7353629 ======\n",
      "=======Loss: 1.686588 ======\n",
      "=======Loss: 1.7794359 ======\n",
      "=======Loss: 2.0005488 ======\n",
      "=======Loss: 1.7657316 ======\n",
      "=======Loss: 1.9988298 ======\n",
      "=======Loss: 1.6866281 ======\n",
      "=======Loss: 1.717298 ======\n",
      "=======Loss: 1.8599465 ======\n",
      "=======Loss: 1.83855 ======\n",
      "=======Loss: 1.7119308 ======\n",
      "=======Loss: 1.7954533 ======\n",
      "=======Loss: 1.6518831 ======\n",
      "=======Loss: 1.7571161 ======\n",
      "=======Loss: 1.7986008 ======\n",
      "=======Loss: 2.0977988 ======\n",
      "=======Loss: 1.7532853 ======\n",
      "=======Loss: 1.9464502 ======\n",
      "=======Loss: 1.7394619 ======\n",
      "=======Loss: 1.8888073 ======\n",
      "=======Loss: 1.7124414 ======\n",
      "=======Loss: 1.6948826 ======\n",
      "=======Loss: 1.8428357 ======\n",
      "=======Loss: 1.6393721 ======\n",
      "=======Loss: 1.753372 ======\n",
      "=======Loss: 1.8257227 ======\n",
      "=======Loss: 1.9444354 ======\n",
      "=======Loss: 1.7566588 ======\n",
      "=======Loss: 1.8721471 ======\n",
      "=======Loss: 1.8261223 ======\n",
      "=======Loss: 1.8001666 ======\n",
      "=======Loss: 1.7225357 ======\n",
      "=======Loss: 1.6637822 ======\n",
      "=======Loss: 1.7795252 ======\n",
      "=======Loss: 1.9623985 ======\n",
      "=======Loss: 1.9281204 ======\n",
      "=======Loss: 1.6845814 ======\n",
      "=======Loss: 1.9547365 ======\n",
      "=======Loss: 1.6758091 ======\n",
      "=======Loss: 1.9587789 ======\n",
      "=======Loss: 1.8694263 ======\n",
      "=======Loss: 1.9010675 ======\n",
      "=======Loss: 1.7575331 ======\n",
      "=======Loss: 1.9362984 ======\n",
      "=======Loss: 1.6986312 ======\n",
      "=======Loss: 1.9587028 ======\n",
      "=======Loss: 1.8139963 ======\n",
      "=======Loss: 2.0037394 ======\n",
      "=======Loss: 1.9741216 ======\n",
      "=======Loss: 2.345543 ======\n",
      "=======Loss: 1.7502412 ======\n",
      "=======Loss: 1.9412706 ======\n",
      "=======Loss: 1.7061557 ======\n",
      "=======Loss: 2.0429218 ======\n",
      "=======Loss: 1.7813568 ======\n",
      "=======Loss: 1.704747 ======\n",
      "=======Loss: 2.030882 ======\n",
      "=======Loss: 1.743707 ======\n",
      "=======Loss: 1.6827083 ======\n",
      "=======Loss: 1.6380116 ======\n",
      "=======Loss: 1.6067288 ======\n",
      "=======Loss: 1.869103 ======\n",
      "=======Loss: 1.8496286 ======\n",
      "=======Loss: 1.7877692 ======\n",
      "=======Loss: 1.7237889 ======\n",
      "=======Loss: 2.112907 ======\n",
      "=======Loss: 1.787226 ======\n",
      "=======Loss: 1.9059265 ======\n",
      "=======Loss: 1.7316103 ======\n",
      "=======Loss: 1.692416 ======\n",
      "=======Loss: 1.8235126 ======\n",
      "=======Loss: 1.7220082 ======\n",
      "=======Loss: 1.7362232 ======\n",
      "=======Loss: 1.7958318 ======\n",
      "=======Loss: 2.056797 ======\n",
      "=======Loss: 1.6799482 ======\n",
      "=======Loss: 1.6528099 ======\n",
      "=======Loss: 1.6788158 ======\n",
      "=======Loss: 1.9848891 ======\n",
      "=======Loss: 1.8993433 ======\n",
      "=======Loss: 1.980598 ======\n",
      "=======Loss: 1.8446307 ======\n",
      "=======Loss: 1.948878 ======\n",
      "=======Loss: 1.864439 ======\n",
      "=======Loss: 1.5831656 ======\n",
      "=======Loss: 1.7239764 ======\n",
      "=======Loss: 1.7445332 ======\n",
      "=======Loss: 1.821586 ======\n",
      "=======Loss: 1.8576943 ======\n",
      "=======Loss: 1.8995692 ======\n",
      "=======Loss: 1.9519912 ======\n",
      "=======Loss: 1.8286715 ======\n",
      "=======Loss: 1.5899768 ======\n",
      "=======Loss: 1.9271533 ======\n",
      "=======Loss: 1.7463384 ======\n",
      "=======Loss: 1.6102755 ======\n",
      "=======Loss: 1.6563584 ======\n",
      "=======Loss: 1.9220059 ======\n",
      "=======Loss: 1.7683419 ======\n",
      "=======Loss: 1.9309366 ======\n",
      "=======Loss: 1.7331405 ======\n",
      "=======Loss: 2.118563 ======\n",
      "=======Loss: 1.8911704 ======\n",
      "=======Loss: 1.6672039 ======\n",
      "=======Loss: 1.7350188 ======\n",
      "=======Loss: 1.7510124 ======\n",
      "=======Loss: 2.0108733 ======\n",
      "=======Loss: 1.6731484 ======\n",
      "=======Loss: 1.8633804 ======\n",
      "=======Loss: 1.9717587 ======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# track number of training examples in batch\u001b[39;00m\n\u001b[0;32m     41\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m     sample \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     46\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(v)\n\u001b[0;32m     49\u001b[0m     }\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\neuralop\\datasets\\spherical_swe.py:86\u001b[0m, in \u001b[0;36mSphericalSWEDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 86\u001b[0m         inp, tar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[0;32m     89\u001b[0m             inp \u001b[38;5;241m=\u001b[39m (inp \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_mean) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_var)\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\neuralop\\datasets\\spherical_swe.py:76\u001b[0m, in \u001b[0;36mSphericalSWEDataset._get_sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mgalewsky_initial_condition()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# solve pde for n steps to return the target\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m tar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mspec2grid(inp)\n\u001b[0;32m     78\u001b[0m tar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mspec2grid(tar)        \n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch_harmonics\\examples\\shallow_water_equations.py:295\u001b[0m, in \u001b[0;36mShallowWaterSolver.timestep\u001b[1;34m(self, uspec, nsteps)\u001b[0m\n\u001b[0;32m    292\u001b[0m iold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nsteps):\n\u001b[1;32m--> 295\u001b[0m     dudtspec[inew] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdudtspec\u001b[49m\u001b[43m(\u001b[49m\u001b[43muspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;66;03m# update vort,div,phiv with third-order adams-bashforth.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# forward euler, then 2nd-order adams-bashforth time steps to start.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch_harmonics\\examples\\shallow_water_equations.py:172\u001b[0m, in \u001b[0;36mShallowWaterSolver.dudtspec\u001b[1;34m(self, uspec)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# compute the derivatives - this should be incorporated into the solver:\u001b[39;00m\n\u001b[0;32m    171\u001b[0m ugrid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec2grid(uspec)\n\u001b[1;32m--> 172\u001b[0m uvgrid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetuv\u001b[49m\u001b[43m(\u001b[49m\u001b[43muspec\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# phi = ugrid[0]\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# vrtdiv = ugrid[1:]\u001b[39;00m\n\u001b[0;32m    177\u001b[0m tmp \u001b[38;5;241m=\u001b[39m uvgrid \u001b[38;5;241m*\u001b[39m (ugrid[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoriolis)\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch_harmonics\\examples\\shallow_water_equations.py:136\u001b[0m, in \u001b[0;36mShallowWaterSolver.getuv\u001b[1;34m(self, vrtdivspec)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetuv\u001b[39m(\u001b[38;5;28mself\u001b[39m, vrtdivspec):\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    compute wind vector from spectral coeffs of vorticity and divergence\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mivsht\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvlap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvrtdivspec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_params = count_model_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# %%\n",
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=8e-4, \n",
    "                                weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2, reduce_dims=(0,1))\n",
    "# h1loss = H1Loss(d=2, reduce_dims=(0,1))\n",
    "\n",
    "train_loss = l2loss\n",
    "eval_losses={'l2': l2loss} #'h1': h1loss, \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()\n",
    "\n",
    "with open('script/sfno_loss.txt', 'w') as f:\n",
    "    for epoch in range(100):\n",
    "        avg_loss = 0\n",
    "        avg_lasso_loss = 0\n",
    "        train_err = 0.0\n",
    "        \n",
    "        # track number of training examples in batch\n",
    "        n_samples = 0\n",
    "        for idx, sample in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            sample = {\n",
    "                k: v.to(device)\n",
    "                for k, v in sample.items()\n",
    "                if torch.is_tensor(v)\n",
    "            }\n",
    "\n",
    "            n_samples += sample[\"y\"].shape[0]\n",
    "            out = model(sample[\"x\"])\n",
    "\n",
    "            loss = l2loss(out, **sample)\n",
    "\n",
    "            loss.backward()\n",
    "            del out\n",
    "\n",
    "            optimizer.step()\n",
    "            train_err += loss.item()\n",
    "            with torch.no_grad():\n",
    "                print(\"=======Loss:\",loss.detach().cpu().numpy(),\"======\")\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_err)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_err /= len(train_loader)\n",
    "        avg_loss /= n_samples\n",
    "        \n",
    "        # 将每个epoch的loss值写入文件\n",
    "        f.write(f'Epoch {epoch + 1}, Loss: {train_err}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=1\n",
      "Test Loss: 0.5864 ± 0.0475\n"
     ]
    }
   ],
   "source": [
    "resolution = (32, 64)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (64, 128) with 50 samples and batch-size=1\n",
      "Test Loss: 2.5072 ± 0.1057\n"
     ]
    }
   ],
   "source": [
    "resolution = (64, 128)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (128, 256) with 50 samples and batch-size=1\n",
      "Test Loss: 3.6762 ± 0.1100\n"
     ]
    }
   ],
   "source": [
    "resolution = (128, 256)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "    # Input x\n",
    "    x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "    # Ground-truth\n",
    "    y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "    # Model prediction\n",
    "    x_in = x.unsqueeze(0).to(device)\n",
    "    out = model(x_in).squeeze()[0, ...].detach().cpu().numpy()\n",
    "    x = x[0, ...].detach().numpy()\n",
    "\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"./script/output_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "#     # Input x\n",
    "#     x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "#     # Ground-truth\n",
    "#     y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "#     x = x[0, ...].detach().numpy()\n",
    "    \n",
    "#     plt.imshow(x)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/input_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    \n",
    "#     plt.imshow(y)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/label_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
