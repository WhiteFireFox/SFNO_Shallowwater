{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Neural Operators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=10\n",
      "Loading test dataloader at resolution (64, 128) with 50 samples and batch-size=10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from neuralop.models import FNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.datasets import load_spherical_swe\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %%\n",
    "# Loading the Navier-Stokes dataset in 128x128 resolution\n",
    "train_loader, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[(32, 64), (64, 128)], n_tests=[50, 50], test_batch_sizes=[10, 10])\n",
    "\n",
    "model = FNO(n_modes=(32, 64), in_channels=3, out_channels=3, hidden_channels=32, projection_channels=64, factorization='dense').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 8666531 parameters.\n",
      "\n",
      "### MODEL ###\n",
      " FNO(\n",
      "  (fno_blocks): FNOBlocks(\n",
      "    (convs): SpectralConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-3): 4 x ComplexDenseTensor(shape=torch.Size([32, 32, 32, 33]), rank=None)\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-3): 4 x Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (lifting): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (projection): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0008\n",
      "    lr: 0.0008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001EB5B2574F0>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.LpLoss object at 0x000001EB5B3B1400>\n",
      "\n",
      " * Test: {'l2': <neuralop.losses.data_losses.LpLoss object at 0x000001EB5B3B1400>}\n",
      "=======Loss: 12.136278 ======\n",
      "=======Loss: 12.249117 ======\n",
      "=======Loss: 12.181902 ======\n",
      "=======Loss: 12.130911 ======\n",
      "=======Loss: 11.981334 ======\n",
      "=======Loss: 12.005163 ======\n",
      "=======Loss: 12.071772 ======\n",
      "=======Loss: 11.991985 ======\n",
      "=======Loss: 11.935195 ======\n",
      "=======Loss: 11.945664 ======\n",
      "=======Loss: 11.929007 ======\n",
      "=======Loss: 11.9033 ======\n",
      "=======Loss: 11.862317 ======\n",
      "=======Loss: 11.776964 ======\n",
      "=======Loss: 11.778975 ======\n",
      "=======Loss: 11.745049 ======\n",
      "=======Loss: 11.56303 ======\n",
      "=======Loss: 11.652597 ======\n",
      "=======Loss: 11.6034565 ======\n",
      "=======Loss: 11.573529 ======\n",
      "=======Loss: 11.445639 ======\n",
      "=======Loss: 11.248421 ======\n",
      "=======Loss: 11.098085 ======\n",
      "=======Loss: 11.089733 ======\n",
      "=======Loss: 10.975782 ======\n",
      "=======Loss: 10.613262 ======\n",
      "=======Loss: 10.418005 ======\n",
      "=======Loss: 10.363087 ======\n",
      "=======Loss: 10.28068 ======\n",
      "=======Loss: 10.372935 ======\n",
      "=======Loss: 10.171725 ======\n",
      "=======Loss: 10.1050205 ======\n",
      "=======Loss: 9.862546 ======\n",
      "=======Loss: 10.024125 ======\n",
      "=======Loss: 9.944071 ======\n",
      "=======Loss: 9.89182 ======\n",
      "=======Loss: 9.653122 ======\n",
      "=======Loss: 9.620187 ======\n",
      "=======Loss: 9.658695 ======\n",
      "=======Loss: 9.431145 ======\n",
      "=======Loss: 9.562002 ======\n",
      "=======Loss: 9.366129 ======\n",
      "=======Loss: 9.0942745 ======\n",
      "=======Loss: 9.275706 ======\n",
      "=======Loss: 9.053225 ======\n",
      "=======Loss: 9.16057 ======\n",
      "=======Loss: 8.975754 ======\n",
      "=======Loss: 9.123064 ======\n",
      "=======Loss: 8.843523 ======\n",
      "=======Loss: 8.943216 ======\n",
      "=======Loss: 8.852572 ======\n",
      "=======Loss: 8.94338 ======\n",
      "=======Loss: 8.931349 ======\n",
      "=======Loss: 8.707207 ======\n",
      "=======Loss: 8.642255 ======\n",
      "=======Loss: 8.763712 ======\n",
      "=======Loss: 8.4004755 ======\n",
      "=======Loss: 8.443569 ======\n",
      "=======Loss: 8.631431 ======\n",
      "=======Loss: 8.3063135 ======\n",
      "=======Loss: 8.4361515 ======\n",
      "=======Loss: 8.136445 ======\n",
      "=======Loss: 8.334417 ======\n",
      "=======Loss: 8.161811 ======\n",
      "=======Loss: 8.267986 ======\n",
      "=======Loss: 8.216806 ======\n",
      "=======Loss: 7.8017626 ======\n",
      "=======Loss: 8.086448 ======\n",
      "=======Loss: 7.990202 ======\n",
      "=======Loss: 7.9202795 ======\n",
      "=======Loss: 7.7390532 ======\n",
      "=======Loss: 7.8966002 ======\n",
      "=======Loss: 7.7053747 ======\n",
      "=======Loss: 7.6253204 ======\n",
      "=======Loss: 7.3692217 ======\n",
      "=======Loss: 7.3499727 ======\n",
      "=======Loss: 7.2565413 ======\n",
      "=======Loss: 7.102062 ======\n",
      "=======Loss: 7.25883 ======\n",
      "=======Loss: 7.0372076 ======\n",
      "=======Loss: 6.8935432 ======\n",
      "=======Loss: 6.9874506 ======\n",
      "=======Loss: 6.557734 ======\n",
      "=======Loss: 6.8209925 ======\n",
      "=======Loss: 6.768382 ======\n",
      "=======Loss: 6.6193843 ======\n",
      "=======Loss: 6.6822205 ======\n",
      "=======Loss: 6.306862 ======\n",
      "=======Loss: 6.1206584 ======\n",
      "=======Loss: 6.329569 ======\n",
      "=======Loss: 6.2727566 ======\n",
      "=======Loss: 6.395809 ======\n",
      "=======Loss: 6.3073573 ======\n",
      "=======Loss: 6.2487564 ======\n",
      "=======Loss: 6.0610437 ======\n",
      "=======Loss: 6.1562834 ======\n",
      "=======Loss: 6.0954742 ======\n",
      "=======Loss: 6.0494475 ======\n",
      "=======Loss: 5.670108 ======\n",
      "=======Loss: 6.001473 ======\n",
      "=======Loss: 6.018381 ======\n",
      "=======Loss: 5.9444785 ======\n",
      "=======Loss: 5.883153 ======\n",
      "=======Loss: 6.3622227 ======\n",
      "=======Loss: 5.7100086 ======\n",
      "=======Loss: 5.758311 ======\n",
      "=======Loss: 5.9336147 ======\n",
      "=======Loss: 5.9134254 ======\n",
      "=======Loss: 5.8124294 ======\n",
      "=======Loss: 5.798701 ======\n",
      "=======Loss: 5.8201704 ======\n",
      "=======Loss: 5.723901 ======\n",
      "=======Loss: 5.802473 ======\n",
      "=======Loss: 5.7933292 ======\n",
      "=======Loss: 5.8019056 ======\n",
      "=======Loss: 6.237483 ======\n",
      "=======Loss: 5.6392307 ======\n",
      "=======Loss: 5.625058 ======\n",
      "=======Loss: 6.035656 ======\n",
      "=======Loss: 5.737723 ======\n",
      "=======Loss: 5.5907006 ======\n",
      "=======Loss: 5.7243996 ======\n",
      "=======Loss: 5.877698 ======\n",
      "=======Loss: 5.6826334 ======\n",
      "=======Loss: 5.6052246 ======\n",
      "=======Loss: 5.6957517 ======\n",
      "=======Loss: 5.731882 ======\n",
      "=======Loss: 5.5540338 ======\n",
      "=======Loss: 5.43207 ======\n",
      "=======Loss: 5.8612256 ======\n",
      "=======Loss: 5.529806 ======\n",
      "=======Loss: 5.762498 ======\n",
      "=======Loss: 5.472637 ======\n",
      "=======Loss: 5.4359274 ======\n",
      "=======Loss: 5.7683425 ======\n",
      "=======Loss: 5.2388544 ======\n",
      "=======Loss: 5.3954496 ======\n",
      "=======Loss: 5.561475 ======\n",
      "=======Loss: 5.437844 ======\n",
      "=======Loss: 5.847828 ======\n",
      "=======Loss: 5.58622 ======\n",
      "=======Loss: 5.3216963 ======\n",
      "=======Loss: 5.6828747 ======\n",
      "=======Loss: 5.268883 ======\n",
      "=======Loss: 5.2099404 ======\n",
      "=======Loss: 5.627107 ======\n",
      "=======Loss: 5.680118 ======\n",
      "=======Loss: 5.3287983 ======\n",
      "=======Loss: 5.2801547 ======\n",
      "=======Loss: 5.320012 ======\n",
      "=======Loss: 5.3242245 ======\n",
      "=======Loss: 5.3008256 ======\n",
      "=======Loss: 5.3543253 ======\n",
      "=======Loss: 5.412698 ======\n",
      "=======Loss: 5.5649204 ======\n",
      "=======Loss: 5.428685 ======\n",
      "=======Loss: 5.295968 ======\n",
      "=======Loss: 5.3907657 ======\n",
      "=======Loss: 4.91026 ======\n",
      "=======Loss: 5.23119 ======\n",
      "=======Loss: 5.403211 ======\n",
      "=======Loss: 4.9815474 ======\n",
      "=======Loss: 5.14041 ======\n",
      "=======Loss: 5.103606 ======\n",
      "=======Loss: 5.1871467 ======\n",
      "=======Loss: 4.9909115 ======\n",
      "=======Loss: 5.06857 ======\n",
      "=======Loss: 5.339443 ======\n",
      "=======Loss: 5.25422 ======\n",
      "=======Loss: 5.113002 ======\n",
      "=======Loss: 5.04591 ======\n",
      "=======Loss: 5.358515 ======\n",
      "=======Loss: 4.989602 ======\n",
      "=======Loss: 4.8026876 ======\n",
      "=======Loss: 4.865888 ======\n",
      "=======Loss: 4.8864603 ======\n",
      "=======Loss: 4.997468 ======\n",
      "=======Loss: 5.269044 ======\n",
      "=======Loss: 4.934405 ======\n",
      "=======Loss: 4.883013 ======\n",
      "=======Loss: 4.895617 ======\n",
      "=======Loss: 4.6357346 ======\n",
      "=======Loss: 4.673069 ======\n",
      "=======Loss: 4.8500757 ======\n",
      "=======Loss: 4.526757 ======\n",
      "=======Loss: 4.6216 ======\n",
      "=======Loss: 4.5240707 ======\n",
      "=======Loss: 4.614081 ======\n",
      "=======Loss: 4.8365355 ======\n",
      "=======Loss: 4.7452335 ======\n",
      "=======Loss: 4.386806 ======\n",
      "=======Loss: 4.6141763 ======\n",
      "=======Loss: 4.631401 ======\n",
      "=======Loss: 4.729687 ======\n",
      "=======Loss: 4.655654 ======\n",
      "=======Loss: 4.735469 ======\n",
      "=======Loss: 4.811294 ======\n",
      "=======Loss: 4.5429845 ======\n",
      "=======Loss: 4.4099016 ======\n",
      "=======Loss: 4.7472095 ======\n",
      "=======Loss: 4.523183 ======\n",
      "=======Loss: 4.056285 ======\n",
      "=======Loss: 4.5253067 ======\n",
      "=======Loss: 4.3756623 ======\n",
      "=======Loss: 4.485315 ======\n",
      "=======Loss: 4.1520433 ======\n",
      "=======Loss: 4.3910236 ======\n",
      "=======Loss: 4.4546824 ======\n",
      "=======Loss: 4.2771254 ======\n",
      "=======Loss: 4.2121005 ======\n",
      "=======Loss: 4.3905616 ======\n",
      "=======Loss: 4.308827 ======\n",
      "=======Loss: 4.5134735 ======\n",
      "=======Loss: 4.217021 ======\n",
      "=======Loss: 4.291193 ======\n",
      "=======Loss: 4.1774454 ======\n",
      "=======Loss: 4.0770254 ======\n",
      "=======Loss: 4.2117805 ======\n",
      "=======Loss: 4.1557775 ======\n",
      "=======Loss: 4.47927 ======\n",
      "=======Loss: 4.027831 ======\n",
      "=======Loss: 3.9119375 ======\n",
      "=======Loss: 3.8983984 ======\n",
      "=======Loss: 3.9633574 ======\n",
      "=======Loss: 3.9671993 ======\n",
      "=======Loss: 4.374192 ======\n",
      "=======Loss: 4.011528 ======\n",
      "=======Loss: 4.015051 ======\n",
      "=======Loss: 4.1583424 ======\n",
      "=======Loss: 4.2383223 ======\n",
      "=======Loss: 3.9618452 ======\n",
      "=======Loss: 4.1216307 ======\n",
      "=======Loss: 3.840695 ======\n",
      "=======Loss: 4.054129 ======\n",
      "=======Loss: 4.099371 ======\n",
      "=======Loss: 3.9878235 ======\n",
      "=======Loss: 3.6235175 ======\n",
      "=======Loss: 3.9734159 ======\n",
      "=======Loss: 4.0294113 ======\n",
      "=======Loss: 4.00517 ======\n",
      "=======Loss: 3.7085445 ======\n",
      "=======Loss: 3.9148443 ======\n",
      "=======Loss: 3.685636 ======\n",
      "=======Loss: 3.8767712 ======\n",
      "=======Loss: 4.3694 ======\n",
      "=======Loss: 3.96172 ======\n",
      "=======Loss: 3.6644843 ======\n",
      "=======Loss: 3.929689 ======\n",
      "=======Loss: 3.8738658 ======\n",
      "=======Loss: 3.6242533 ======\n",
      "=======Loss: 3.6051574 ======\n",
      "=======Loss: 4.2076497 ======\n",
      "=======Loss: 3.9619513 ======\n",
      "=======Loss: 3.6977024 ======\n",
      "=======Loss: 3.959336 ======\n",
      "=======Loss: 3.9372463 ======\n",
      "=======Loss: 3.960742 ======\n",
      "=======Loss: 3.5814612 ======\n",
      "=======Loss: 3.707026 ======\n",
      "=======Loss: 3.8490386 ======\n",
      "=======Loss: 3.711687 ======\n",
      "=======Loss: 3.7536235 ======\n",
      "=======Loss: 3.7512484 ======\n",
      "=======Loss: 3.8918874 ======\n",
      "=======Loss: 3.7258239 ======\n",
      "=======Loss: 3.5003674 ======\n",
      "=======Loss: 3.573084 ======\n",
      "=======Loss: 3.7881765 ======\n",
      "=======Loss: 3.7877944 ======\n",
      "=======Loss: 3.86764 ======\n",
      "=======Loss: 3.462569 ======\n",
      "=======Loss: 3.6541934 ======\n",
      "=======Loss: 3.617775 ======\n",
      "=======Loss: 3.5195127 ======\n",
      "=======Loss: 3.628088 ======\n",
      "=======Loss: 3.7373977 ======\n",
      "=======Loss: 3.6412792 ======\n",
      "=======Loss: 3.7266893 ======\n",
      "=======Loss: 3.7706635 ======\n",
      "=======Loss: 3.9417884 ======\n",
      "=======Loss: 3.7442157 ======\n",
      "=======Loss: 3.3451724 ======\n",
      "=======Loss: 3.6075807 ======\n",
      "=======Loss: 3.713562 ======\n",
      "=======Loss: 3.7224941 ======\n",
      "=======Loss: 3.67613 ======\n",
      "=======Loss: 3.7000225 ======\n",
      "=======Loss: 3.734335 ======\n",
      "=======Loss: 3.7361083 ======\n",
      "=======Loss: 3.5948372 ======\n",
      "=======Loss: 3.552349 ======\n",
      "=======Loss: 3.4727726 ======\n",
      "=======Loss: 3.7247999 ======\n",
      "=======Loss: 3.5777307 ======\n",
      "=======Loss: 3.813125 ======\n",
      "=======Loss: 3.4992561 ======\n",
      "=======Loss: 3.3886352 ======\n",
      "=======Loss: 3.481519 ======\n",
      "=======Loss: 3.4037623 ======\n",
      "=======Loss: 3.4797313 ======\n",
      "=======Loss: 3.4555688 ======\n",
      "=======Loss: 3.390062 ======\n",
      "=======Loss: 3.6791701 ======\n",
      "=======Loss: 3.668387 ======\n",
      "=======Loss: 3.5682907 ======\n",
      "=======Loss: 3.4935794 ======\n",
      "=======Loss: 3.2776358 ======\n",
      "=======Loss: 3.4092383 ======\n",
      "=======Loss: 3.63855 ======\n",
      "=======Loss: 3.1946867 ======\n",
      "=======Loss: 3.3683867 ======\n",
      "=======Loss: 3.559666 ======\n",
      "=======Loss: 3.6844237 ======\n",
      "=======Loss: 3.803788 ======\n",
      "=======Loss: 3.3622184 ======\n",
      "=======Loss: 3.5587945 ======\n",
      "=======Loss: 3.8097813 ======\n",
      "=======Loss: 3.637729 ======\n",
      "=======Loss: 3.5059905 ======\n",
      "=======Loss: 3.4319263 ======\n",
      "=======Loss: 3.327773 ======\n",
      "=======Loss: 3.284165 ======\n",
      "=======Loss: 3.418931 ======\n",
      "=======Loss: 3.5338311 ======\n",
      "=======Loss: 3.2796571 ======\n",
      "=======Loss: 3.4060583 ======\n",
      "=======Loss: 3.6161184 ======\n",
      "=======Loss: 3.5315943 ======\n",
      "=======Loss: 3.7209537 ======\n",
      "=======Loss: 3.3207173 ======\n",
      "=======Loss: 3.2123713 ======\n",
      "=======Loss: 3.3991728 ======\n",
      "=======Loss: 3.5708663 ======\n",
      "=======Loss: 3.670216 ======\n",
      "=======Loss: 3.9245498 ======\n",
      "=======Loss: 3.7483182 ======\n",
      "=======Loss: 3.5764897 ======\n",
      "=======Loss: 3.1562927 ======\n",
      "=======Loss: 3.6515567 ======\n",
      "=======Loss: 3.3984458 ======\n",
      "=======Loss: 3.4612055 ======\n",
      "=======Loss: 3.5266562 ======\n",
      "=======Loss: 3.7638955 ======\n",
      "=======Loss: 3.5882688 ======\n",
      "=======Loss: 3.2894692 ======\n",
      "=======Loss: 3.65809 ======\n",
      "=======Loss: 3.5527053 ======\n",
      "=======Loss: 3.1920924 ======\n",
      "=======Loss: 3.3223329 ======\n",
      "=======Loss: 3.3188074 ======\n",
      "=======Loss: 3.530287 ======\n",
      "=======Loss: 3.2887063 ======\n",
      "=======Loss: 3.4213 ======\n",
      "=======Loss: 3.4701633 ======\n",
      "=======Loss: 3.4842303 ======\n",
      "=======Loss: 3.4016068 ======\n",
      "=======Loss: 3.2965417 ======\n",
      "=======Loss: 3.1771739 ======\n",
      "=======Loss: 3.2867217 ======\n",
      "=======Loss: 3.3696003 ======\n",
      "=======Loss: 3.1163483 ======\n",
      "=======Loss: 3.3025968 ======\n",
      "=======Loss: 3.2840142 ======\n",
      "=======Loss: 3.5927734 ======\n",
      "=======Loss: 3.4221327 ======\n",
      "=======Loss: 3.1325018 ======\n",
      "=======Loss: 3.154046 ======\n",
      "=======Loss: 3.4446926 ======\n",
      "=======Loss: 3.443235 ======\n",
      "=======Loss: 3.3952692 ======\n",
      "=======Loss: 3.4708636 ======\n",
      "=======Loss: 3.361055 ======\n",
      "=======Loss: 3.509553 ======\n",
      "=======Loss: 3.0429277 ======\n",
      "=======Loss: 3.235963 ======\n",
      "=======Loss: 3.463256 ======\n",
      "=======Loss: 3.3110557 ======\n",
      "=======Loss: 3.2365563 ======\n",
      "=======Loss: 3.2559128 ======\n",
      "=======Loss: 3.3851402 ======\n",
      "=======Loss: 3.15977 ======\n",
      "=======Loss: 3.3960419 ======\n",
      "=======Loss: 3.0368967 ======\n",
      "=======Loss: 3.3828824 ======\n",
      "=======Loss: 3.0218813 ======\n",
      "=======Loss: 3.36258 ======\n",
      "=======Loss: 3.121262 ======\n",
      "=======Loss: 3.0698383 ======\n",
      "=======Loss: 3.3218288 ======\n",
      "=======Loss: 3.3426247 ======\n",
      "=======Loss: 3.1325402 ======\n",
      "=======Loss: 3.1727471 ======\n",
      "=======Loss: 3.115878 ======\n",
      "=======Loss: 3.2340593 ======\n",
      "=======Loss: 3.2732844 ======\n",
      "=======Loss: 3.5039866 ======\n",
      "=======Loss: 3.1554997 ======\n",
      "=======Loss: 3.3693979 ======\n",
      "=======Loss: 3.0842078 ======\n",
      "=======Loss: 3.09894 ======\n",
      "=======Loss: 3.2965975 ======\n",
      "=======Loss: 3.343607 ======\n",
      "=======Loss: 3.0173702 ======\n",
      "=======Loss: 3.109865 ======\n",
      "=======Loss: 3.0249403 ======\n",
      "=======Loss: 3.039345 ======\n",
      "=======Loss: 3.2619984 ======\n",
      "=======Loss: 3.100846 ======\n",
      "=======Loss: 3.2163877 ======\n",
      "=======Loss: 3.0746596 ======\n",
      "=======Loss: 3.3221214 ======\n",
      "=======Loss: 2.9617293 ======\n",
      "=======Loss: 3.0912454 ======\n",
      "=======Loss: 3.264298 ======\n",
      "=======Loss: 3.1708457 ======\n",
      "=======Loss: 3.3164318 ======\n",
      "=======Loss: 3.355856 ======\n",
      "=======Loss: 3.1697097 ======\n",
      "=======Loss: 3.2265053 ======\n",
      "=======Loss: 3.2005992 ======\n",
      "=======Loss: 3.0932076 ======\n",
      "=======Loss: 3.1664984 ======\n",
      "=======Loss: 2.9787169 ======\n",
      "=======Loss: 2.9524362 ======\n",
      "=======Loss: 3.1116164 ======\n",
      "=======Loss: 3.1270037 ======\n",
      "=======Loss: 3.0481715 ======\n",
      "=======Loss: 3.2936778 ======\n",
      "=======Loss: 3.0608964 ======\n",
      "=======Loss: 3.1976428 ======\n",
      "=======Loss: 3.0643003 ======\n",
      "=======Loss: 3.0431442 ======\n",
      "=======Loss: 3.1253662 ======\n",
      "=======Loss: 3.0000348 ======\n",
      "=======Loss: 2.9380322 ======\n",
      "=======Loss: 3.2452033 ======\n",
      "=======Loss: 2.8302994 ======\n",
      "=======Loss: 3.0186086 ======\n",
      "=======Loss: 3.0176644 ======\n",
      "=======Loss: 2.8150346 ======\n",
      "=======Loss: 3.0961962 ======\n",
      "=======Loss: 2.9841251 ======\n",
      "=======Loss: 2.9735963 ======\n",
      "=======Loss: 3.118964 ======\n",
      "=======Loss: 3.053515 ======\n",
      "=======Loss: 3.0597908 ======\n",
      "=======Loss: 3.1625876 ======\n",
      "=======Loss: 3.1143043 ======\n",
      "=======Loss: 3.02985 ======\n",
      "=======Loss: 3.0121062 ======\n",
      "=======Loss: 2.8541307 ======\n",
      "=======Loss: 3.0547805 ======\n",
      "=======Loss: 3.0490384 ======\n",
      "=======Loss: 2.977112 ======\n",
      "=======Loss: 2.9974537 ======\n",
      "=======Loss: 2.9325845 ======\n",
      "=======Loss: 2.888094 ======\n",
      "=======Loss: 2.793181 ======\n",
      "=======Loss: 2.7586873 ======\n",
      "=======Loss: 2.833025 ======\n",
      "=======Loss: 2.9863076 ======\n",
      "=======Loss: 2.9578187 ======\n",
      "=======Loss: 2.9873412 ======\n",
      "=======Loss: 2.9040172 ======\n",
      "=======Loss: 2.8462813 ======\n",
      "=======Loss: 2.9000661 ======\n",
      "=======Loss: 3.0039492 ======\n",
      "=======Loss: 3.1119366 ======\n",
      "=======Loss: 2.9356878 ======\n",
      "=======Loss: 2.9124298 ======\n",
      "=======Loss: 2.948397 ======\n",
      "=======Loss: 2.9899452 ======\n",
      "=======Loss: 2.9077024 ======\n",
      "=======Loss: 2.7375767 ======\n",
      "=======Loss: 2.9744813 ======\n",
      "=======Loss: 2.8580756 ======\n",
      "=======Loss: 3.107595 ======\n",
      "=======Loss: 3.036346 ======\n",
      "=======Loss: 2.7160625 ======\n",
      "=======Loss: 2.8825126 ======\n",
      "=======Loss: 2.999227 ======\n",
      "=======Loss: 2.8009644 ======\n",
      "=======Loss: 2.7779105 ======\n",
      "=======Loss: 2.751712 ======\n",
      "=======Loss: 2.814637 ======\n",
      "=======Loss: 2.7066574 ======\n",
      "=======Loss: 2.8144126 ======\n",
      "=======Loss: 2.996666 ======\n",
      "=======Loss: 2.909809 ======\n",
      "=======Loss: 2.691784 ======\n",
      "=======Loss: 2.979682 ======\n",
      "=======Loss: 2.8452764 ======\n",
      "=======Loss: 2.7215347 ======\n",
      "=======Loss: 2.8226345 ======\n",
      "=======Loss: 2.9406705 ======\n",
      "=======Loss: 2.8240118 ======\n",
      "=======Loss: 2.7563088 ======\n",
      "=======Loss: 2.789011 ======\n",
      "=======Loss: 2.8665657 ======\n",
      "=======Loss: 2.7262669 ======\n",
      "=======Loss: 2.863919 ======\n",
      "=======Loss: 2.8172946 ======\n",
      "=======Loss: 2.8767552 ======\n",
      "=======Loss: 2.6709437 ======\n",
      "=======Loss: 2.8424432 ======\n",
      "=======Loss: 2.967607 ======\n",
      "=======Loss: 2.7078025 ======\n",
      "=======Loss: 2.6836011 ======\n",
      "=======Loss: 2.6529994 ======\n",
      "=======Loss: 2.6606205 ======\n",
      "=======Loss: 2.7632613 ======\n",
      "=======Loss: 2.7236395 ======\n",
      "=======Loss: 2.6956415 ======\n",
      "=======Loss: 2.8216457 ======\n",
      "=======Loss: 2.729793 ======\n",
      "=======Loss: 2.7359405 ======\n",
      "=======Loss: 2.9319086 ======\n",
      "=======Loss: 2.9796827 ======\n",
      "=======Loss: 2.9877245 ======\n",
      "=======Loss: 2.8265157 ======\n",
      "=======Loss: 2.7811332 ======\n",
      "=======Loss: 2.826128 ======\n",
      "=======Loss: 2.8918967 ======\n",
      "=======Loss: 2.834624 ======\n",
      "=======Loss: 2.8219128 ======\n",
      "=======Loss: 3.0792377 ======\n",
      "=======Loss: 2.6463823 ======\n",
      "=======Loss: 2.946979 ======\n",
      "=======Loss: 2.5774965 ======\n",
      "=======Loss: 2.750331 ======\n",
      "=======Loss: 2.529148 ======\n",
      "=======Loss: 2.4290802 ======\n",
      "=======Loss: 2.8261585 ======\n",
      "=======Loss: 2.853819 ======\n",
      "=======Loss: 2.7968318 ======\n",
      "=======Loss: 2.719644 ======\n",
      "=======Loss: 2.8418508 ======\n",
      "=======Loss: 2.6687348 ======\n",
      "=======Loss: 2.6450756 ======\n",
      "=======Loss: 2.6587377 ======\n",
      "=======Loss: 2.6867685 ======\n",
      "=======Loss: 2.7251658 ======\n",
      "=======Loss: 2.7861342 ======\n",
      "=======Loss: 2.6143618 ======\n",
      "=======Loss: 2.7298012 ======\n",
      "=======Loss: 2.9091303 ======\n",
      "=======Loss: 2.6142192 ======\n",
      "=======Loss: 2.7728953 ======\n",
      "=======Loss: 2.6222172 ======\n",
      "=======Loss: 2.8094182 ======\n",
      "=======Loss: 2.558116 ======\n",
      "=======Loss: 2.66458 ======\n",
      "=======Loss: 2.6730504 ======\n",
      "=======Loss: 2.5174072 ======\n",
      "=======Loss: 2.5549607 ======\n",
      "=======Loss: 2.70056 ======\n",
      "=======Loss: 2.583241 ======\n",
      "=======Loss: 2.825571 ======\n",
      "=======Loss: 2.62683 ======\n",
      "=======Loss: 2.5927987 ======\n",
      "=======Loss: 2.804257 ======\n",
      "=======Loss: 2.619659 ======\n",
      "=======Loss: 2.616374 ======\n",
      "=======Loss: 2.8082023 ======\n",
      "=======Loss: 2.484009 ======\n",
      "=======Loss: 2.5979161 ======\n",
      "=======Loss: 2.6268926 ======\n",
      "=======Loss: 2.5949893 ======\n",
      "=======Loss: 2.5408158 ======\n",
      "=======Loss: 2.5193183 ======\n",
      "=======Loss: 2.6350436 ======\n",
      "=======Loss: 2.6515832 ======\n",
      "=======Loss: 2.6660037 ======\n",
      "=======Loss: 2.7868328 ======\n",
      "=======Loss: 2.7121162 ======\n",
      "=======Loss: 2.497703 ======\n",
      "=======Loss: 2.8033776 ======\n",
      "=======Loss: 2.7279825 ======\n",
      "=======Loss: 2.6338608 ======\n",
      "=======Loss: 2.601225 ======\n",
      "=======Loss: 2.5593066 ======\n",
      "=======Loss: 2.52322 ======\n",
      "=======Loss: 2.5147262 ======\n",
      "=======Loss: 2.4249306 ======\n",
      "=======Loss: 2.6044369 ======\n",
      "=======Loss: 2.7362726 ======\n",
      "=======Loss: 2.463538 ======\n",
      "=======Loss: 2.6480293 ======\n",
      "=======Loss: 2.5061388 ======\n",
      "=======Loss: 2.6370168 ======\n",
      "=======Loss: 2.6394238 ======\n",
      "=======Loss: 2.4578426 ======\n",
      "=======Loss: 2.4684148 ======\n",
      "=======Loss: 2.4236007 ======\n",
      "=======Loss: 2.6854491 ======\n",
      "=======Loss: 2.5115337 ======\n",
      "=======Loss: 2.377387 ======\n",
      "=======Loss: 2.3986018 ======\n",
      "=======Loss: 2.6407478 ======\n",
      "=======Loss: 2.5443747 ======\n",
      "=======Loss: 2.477172 ======\n",
      "=======Loss: 2.6054778 ======\n",
      "=======Loss: 2.5147028 ======\n",
      "=======Loss: 2.4585083 ======\n",
      "=======Loss: 2.396352 ======\n",
      "=======Loss: 2.4972687 ======\n",
      "=======Loss: 2.5281708 ======\n",
      "=======Loss: 2.6108396 ======\n",
      "=======Loss: 2.5635138 ======\n",
      "=======Loss: 2.5421023 ======\n",
      "=======Loss: 2.6809292 ======\n",
      "=======Loss: 2.3851295 ======\n",
      "=======Loss: 2.5067005 ======\n",
      "=======Loss: 2.4640772 ======\n",
      "=======Loss: 2.4308887 ======\n",
      "=======Loss: 2.4897244 ======\n",
      "=======Loss: 2.5444984 ======\n",
      "=======Loss: 2.5867434 ======\n",
      "=======Loss: 2.6129804 ======\n",
      "=======Loss: 2.396129 ======\n",
      "=======Loss: 2.516102 ======\n",
      "=======Loss: 2.4162583 ======\n",
      "=======Loss: 2.5084825 ======\n",
      "=======Loss: 2.631998 ======\n",
      "=======Loss: 2.4383652 ======\n",
      "=======Loss: 2.7043443 ======\n",
      "=======Loss: 2.580786 ======\n",
      "=======Loss: 2.5147047 ======\n",
      "=======Loss: 2.3786502 ======\n",
      "=======Loss: 2.3525746 ======\n",
      "=======Loss: 2.6429722 ======\n",
      "=======Loss: 2.4119706 ======\n",
      "=======Loss: 2.5371156 ======\n",
      "=======Loss: 2.3201854 ======\n",
      "=======Loss: 2.4914012 ======\n",
      "=======Loss: 2.3017838 ======\n",
      "=======Loss: 2.5742476 ======\n",
      "=======Loss: 2.3864293 ======\n",
      "=======Loss: 2.5141673 ======\n",
      "=======Loss: 2.7117195 ======\n",
      "=======Loss: 2.442968 ======\n",
      "=======Loss: 2.6226532 ======\n",
      "=======Loss: 2.4502716 ======\n",
      "=======Loss: 2.445374 ======\n",
      "=======Loss: 2.411727 ======\n",
      "=======Loss: 2.5515623 ======\n",
      "=======Loss: 2.5605812 ======\n",
      "=======Loss: 2.460288 ======\n",
      "=======Loss: 2.5478158 ======\n",
      "=======Loss: 2.4726892 ======\n",
      "=======Loss: 2.5588858 ======\n",
      "=======Loss: 2.6362476 ======\n",
      "=======Loss: 2.4892864 ======\n",
      "=======Loss: 2.48064 ======\n",
      "=======Loss: 2.4419782 ======\n",
      "=======Loss: 2.533356 ======\n",
      "=======Loss: 2.4135623 ======\n",
      "=======Loss: 2.424689 ======\n",
      "=======Loss: 2.4020483 ======\n",
      "=======Loss: 2.370508 ======\n",
      "=======Loss: 2.5002933 ======\n",
      "=======Loss: 2.519052 ======\n",
      "=======Loss: 2.7088497 ======\n",
      "=======Loss: 2.4596667 ======\n",
      "=======Loss: 2.51853 ======\n",
      "=======Loss: 2.425911 ======\n",
      "=======Loss: 2.3598757 ======\n",
      "=======Loss: 2.5204525 ======\n",
      "=======Loss: 2.4836853 ======\n",
      "=======Loss: 2.5117164 ======\n",
      "=======Loss: 2.532712 ======\n",
      "=======Loss: 2.4073906 ======\n",
      "=======Loss: 2.460134 ======\n",
      "=======Loss: 2.5587487 ======\n",
      "=======Loss: 2.5051408 ======\n",
      "=======Loss: 2.3886244 ======\n",
      "=======Loss: 2.4641738 ======\n",
      "=======Loss: 2.558916 ======\n",
      "=======Loss: 2.4483337 ======\n",
      "=======Loss: 2.3702211 ======\n",
      "=======Loss: 2.4254513 ======\n",
      "=======Loss: 2.468565 ======\n",
      "=======Loss: 2.1770792 ======\n",
      "=======Loss: 2.4060216 ======\n",
      "=======Loss: 2.475619 ======\n",
      "=======Loss: 2.5053477 ======\n",
      "=======Loss: 2.3681397 ======\n",
      "=======Loss: 2.3565621 ======\n",
      "=======Loss: 2.3493438 ======\n",
      "=======Loss: 2.3764553 ======\n",
      "=======Loss: 2.3381593 ======\n",
      "=======Loss: 2.3581247 ======\n",
      "=======Loss: 2.4397388 ======\n",
      "=======Loss: 2.3977497 ======\n",
      "=======Loss: 2.2669654 ======\n",
      "=======Loss: 2.4151928 ======\n",
      "=======Loss: 2.374404 ======\n",
      "=======Loss: 2.4158623 ======\n",
      "=======Loss: 2.3960977 ======\n",
      "=======Loss: 2.2701118 ======\n",
      "=======Loss: 2.3007724 ======\n",
      "=======Loss: 2.4442105 ======\n",
      "=======Loss: 2.4028583 ======\n",
      "=======Loss: 2.3592172 ======\n",
      "=======Loss: 2.4434698 ======\n",
      "=======Loss: 2.3825307 ======\n",
      "=======Loss: 2.3332462 ======\n",
      "=======Loss: 2.3972178 ======\n",
      "=======Loss: 2.1626396 ======\n",
      "=======Loss: 2.2091458 ======\n",
      "=======Loss: 2.330134 ======\n",
      "=======Loss: 2.308534 ======\n",
      "=======Loss: 2.2823274 ======\n",
      "=======Loss: 2.37894 ======\n",
      "=======Loss: 2.3411355 ======\n",
      "=======Loss: 2.4146943 ======\n",
      "=======Loss: 2.3363974 ======\n",
      "=======Loss: 2.3258495 ======\n",
      "=======Loss: 2.4193945 ======\n",
      "=======Loss: 2.3725808 ======\n",
      "=======Loss: 2.3814006 ======\n",
      "=======Loss: 2.3917894 ======\n",
      "=======Loss: 2.309618 ======\n",
      "=======Loss: 2.2700765 ======\n",
      "=======Loss: 2.2706127 ======\n",
      "=======Loss: 2.3900795 ======\n",
      "=======Loss: 2.3092234 ======\n",
      "=======Loss: 2.3585587 ======\n",
      "=======Loss: 2.2438345 ======\n",
      "=======Loss: 2.3799458 ======\n",
      "=======Loss: 2.4883928 ======\n",
      "=======Loss: 2.3117158 ======\n",
      "=======Loss: 2.2586234 ======\n",
      "=======Loss: 2.22472 ======\n",
      "=======Loss: 2.3728533 ======\n",
      "=======Loss: 2.2864406 ======\n",
      "=======Loss: 2.2993739 ======\n",
      "=======Loss: 2.3485284 ======\n",
      "=======Loss: 2.2361748 ======\n",
      "=======Loss: 2.2698636 ======\n",
      "=======Loss: 2.374867 ======\n",
      "=======Loss: 2.267969 ======\n",
      "=======Loss: 2.2106345 ======\n",
      "=======Loss: 2.226441 ======\n",
      "=======Loss: 2.2598639 ======\n",
      "=======Loss: 2.2290502 ======\n",
      "=======Loss: 2.3252757 ======\n",
      "=======Loss: 2.3604121 ======\n",
      "=======Loss: 2.2502098 ======\n",
      "=======Loss: 2.1776655 ======\n",
      "=======Loss: 2.260778 ======\n",
      "=======Loss: 2.216627 ======\n",
      "=======Loss: 2.144985 ======\n",
      "=======Loss: 2.3074155 ======\n",
      "=======Loss: 2.3452086 ======\n",
      "=======Loss: 2.2941785 ======\n",
      "=======Loss: 2.4263911 ======\n",
      "=======Loss: 2.3153553 ======\n",
      "=======Loss: 2.2769692 ======\n",
      "=======Loss: 2.2719903 ======\n",
      "=======Loss: 2.1428638 ======\n",
      "=======Loss: 2.0964723 ======\n",
      "=======Loss: 2.207821 ======\n",
      "=======Loss: 2.251047 ======\n",
      "=======Loss: 2.235818 ======\n",
      "=======Loss: 2.2747104 ======\n",
      "=======Loss: 2.279561 ======\n",
      "=======Loss: 2.2420626 ======\n",
      "=======Loss: 2.2385702 ======\n",
      "=======Loss: 2.160281 ======\n",
      "=======Loss: 2.309145 ======\n",
      "=======Loss: 2.1723967 ======\n",
      "=======Loss: 2.2689097 ======\n",
      "=======Loss: 2.2319376 ======\n",
      "=======Loss: 2.2870896 ======\n",
      "=======Loss: 2.0403383 ======\n",
      "=======Loss: 2.3500392 ======\n",
      "=======Loss: 2.1499834 ======\n",
      "=======Loss: 2.240277 ======\n",
      "=======Loss: 2.2973075 ======\n",
      "=======Loss: 2.1177735 ======\n",
      "=======Loss: 2.2499108 ======\n",
      "=======Loss: 2.2853987 ======\n",
      "=======Loss: 2.2039416 ======\n",
      "=======Loss: 2.1806812 ======\n",
      "=======Loss: 2.3147867 ======\n",
      "=======Loss: 2.2058218 ======\n",
      "=======Loss: 2.2460454 ======\n",
      "=======Loss: 2.42727 ======\n",
      "=======Loss: 2.3565402 ======\n",
      "=======Loss: 2.4044838 ======\n",
      "=======Loss: 2.257433 ======\n",
      "=======Loss: 2.2492137 ======\n",
      "=======Loss: 2.2801015 ======\n",
      "=======Loss: 2.240799 ======\n",
      "=======Loss: 2.2656708 ======\n",
      "=======Loss: 2.132282 ======\n",
      "=======Loss: 2.2037358 ======\n",
      "=======Loss: 2.2588623 ======\n",
      "=======Loss: 2.3641496 ======\n",
      "=======Loss: 2.1154857 ======\n",
      "=======Loss: 2.1289635 ======\n",
      "=======Loss: 2.2592576 ======\n",
      "=======Loss: 2.3558903 ======\n",
      "=======Loss: 2.1389835 ======\n",
      "=======Loss: 2.0843635 ======\n",
      "=======Loss: 2.1310577 ======\n",
      "=======Loss: 2.3652742 ======\n",
      "=======Loss: 2.1543102 ======\n",
      "=======Loss: 2.2514224 ======\n",
      "=======Loss: 2.3572512 ======\n",
      "=======Loss: 2.1571982 ======\n",
      "=======Loss: 2.2383802 ======\n",
      "=======Loss: 2.1418462 ======\n",
      "=======Loss: 2.3350806 ======\n",
      "=======Loss: 2.2731106 ======\n",
      "=======Loss: 2.3311324 ======\n",
      "=======Loss: 2.1212664 ======\n",
      "=======Loss: 2.221921 ======\n",
      "=======Loss: 2.2939277 ======\n",
      "=======Loss: 2.1478584 ======\n",
      "=======Loss: 2.3059964 ======\n",
      "=======Loss: 2.3970098 ======\n",
      "=======Loss: 2.3164978 ======\n",
      "=======Loss: 2.109039 ======\n",
      "=======Loss: 2.2987914 ======\n",
      "=======Loss: 2.4193525 ======\n",
      "=======Loss: 2.4578106 ======\n",
      "=======Loss: 2.244742 ======\n",
      "=======Loss: 2.1793375 ======\n",
      "=======Loss: 2.1133132 ======\n",
      "=======Loss: 2.3133519 ======\n",
      "=======Loss: 2.2269828 ======\n",
      "=======Loss: 2.239966 ======\n",
      "=======Loss: 2.11812 ======\n",
      "=======Loss: 2.1156194 ======\n",
      "=======Loss: 2.2886906 ======\n",
      "=======Loss: 2.203663 ======\n",
      "=======Loss: 2.2102737 ======\n",
      "=======Loss: 2.305564 ======\n",
      "=======Loss: 2.213183 ======\n",
      "=======Loss: 2.2880282 ======\n",
      "=======Loss: 2.0923111 ======\n",
      "=======Loss: 2.1234946 ======\n",
      "=======Loss: 2.1194286 ======\n",
      "=======Loss: 2.1057405 ======\n",
      "=======Loss: 2.1807926 ======\n",
      "=======Loss: 2.141579 ======\n",
      "=======Loss: 2.1566858 ======\n",
      "=======Loss: 2.1884506 ======\n",
      "=======Loss: 2.19333 ======\n",
      "=======Loss: 2.1752524 ======\n",
      "=======Loss: 2.2004654 ======\n",
      "=======Loss: 2.1525254 ======\n",
      "=======Loss: 2.1179204 ======\n",
      "=======Loss: 2.273511 ======\n",
      "=======Loss: 2.1891403 ======\n",
      "=======Loss: 2.0789368 ======\n",
      "=======Loss: 2.141659 ======\n",
      "=======Loss: 2.1706882 ======\n",
      "=======Loss: 1.990808 ======\n",
      "=======Loss: 2.219223 ======\n",
      "=======Loss: 2.248245 ======\n",
      "=======Loss: 2.1505191 ======\n",
      "=======Loss: 2.102034 ======\n",
      "=======Loss: 2.0012236 ======\n",
      "=======Loss: 2.0612254 ======\n",
      "=======Loss: 2.063663 ======\n",
      "=======Loss: 2.1023135 ======\n",
      "=======Loss: 2.092761 ======\n",
      "=======Loss: 2.0986376 ======\n",
      "=======Loss: 2.085802 ======\n",
      "=======Loss: 2.0240679 ======\n",
      "=======Loss: 2.2518044 ======\n",
      "=======Loss: 1.9983361 ======\n",
      "=======Loss: 2.1034539 ======\n",
      "=======Loss: 2.2567031 ======\n",
      "=======Loss: 2.271369 ======\n",
      "=======Loss: 2.0719101 ======\n",
      "=======Loss: 1.9723837 ======\n",
      "=======Loss: 2.0694485 ======\n",
      "=======Loss: 2.1032774 ======\n",
      "=======Loss: 2.1201932 ======\n",
      "=======Loss: 2.150601 ======\n",
      "=======Loss: 2.0173876 ======\n",
      "=======Loss: 2.1097531 ======\n",
      "=======Loss: 2.11687 ======\n",
      "=======Loss: 2.1859388 ======\n",
      "=======Loss: 2.036018 ======\n",
      "=======Loss: 2.0497437 ======\n",
      "=======Loss: 2.0950208 ======\n",
      "=======Loss: 2.0949206 ======\n",
      "=======Loss: 2.1037536 ======\n",
      "=======Loss: 2.0050592 ======\n",
      "=======Loss: 2.1574852 ======\n",
      "=======Loss: 2.0199754 ======\n",
      "=======Loss: 2.0684407 ======\n",
      "=======Loss: 2.0722895 ======\n",
      "=======Loss: 2.078394 ======\n",
      "=======Loss: 2.1106174 ======\n",
      "=======Loss: 2.1852455 ======\n",
      "=======Loss: 2.1009083 ======\n",
      "=======Loss: 2.217168 ======\n",
      "=======Loss: 2.1047199 ======\n",
      "=======Loss: 2.0323558 ======\n",
      "=======Loss: 2.074549 ======\n",
      "=======Loss: 1.9949206 ======\n",
      "=======Loss: 2.20047 ======\n",
      "=======Loss: 2.202289 ======\n",
      "=======Loss: 2.0330188 ======\n",
      "=======Loss: 2.0463736 ======\n",
      "=======Loss: 2.2216005 ======\n",
      "=======Loss: 2.1723452 ======\n",
      "=======Loss: 2.1275125 ======\n",
      "=======Loss: 2.1736772 ======\n",
      "=======Loss: 2.146525 ======\n",
      "=======Loss: 2.0588243 ======\n",
      "=======Loss: 2.1281996 ======\n",
      "=======Loss: 2.0924509 ======\n",
      "=======Loss: 2.0382042 ======\n",
      "=======Loss: 2.0858972 ======\n",
      "=======Loss: 2.087744 ======\n",
      "=======Loss: 2.0467985 ======\n",
      "=======Loss: 2.063413 ======\n",
      "=======Loss: 2.0638318 ======\n",
      "=======Loss: 2.0243816 ======\n",
      "=======Loss: 2.2265706 ======\n",
      "=======Loss: 2.0964646 ======\n",
      "=======Loss: 2.0099638 ======\n",
      "=======Loss: 2.0020244 ======\n",
      "=======Loss: 2.0588548 ======\n",
      "=======Loss: 2.0134442 ======\n",
      "=======Loss: 2.0361931 ======\n",
      "=======Loss: 2.1463554 ======\n",
      "=======Loss: 2.1741009 ======\n",
      "=======Loss: 2.0169618 ======\n",
      "=======Loss: 2.0289536 ======\n",
      "=======Loss: 2.096405 ======\n",
      "=======Loss: 2.0811343 ======\n",
      "=======Loss: 2.026568 ======\n",
      "=======Loss: 1.9263532 ======\n",
      "=======Loss: 1.9922638 ======\n",
      "=======Loss: 2.0401676 ======\n",
      "=======Loss: 2.0108755 ======\n",
      "=======Loss: 2.1321695 ======\n",
      "=======Loss: 2.0900865 ======\n",
      "=======Loss: 1.9908137 ======\n",
      "=======Loss: 2.0731525 ======\n",
      "=======Loss: 1.877337 ======\n",
      "=======Loss: 2.1022289 ======\n",
      "=======Loss: 2.214868 ======\n",
      "=======Loss: 2.0778768 ======\n",
      "=======Loss: 1.9561965 ======\n",
      "=======Loss: 1.9867563 ======\n",
      "=======Loss: 2.139308 ======\n",
      "=======Loss: 2.2312086 ======\n",
      "=======Loss: 2.031931 ======\n",
      "=======Loss: 1.9795163 ======\n",
      "=======Loss: 2.1262379 ======\n",
      "=======Loss: 2.1808977 ======\n",
      "=======Loss: 2.147817 ======\n",
      "=======Loss: 1.9110901 ======\n",
      "=======Loss: 2.1865811 ======\n",
      "=======Loss: 2.1596086 ======\n",
      "=======Loss: 2.124508 ======\n",
      "=======Loss: 2.0639954 ======\n",
      "=======Loss: 1.9401385 ======\n",
      "=======Loss: 2.085039 ======\n",
      "=======Loss: 2.1658108 ======\n",
      "=======Loss: 1.9154975 ======\n",
      "=======Loss: 2.0757391 ======\n",
      "=======Loss: 2.0609066 ======\n",
      "=======Loss: 2.0194995 ======\n",
      "=======Loss: 2.102452 ======\n",
      "=======Loss: 2.081069 ======\n",
      "=======Loss: 2.0320296 ======\n",
      "=======Loss: 2.138588 ======\n",
      "=======Loss: 2.1473818 ======\n",
      "=======Loss: 2.0558903 ======\n",
      "=======Loss: 2.1157908 ======\n",
      "=======Loss: 2.098864 ======\n",
      "=======Loss: 2.0626981 ======\n",
      "=======Loss: 2.0111146 ======\n",
      "=======Loss: 2.0217028 ======\n",
      "=======Loss: 1.987818 ======\n",
      "=======Loss: 2.0128381 ======\n",
      "=======Loss: 1.9769609 ======\n",
      "=======Loss: 2.02523 ======\n",
      "=======Loss: 1.9645646 ======\n",
      "=======Loss: 1.9589093 ======\n",
      "=======Loss: 2.0728464 ======\n",
      "=======Loss: 1.9157801 ======\n",
      "=======Loss: 2.1279109 ======\n",
      "=======Loss: 2.176397 ======\n",
      "=======Loss: 1.9799716 ======\n",
      "=======Loss: 2.1242027 ======\n",
      "=======Loss: 2.0043836 ======\n",
      "=======Loss: 2.000923 ======\n",
      "=======Loss: 2.28321 ======\n",
      "=======Loss: 2.05602 ======\n"
     ]
    }
   ],
   "source": [
    "n_params = count_model_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# %%\n",
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=8e-4, \n",
    "                                weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2, reduce_dims=(0,1))\n",
    "# h1loss = H1Loss(d=2, reduce_dims=(0,1))\n",
    "\n",
    "train_loss = l2loss\n",
    "eval_losses={'l2': l2loss} #'h1': h1loss, \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()\n",
    "\n",
    "step = 0\n",
    "\n",
    "with open('script/sfno_loss.txt', 'w') as f:\n",
    "    for epoch in range(20):\n",
    "        avg_loss = 0\n",
    "        train_err = 0.0\n",
    "        \n",
    "        # track number of training examples in batch\n",
    "        n_samples = 0\n",
    "        for idx, sample in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            sample = {\n",
    "                k: v.to(device)\n",
    "                for k, v in sample.items()\n",
    "                if torch.is_tensor(v)\n",
    "            }\n",
    "\n",
    "            n_samples += sample[\"y\"].shape[0]\n",
    "            out = model(sample[\"x\"])\n",
    "\n",
    "            loss = l2loss(out, **sample)\n",
    "\n",
    "            loss.backward()\n",
    "            del out\n",
    "\n",
    "            optimizer.step()\n",
    "            train_err += loss.item()\n",
    "            with torch.no_grad():\n",
    "                print(\"=======Loss:\",loss.detach().cpu().numpy(),\"======\")\n",
    "                f.write(f'Step {step + 1}, Loss: {loss.item()}\\n')\n",
    "                step += 1\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_err)\n",
    "        else:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (32, 64)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (64, 128) with 50 samples and batch-size=1\n",
      "Test Loss: 1.0431 ± 0.0477\n"
     ]
    }
   ],
   "source": [
    "resolution = (64, 128)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (128, 256) with 50 samples and batch-size=1\n",
      "Test Loss: 2.3513 ± 0.0391\n"
     ]
    }
   ],
   "source": [
    "resolution = (128, 256)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time: 0.00400090217590332\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "outputs = model(inputs)\n",
    "print(\"test time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "    # Input x\n",
    "    x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "    # Ground-truth\n",
    "    y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "    # Model prediction\n",
    "    x_in = x.unsqueeze(0).to(device)\n",
    "    out = model(x_in).squeeze()[0, ...].detach().cpu().numpy()\n",
    "    x = x[0, ...].detach().numpy()\n",
    "\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"./script/output_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "#     # Input x\n",
    "#     x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "#     # Ground-truth\n",
    "#     y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "#     x = x[0, ...].detach().numpy()\n",
    "    \n",
    "#     plt.imshow(x)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/input_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    \n",
    "#     plt.imshow(y)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/label_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (128, 256) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (128, 256) with 50 samples and batch-size=1\n"
     ]
    }
   ],
   "source": [
    "resolution = (128, 256)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=resolution,\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "loss_low = 1e5\n",
    "loss_high = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        if loss > loss_high:\n",
    "            loss_high = loss\n",
    "            target_high = targets.squeeze()[0, ...].detach().cpu().numpy()\n",
    "            output_high = outputs.squeeze()[0, ...].detach().cpu().numpy()\n",
    "        if loss < loss_low:\n",
    "            loss_low = loss\n",
    "            target_low = targets.squeeze()[0, ...].detach().cpu().numpy()\n",
    "            output_low = outputs.squeeze()[0, ...].detach().cpu().numpy()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "plt.imshow(target_high)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/target_high.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(output_high)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/output_high.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(target_low)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/target_low.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(output_low)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/output_low.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
