{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Fourier Neural Operators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_harmonics.examples.sfno import SphericalFourierNeuralOperatorNet as SFNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.datasets import load_spherical_swe\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %%\n",
    "# Loading the Navier-Stokes dataset in 128x128 resolution\n",
    "resolution = (32, 64)\n",
    "train_loader, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=resolution,\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[10])\n",
    "\n",
    "model = SFNO(spectral_transform='sht', img_size=resolution, grid=\"equiangular\",\n",
    "             scale_factor=3, embed_dim=32, big_skip=True, pos_embed=\"lat\").to(device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 99913 parameters.\n",
      "\n",
      "### MODEL ###\n",
      " SphericalFourierNeuralOperatorNet(\n",
      "  (pos_drop): Identity()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (trans_down): RealSHT(\n",
      "    nlat=32, nlon=64,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=equiangular, csphase=True\n",
      "  )\n",
      "  (itrans_up): InverseRealSHT(\n",
      "    nlat=32, nlon=64,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=equiangular, csphase=True\n",
      "  )\n",
      "  (trans): RealSHT(\n",
      "    nlat=10, nlon=21,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=legendre-gauss, csphase=True\n",
      "  )\n",
      "  (itrans): InverseRealSHT(\n",
      "    nlat=10, nlon=21,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=legendre-gauss, csphase=True\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): SphericalFourierNeuralOperatorBlock(\n",
      "      (filter): SpectralFilterLayer(\n",
      "        (filter): SpectralConvS2(\n",
      "          (forward_transform): RealSHT(\n",
      "            nlat=32, nlon=64,\n",
      "             lmax=10, mmax=10,\n",
      "             grid=equiangular, csphase=True\n",
      "          )\n",
      "          (inverse_transform): InverseRealSHT(\n",
      "            nlat=10, nlon=21,\n",
      "             lmax=10, mmax=10,\n",
      "             grid=legendre-gauss, csphase=True\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (act_layer): ReLU()\n",
      "      (norm0): Identity()\n",
      "      (drop_path): Identity()\n",
      "      (mlp): MLP(\n",
      "        (fwd): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (outer_skip): Identity()\n",
      "      (norm1): Identity()\n",
      "    )\n",
      "    (1-2): 2 x SphericalFourierNeuralOperatorBlock(\n",
      "      (filter): SpectralFilterLayer(\n",
      "        (filter): SpectralConvS2(\n",
      "          (forward_transform): RealSHT(\n",
      "            nlat=10, nlon=21,\n",
      "             lmax=10, mmax=10,\n",
      "             grid=legendre-gauss, csphase=True\n",
      "          )\n",
      "          (inverse_transform): InverseRealSHT(\n",
      "            nlat=10, nlon=21,\n",
      "             lmax=10, mmax=10,\n",
      "             grid=legendre-gauss, csphase=True\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (act_layer): ReLU()\n",
      "      (norm0): Identity()\n",
      "      (drop_path): Identity()\n",
      "      (mlp): MLP(\n",
      "        (fwd): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (outer_skip): Identity()\n",
      "      (norm1): Identity()\n",
      "    )\n",
      "    (3): SphericalFourierNeuralOperatorBlock(\n",
      "      (filter): SpectralFilterLayer(\n",
      "        (filter): SpectralConvS2(\n",
      "          (forward_transform): RealSHT(\n",
      "            nlat=10, nlon=21,\n",
      "             lmax=10, mmax=10,\n",
      "             grid=legendre-gauss, csphase=True\n",
      "          )\n",
      "          (inverse_transform): InverseRealSHT(\n",
      "            nlat=32, nlon=64,\n",
      "             lmax=10, mmax=10,\n",
      "             grid=equiangular, csphase=True\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (act_layer): ReLU()\n",
      "      (norm0): Identity()\n",
      "      (drop_path): Identity()\n",
      "      (mlp): MLP(\n",
      "        (fwd): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (outer_skip): Identity()\n",
      "      (norm1): Identity()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv2d(35, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0008\n",
      "    lr: 0.0008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x0000022C859ADF10>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.LpLoss object at 0x0000022C9A89C850>\n",
      "\n",
      " * Test: {'l2': <neuralop.losses.data_losses.LpLoss object at 0x0000022C9A89C850>}\n",
      "=======Loss: 30.308283 ======\n",
      "=======Loss: 22.357563 ======\n",
      "=======Loss: 17.699978 ======\n",
      "=======Loss: 16.589249 ======\n",
      "=======Loss: 18.146564 ======\n",
      "=======Loss: 18.978016 ======\n",
      "=======Loss: 17.356197 ======\n",
      "=======Loss: 16.985863 ======\n",
      "=======Loss: 14.432426 ======\n",
      "=======Loss: 14.419553 ======\n",
      "=======Loss: 14.209379 ======\n",
      "=======Loss: 14.529566 ======\n",
      "=======Loss: 14.508084 ======\n",
      "=======Loss: 13.603334 ======\n",
      "=======Loss: 13.023333 ======\n",
      "=======Loss: 12.872772 ======\n",
      "=======Loss: 12.660445 ======\n",
      "=======Loss: 12.660109 ======\n",
      "=======Loss: 11.998642 ======\n",
      "=======Loss: 11.380592 ======\n",
      "=======Loss: 11.83193 ======\n",
      "=======Loss: 11.280254 ======\n",
      "=======Loss: 11.349588 ======\n",
      "=======Loss: 10.784309 ======\n",
      "=======Loss: 10.693838 ======\n",
      "=======Loss: 10.8754 ======\n",
      "=======Loss: 10.167263 ======\n",
      "=======Loss: 10.825105 ======\n",
      "=======Loss: 10.406542 ======\n",
      "=======Loss: 10.323715 ======\n",
      "=======Loss: 10.274357 ======\n",
      "=======Loss: 10.245778 ======\n",
      "=======Loss: 9.791176 ======\n",
      "=======Loss: 10.134451 ======\n",
      "=======Loss: 9.820864 ======\n",
      "=======Loss: 9.498354 ======\n",
      "=======Loss: 9.467069 ======\n",
      "=======Loss: 9.949147 ======\n",
      "=======Loss: 9.294768 ======\n",
      "=======Loss: 9.482679 ======\n",
      "=======Loss: 9.639263 ======\n",
      "=======Loss: 9.35607 ======\n",
      "=======Loss: 9.4036255 ======\n",
      "=======Loss: 9.130402 ======\n",
      "=======Loss: 8.791197 ======\n",
      "=======Loss: 8.902473 ======\n",
      "=======Loss: 8.825071 ======\n",
      "=======Loss: 8.782833 ======\n",
      "=======Loss: 8.732784 ======\n",
      "=======Loss: 8.249706 ======\n",
      "=======Loss: 8.426286 ======\n",
      "=======Loss: 8.257064 ======\n",
      "=======Loss: 7.968631 ======\n",
      "=======Loss: 7.9940405 ======\n",
      "=======Loss: 8.498528 ======\n",
      "=======Loss: 8.457989 ======\n",
      "=======Loss: 8.23382 ======\n",
      "=======Loss: 8.310654 ======\n",
      "=======Loss: 8.218683 ======\n",
      "=======Loss: 8.000518 ======\n",
      "=======Loss: 7.862733 ======\n",
      "=======Loss: 7.9240484 ======\n",
      "=======Loss: 7.7806773 ======\n",
      "=======Loss: 8.072199 ======\n",
      "=======Loss: 7.8822994 ======\n",
      "=======Loss: 7.836134 ======\n",
      "=======Loss: 7.807584 ======\n",
      "=======Loss: 7.529918 ======\n",
      "=======Loss: 7.823553 ======\n",
      "=======Loss: 7.8964677 ======\n",
      "=======Loss: 7.474341 ======\n",
      "=======Loss: 7.5588903 ======\n",
      "=======Loss: 7.793565 ======\n",
      "=======Loss: 7.672635 ======\n",
      "=======Loss: 7.4798775 ======\n",
      "=======Loss: 7.5624304 ======\n",
      "=======Loss: 7.5824356 ======\n",
      "=======Loss: 7.1639233 ======\n",
      "=======Loss: 7.399111 ======\n",
      "=======Loss: 7.3938694 ======\n",
      "=======Loss: 6.9700775 ======\n",
      "=======Loss: 7.2167444 ======\n",
      "=======Loss: 7.391798 ======\n",
      "=======Loss: 7.1005454 ======\n",
      "=======Loss: 7.141364 ======\n",
      "=======Loss: 7.382862 ======\n",
      "=======Loss: 7.112158 ======\n",
      "=======Loss: 6.769285 ======\n",
      "=======Loss: 7.24864 ======\n",
      "=======Loss: 6.913488 ======\n",
      "=======Loss: 6.8081717 ======\n",
      "=======Loss: 6.8898716 ======\n",
      "=======Loss: 6.892906 ======\n",
      "=======Loss: 7.132581 ======\n",
      "=======Loss: 6.7675133 ======\n",
      "=======Loss: 6.8460855 ======\n",
      "=======Loss: 6.9684668 ======\n",
      "=======Loss: 6.9488707 ======\n",
      "=======Loss: 6.871383 ======\n",
      "=======Loss: 6.718143 ======\n",
      "=======Loss: 6.697933 ======\n",
      "=======Loss: 6.7935104 ======\n",
      "=======Loss: 6.810569 ======\n",
      "=======Loss: 6.7994323 ======\n",
      "=======Loss: 6.709652 ======\n",
      "=======Loss: 6.55492 ======\n",
      "=======Loss: 6.7181296 ======\n",
      "=======Loss: 6.3901124 ======\n",
      "=======Loss: 6.2501116 ======\n",
      "=======Loss: 6.488901 ======\n",
      "=======Loss: 6.7056704 ======\n",
      "=======Loss: 6.4193697 ======\n",
      "=======Loss: 6.857031 ======\n",
      "=======Loss: 6.3315334 ======\n",
      "=======Loss: 6.6894083 ======\n",
      "=======Loss: 6.341835 ======\n",
      "=======Loss: 6.3080025 ======\n",
      "=======Loss: 6.5119123 ======\n",
      "=======Loss: 6.45827 ======\n",
      "=======Loss: 6.5040803 ======\n",
      "=======Loss: 6.3165903 ======\n",
      "=======Loss: 6.108097 ======\n",
      "=======Loss: 6.1933107 ======\n",
      "=======Loss: 6.3106813 ======\n",
      "=======Loss: 6.1651273 ======\n",
      "=======Loss: 6.4635177 ======\n",
      "=======Loss: 6.474983 ======\n",
      "=======Loss: 6.229245 ======\n",
      "=======Loss: 6.2939253 ======\n",
      "=======Loss: 6.3830338 ======\n",
      "=======Loss: 6.006409 ======\n",
      "=======Loss: 6.1833572 ======\n",
      "=======Loss: 6.4101524 ======\n",
      "=======Loss: 6.2913847 ======\n",
      "=======Loss: 6.3361473 ======\n",
      "=======Loss: 6.1214952 ======\n",
      "=======Loss: 6.00428 ======\n",
      "=======Loss: 6.1031976 ======\n",
      "=======Loss: 6.243454 ======\n",
      "=======Loss: 5.910693 ======\n",
      "=======Loss: 5.973422 ======\n",
      "=======Loss: 5.852517 ======\n",
      "=======Loss: 6.4078674 ======\n",
      "=======Loss: 5.8649926 ======\n",
      "=======Loss: 6.0743217 ======\n",
      "=======Loss: 5.8682566 ======\n",
      "=======Loss: 5.863514 ======\n",
      "=======Loss: 5.9591813 ======\n",
      "=======Loss: 5.7641697 ======\n",
      "=======Loss: 5.969841 ======\n",
      "=======Loss: 5.84966 ======\n",
      "=======Loss: 6.017089 ======\n",
      "=======Loss: 6.0262003 ======\n",
      "=======Loss: 6.0613804 ======\n",
      "=======Loss: 6.067483 ======\n",
      "=======Loss: 6.322102 ======\n",
      "=======Loss: 6.0798073 ======\n",
      "=======Loss: 6.0059843 ======\n",
      "=======Loss: 5.7208953 ======\n",
      "=======Loss: 5.670267 ======\n",
      "=======Loss: 5.863391 ======\n",
      "=======Loss: 5.888833 ======\n",
      "=======Loss: 5.9539127 ======\n",
      "=======Loss: 5.9030447 ======\n",
      "=======Loss: 5.797965 ======\n",
      "=======Loss: 5.768059 ======\n",
      "=======Loss: 5.9025803 ======\n",
      "=======Loss: 5.8008986 ======\n",
      "=======Loss: 5.89377 ======\n",
      "=======Loss: 5.6759644 ======\n",
      "=======Loss: 5.6107974 ======\n",
      "=======Loss: 5.8281116 ======\n",
      "=======Loss: 5.688365 ======\n",
      "=======Loss: 5.6656322 ======\n",
      "=======Loss: 5.502047 ======\n",
      "=======Loss: 5.542526 ======\n",
      "=======Loss: 5.7828665 ======\n",
      "=======Loss: 5.77009 ======\n",
      "=======Loss: 6.0584903 ======\n",
      "=======Loss: 5.753952 ======\n",
      "=======Loss: 5.848962 ======\n",
      "=======Loss: 5.88785 ======\n",
      "=======Loss: 5.793955 ======\n",
      "=======Loss: 5.581825 ======\n",
      "=======Loss: 5.419465 ======\n",
      "=======Loss: 5.7335057 ======\n",
      "=======Loss: 5.997778 ======\n",
      "=======Loss: 5.666672 ======\n",
      "=======Loss: 5.529916 ======\n",
      "=======Loss: 5.648834 ======\n",
      "=======Loss: 5.8718586 ======\n",
      "=======Loss: 5.530576 ======\n",
      "=======Loss: 5.782139 ======\n",
      "=======Loss: 5.4247527 ======\n",
      "=======Loss: 5.6850634 ======\n",
      "=======Loss: 5.3316426 ======\n",
      "=======Loss: 5.846837 ======\n",
      "=======Loss: 5.662281 ======\n",
      "=======Loss: 5.699107 ======\n",
      "=======Loss: 5.483465 ======\n",
      "=======Loss: 5.4768896 ======\n",
      "=======Loss: 5.6271453 ======\n",
      "=======Loss: 5.8131547 ======\n",
      "=======Loss: 5.9392776 ======\n",
      "=======Loss: 5.6476893 ======\n",
      "=======Loss: 5.6606574 ======\n",
      "=======Loss: 5.613664 ======\n",
      "=======Loss: 5.6248136 ======\n",
      "=======Loss: 5.4750547 ======\n",
      "=======Loss: 5.4915934 ======\n",
      "=======Loss: 5.376752 ======\n",
      "=======Loss: 5.433764 ======\n",
      "=======Loss: 5.4346313 ======\n",
      "=======Loss: 5.3765717 ======\n",
      "=======Loss: 5.528648 ======\n",
      "=======Loss: 5.6938896 ======\n",
      "=======Loss: 5.5293655 ======\n",
      "=======Loss: 5.2632475 ======\n",
      "=======Loss: 5.50434 ======\n",
      "=======Loss: 5.630621 ======\n",
      "=======Loss: 5.390635 ======\n",
      "=======Loss: 5.452571 ======\n",
      "=======Loss: 5.7221828 ======\n",
      "=======Loss: 5.438654 ======\n",
      "=======Loss: 5.2686715 ======\n",
      "=======Loss: 5.5981445 ======\n",
      "=======Loss: 5.368103 ======\n",
      "=======Loss: 5.3531876 ======\n",
      "=======Loss: 5.872882 ======\n",
      "=======Loss: 5.8279414 ======\n",
      "=======Loss: 5.183237 ======\n",
      "=======Loss: 5.5349364 ======\n",
      "=======Loss: 5.607774 ======\n",
      "=======Loss: 5.5472007 ======\n",
      "=======Loss: 5.4417996 ======\n",
      "=======Loss: 5.4916387 ======\n",
      "=======Loss: 5.4230328 ======\n",
      "=======Loss: 5.5137224 ======\n",
      "=======Loss: 5.3542085 ======\n",
      "=======Loss: 5.441375 ======\n",
      "=======Loss: 5.020915 ======\n",
      "=======Loss: 5.5688324 ======\n",
      "=======Loss: 5.631428 ======\n",
      "=======Loss: 5.4444814 ======\n",
      "=======Loss: 5.2953706 ======\n",
      "=======Loss: 5.504519 ======\n",
      "=======Loss: 5.2348742 ======\n",
      "=======Loss: 5.3452044 ======\n",
      "=======Loss: 5.1734066 ======\n",
      "=======Loss: 5.391683 ======\n",
      "=======Loss: 5.151748 ======\n",
      "=======Loss: 5.0221906 ======\n",
      "=======Loss: 5.406099 ======\n",
      "=======Loss: 5.2197714 ======\n",
      "=======Loss: 5.198599 ======\n",
      "=======Loss: 5.157427 ======\n",
      "=======Loss: 5.0211067 ======\n",
      "=======Loss: 5.2721267 ======\n",
      "=======Loss: 5.044576 ======\n",
      "=======Loss: 5.4061465 ======\n",
      "=======Loss: 5.3262434 ======\n",
      "=======Loss: 5.347183 ======\n",
      "=======Loss: 5.4334173 ======\n",
      "=======Loss: 5.035042 ======\n",
      "=======Loss: 5.2971325 ======\n",
      "=======Loss: 5.1012444 ======\n",
      "=======Loss: 5.3179526 ======\n",
      "=======Loss: 5.14517 ======\n",
      "=======Loss: 5.3747625 ======\n",
      "=======Loss: 5.0462885 ======\n",
      "=======Loss: 5.283487 ======\n",
      "=======Loss: 5.1413517 ======\n",
      "=======Loss: 4.9625134 ======\n",
      "=======Loss: 5.114219 ======\n",
      "=======Loss: 5.032969 ======\n",
      "=======Loss: 5.119895 ======\n",
      "=======Loss: 5.018732 ======\n",
      "=======Loss: 5.1411357 ======\n",
      "=======Loss: 5.0958834 ======\n",
      "=======Loss: 5.0316358 ======\n",
      "=======Loss: 5.3185325 ======\n",
      "=======Loss: 5.1915307 ======\n",
      "=======Loss: 5.1545897 ======\n",
      "=======Loss: 5.1708155 ======\n",
      "=======Loss: 5.1761045 ======\n",
      "=======Loss: 4.8218255 ======\n",
      "=======Loss: 5.2210255 ======\n",
      "=======Loss: 5.157746 ======\n",
      "=======Loss: 5.0558224 ======\n",
      "=======Loss: 5.103765 ======\n",
      "=======Loss: 5.194981 ======\n",
      "=======Loss: 5.1026254 ======\n",
      "=======Loss: 4.925988 ======\n",
      "=======Loss: 4.9922457 ======\n",
      "=======Loss: 5.2250495 ======\n",
      "=======Loss: 4.8423166 ======\n",
      "=======Loss: 5.1319146 ======\n",
      "=======Loss: 5.0086684 ======\n",
      "=======Loss: 5.035547 ======\n",
      "=======Loss: 4.8685884 ======\n",
      "=======Loss: 5.103188 ======\n",
      "=======Loss: 5.2938366 ======\n",
      "=======Loss: 5.0229816 ======\n",
      "=======Loss: 4.8361597 ======\n",
      "=======Loss: 5.2098236 ======\n",
      "=======Loss: 4.933038 ======\n",
      "=======Loss: 5.021452 ======\n",
      "=======Loss: 4.9490128 ======\n",
      "=======Loss: 4.881004 ======\n",
      "=======Loss: 4.993463 ======\n",
      "=======Loss: 4.9112535 ======\n",
      "=======Loss: 4.7695484 ======\n",
      "=======Loss: 4.8725324 ======\n",
      "=======Loss: 5.0496016 ======\n",
      "=======Loss: 4.8968353 ======\n",
      "=======Loss: 4.730126 ======\n",
      "=======Loss: 5.1333647 ======\n",
      "=======Loss: 4.8868427 ======\n",
      "=======Loss: 4.967737 ======\n",
      "=======Loss: 5.0110607 ======\n",
      "=======Loss: 4.9185147 ======\n",
      "=======Loss: 5.1315937 ======\n",
      "=======Loss: 4.8109384 ======\n",
      "=======Loss: 5.06016 ======\n",
      "=======Loss: 4.9781036 ======\n",
      "=======Loss: 4.97703 ======\n",
      "=======Loss: 4.7494144 ======\n",
      "=======Loss: 4.74512 ======\n",
      "=======Loss: 5.2123632 ======\n",
      "=======Loss: 4.850466 ======\n",
      "=======Loss: 4.750665 ======\n",
      "=======Loss: 4.959998 ======\n",
      "=======Loss: 4.7743373 ======\n",
      "=======Loss: 4.8709526 ======\n",
      "=======Loss: 4.9957004 ======\n",
      "=======Loss: 4.70815 ======\n",
      "=======Loss: 4.69729 ======\n",
      "=======Loss: 4.837517 ======\n",
      "=======Loss: 4.9517183 ======\n",
      "=======Loss: 5.0194664 ======\n",
      "=======Loss: 4.957425 ======\n",
      "=======Loss: 4.9952855 ======\n",
      "=======Loss: 5.1328173 ======\n",
      "=======Loss: 4.877387 ======\n",
      "=======Loss: 5.0037107 ======\n",
      "=======Loss: 4.812551 ======\n",
      "=======Loss: 4.7940435 ======\n",
      "=======Loss: 4.810789 ======\n",
      "=======Loss: 4.7525115 ======\n",
      "=======Loss: 4.910033 ======\n",
      "=======Loss: 5.039467 ======\n",
      "=======Loss: 4.863657 ======\n",
      "=======Loss: 4.686021 ======\n",
      "=======Loss: 4.922062 ======\n",
      "=======Loss: 4.8230863 ======\n",
      "=======Loss: 4.984803 ======\n",
      "=======Loss: 4.838039 ======\n",
      "=======Loss: 5.0188446 ======\n",
      "=======Loss: 4.7728877 ======\n",
      "=======Loss: 4.8159676 ======\n",
      "=======Loss: 4.782278 ======\n",
      "=======Loss: 4.9164486 ======\n",
      "=======Loss: 4.7769504 ======\n",
      "=======Loss: 4.747428 ======\n",
      "=======Loss: 4.654762 ======\n",
      "=======Loss: 4.597788 ======\n",
      "=======Loss: 4.7228394 ======\n",
      "=======Loss: 4.8479443 ======\n",
      "=======Loss: 4.8162055 ======\n",
      "=======Loss: 4.650175 ======\n",
      "=======Loss: 4.759921 ======\n",
      "=======Loss: 4.732174 ======\n",
      "=======Loss: 4.9216185 ======\n",
      "=======Loss: 4.6195946 ======\n",
      "=======Loss: 4.744212 ======\n",
      "=======Loss: 4.704979 ======\n",
      "=======Loss: 4.8678923 ======\n",
      "=======Loss: 5.0981345 ======\n",
      "=======Loss: 4.8735013 ======\n",
      "=======Loss: 4.704992 ======\n",
      "=======Loss: 4.779028 ======\n",
      "=======Loss: 4.641292 ======\n",
      "=======Loss: 4.7045527 ======\n",
      "=======Loss: 4.6064672 ======\n",
      "=======Loss: 4.746686 ======\n",
      "=======Loss: 4.5984707 ======\n",
      "=======Loss: 4.7068186 ======\n",
      "=======Loss: 4.7835464 ======\n",
      "=======Loss: 4.5581336 ======\n",
      "=======Loss: 4.8000984 ======\n",
      "=======Loss: 4.475152 ======\n",
      "=======Loss: 4.602975 ======\n",
      "=======Loss: 4.4459243 ======\n",
      "=======Loss: 4.829886 ======\n",
      "=======Loss: 4.72692 ======\n",
      "=======Loss: 4.627038 ======\n",
      "=======Loss: 4.6596212 ======\n",
      "=======Loss: 4.760001 ======\n",
      "=======Loss: 4.5692086 ======\n",
      "=======Loss: 4.553914 ======\n",
      "=======Loss: 4.655131 ======\n",
      "=======Loss: 4.618707 ======\n",
      "=======Loss: 4.8251915 ======\n",
      "=======Loss: 4.468788 ======\n",
      "=======Loss: 4.6033425 ======\n",
      "=======Loss: 4.672329 ======\n",
      "=======Loss: 4.7968554 ======\n",
      "=======Loss: 4.6020145 ======\n",
      "=======Loss: 4.5537386 ======\n",
      "=======Loss: 4.549228 ======\n",
      "=======Loss: 4.4883256 ======\n",
      "=======Loss: 4.5349784 ======\n",
      "=======Loss: 4.465336 ======\n",
      "=======Loss: 4.464184 ======\n",
      "=======Loss: 4.803068 ======\n",
      "=======Loss: 4.3632803 ======\n",
      "=======Loss: 4.4958105 ======\n",
      "=======Loss: 4.4868736 ======\n",
      "=======Loss: 4.502275 ======\n",
      "=======Loss: 4.3367343 ======\n",
      "=======Loss: 4.7220135 ======\n",
      "=======Loss: 4.3989577 ======\n",
      "=======Loss: 4.4712944 ======\n",
      "=======Loss: 4.645042 ======\n",
      "=======Loss: 4.4987526 ======\n",
      "=======Loss: 4.590749 ======\n",
      "=======Loss: 4.7384677 ======\n",
      "=======Loss: 4.5795007 ======\n",
      "=======Loss: 4.5115504 ======\n",
      "=======Loss: 4.438883 ======\n",
      "=======Loss: 4.429823 ======\n",
      "=======Loss: 4.2683945 ======\n",
      "=======Loss: 4.237262 ======\n",
      "=======Loss: 4.392229 ======\n",
      "=======Loss: 4.267706 ======\n",
      "=======Loss: 4.2856703 ======\n",
      "=======Loss: 4.4862657 ======\n",
      "=======Loss: 4.3172445 ======\n",
      "=======Loss: 4.526265 ======\n",
      "=======Loss: 4.4209757 ======\n",
      "=======Loss: 4.3387833 ======\n",
      "=======Loss: 4.5905857 ======\n",
      "=======Loss: 4.2864246 ======\n",
      "=======Loss: 4.556211 ======\n",
      "=======Loss: 4.5677967 ======\n",
      "=======Loss: 4.2802153 ======\n",
      "=======Loss: 4.531452 ======\n",
      "=======Loss: 4.4087095 ======\n",
      "=======Loss: 4.396681 ======\n",
      "=======Loss: 4.6189547 ======\n",
      "=======Loss: 4.565156 ======\n",
      "=======Loss: 4.3334565 ======\n",
      "=======Loss: 4.2354136 ======\n",
      "=======Loss: 4.555416 ======\n",
      "=======Loss: 4.2209435 ======\n",
      "=======Loss: 4.6186485 ======\n",
      "=======Loss: 4.2706547 ======\n",
      "=======Loss: 4.464553 ======\n",
      "=======Loss: 4.3801537 ======\n",
      "=======Loss: 4.2956724 ======\n",
      "=======Loss: 4.4774613 ======\n",
      "=======Loss: 4.5773015 ======\n",
      "=======Loss: 4.373724 ======\n",
      "=======Loss: 4.2365785 ======\n",
      "=======Loss: 4.3659325 ======\n",
      "=======Loss: 4.33563 ======\n",
      "=======Loss: 4.3318253 ======\n",
      "=======Loss: 4.2248154 ======\n",
      "=======Loss: 4.2127314 ======\n",
      "=======Loss: 4.4418845 ======\n",
      "=======Loss: 4.464986 ======\n",
      "=======Loss: 4.339926 ======\n",
      "=======Loss: 4.307453 ======\n",
      "=======Loss: 4.2848396 ======\n",
      "=======Loss: 4.385726 ======\n",
      "=======Loss: 4.1334925 ======\n",
      "=======Loss: 4.149031 ======\n",
      "=======Loss: 4.2616744 ======\n",
      "=======Loss: 4.369552 ======\n",
      "=======Loss: 4.2454467 ======\n",
      "=======Loss: 4.179834 ======\n",
      "=======Loss: 4.1513042 ======\n",
      "=======Loss: 4.190023 ======\n",
      "=======Loss: 4.276696 ======\n",
      "=======Loss: 4.261606 ======\n",
      "=======Loss: 4.389228 ======\n",
      "=======Loss: 4.2003508 ======\n",
      "=======Loss: 4.316041 ======\n",
      "=======Loss: 4.179623 ======\n",
      "=======Loss: 4.1460514 ======\n",
      "=======Loss: 4.0258856 ======\n",
      "=======Loss: 4.241121 ======\n",
      "=======Loss: 3.8704047 ======\n",
      "=======Loss: 4.117831 ======\n",
      "=======Loss: 4.153557 ======\n",
      "=======Loss: 3.986458 ======\n",
      "=======Loss: 4.198562 ======\n",
      "=======Loss: 4.0811496 ======\n",
      "=======Loss: 4.2218857 ======\n",
      "=======Loss: 4.0768 ======\n",
      "=======Loss: 3.9594984 ======\n",
      "=======Loss: 4.3029623 ======\n",
      "=======Loss: 4.2819686 ======\n",
      "=======Loss: 3.9249125 ======\n",
      "=======Loss: 3.76096 ======\n",
      "=======Loss: 4.145999 ======\n",
      "=======Loss: 4.0235276 ======\n",
      "=======Loss: 4.035919 ======\n",
      "=======Loss: 4.121707 ======\n",
      "=======Loss: 4.136084 ======\n",
      "=======Loss: 4.111657 ======\n",
      "=======Loss: 4.0329795 ======\n",
      "=======Loss: 3.9948208 ======\n",
      "=======Loss: 3.887612 ======\n",
      "=======Loss: 4.1790304 ======\n",
      "=======Loss: 4.1388288 ======\n",
      "=======Loss: 3.929148 ======\n",
      "=======Loss: 4.0156775 ======\n",
      "=======Loss: 4.031915 ======\n",
      "=======Loss: 3.9943526 ======\n",
      "=======Loss: 3.8240738 ======\n",
      "=======Loss: 3.8521595 ======\n",
      "=======Loss: 3.8365757 ======\n",
      "=======Loss: 4.0344567 ======\n",
      "=======Loss: 3.8831773 ======\n",
      "=======Loss: 3.8322632 ======\n",
      "=======Loss: 3.8136075 ======\n",
      "=======Loss: 3.8998253 ======\n",
      "=======Loss: 3.8088715 ======\n",
      "=======Loss: 3.9286072 ======\n",
      "=======Loss: 3.8431182 ======\n",
      "=======Loss: 3.7428453 ======\n",
      "=======Loss: 3.795786 ======\n",
      "=======Loss: 3.8671222 ======\n",
      "=======Loss: 3.680256 ======\n",
      "=======Loss: 3.7588396 ======\n",
      "=======Loss: 3.6352525 ======\n",
      "=======Loss: 3.7457964 ======\n",
      "=======Loss: 3.68792 ======\n",
      "=======Loss: 3.76545 ======\n",
      "=======Loss: 3.5890417 ======\n",
      "=======Loss: 3.824399 ======\n",
      "=======Loss: 3.8496315 ======\n",
      "=======Loss: 3.643339 ======\n",
      "=======Loss: 3.3939185 ======\n",
      "=======Loss: 3.5966387 ======\n",
      "=======Loss: 3.8083334 ======\n",
      "=======Loss: 3.6149354 ======\n",
      "=======Loss: 3.5672293 ======\n",
      "=======Loss: 3.6138613 ======\n",
      "=======Loss: 3.5935159 ======\n",
      "=======Loss: 3.3704321 ======\n",
      "=======Loss: 3.6767092 ======\n",
      "=======Loss: 3.4559572 ======\n",
      "=======Loss: 3.4239209 ======\n",
      "=======Loss: 3.6107666 ======\n",
      "=======Loss: 3.5941348 ======\n",
      "=======Loss: 3.531575 ======\n",
      "=======Loss: 3.5067468 ======\n",
      "=======Loss: 3.5026548 ======\n",
      "=======Loss: 3.5958173 ======\n",
      "=======Loss: 3.6461496 ======\n",
      "=======Loss: 3.6527588 ======\n",
      "=======Loss: 3.558773 ======\n",
      "=======Loss: 3.6227207 ======\n",
      "=======Loss: 3.4910529 ======\n",
      "=======Loss: 3.5636945 ======\n",
      "=======Loss: 3.613315 ======\n",
      "=======Loss: 3.454803 ======\n",
      "=======Loss: 3.5084987 ======\n",
      "=======Loss: 3.4036586 ======\n",
      "=======Loss: 3.437102 ======\n",
      "=======Loss: 3.331637 ======\n",
      "=======Loss: 3.4889505 ======\n",
      "=======Loss: 3.5549803 ======\n",
      "=======Loss: 3.3507414 ======\n",
      "=======Loss: 3.4450727 ======\n",
      "=======Loss: 3.337645 ======\n",
      "=======Loss: 3.3911061 ======\n",
      "=======Loss: 3.362377 ======\n",
      "=======Loss: 3.2281032 ======\n",
      "=======Loss: 3.33328 ======\n",
      "=======Loss: 3.1961153 ======\n",
      "=======Loss: 3.2247527 ======\n",
      "=======Loss: 3.3488355 ======\n",
      "=======Loss: 3.426784 ======\n",
      "=======Loss: 3.385341 ======\n",
      "=======Loss: 3.4453359 ======\n",
      "=======Loss: 3.271476 ======\n",
      "=======Loss: 3.302092 ======\n",
      "=======Loss: 3.2541966 ======\n",
      "=======Loss: 3.1484041 ======\n",
      "=======Loss: 3.112447 ======\n",
      "=======Loss: 3.365613 ======\n",
      "=======Loss: 3.4243202 ======\n",
      "=======Loss: 3.320356 ======\n",
      "=======Loss: 3.053081 ======\n",
      "=======Loss: 3.1723719 ======\n",
      "=======Loss: 3.1285672 ======\n",
      "=======Loss: 3.273344 ======\n",
      "=======Loss: 3.1594326 ======\n",
      "=======Loss: 3.1009974 ======\n",
      "=======Loss: 3.114468 ======\n",
      "=======Loss: 3.2674572 ======\n",
      "=======Loss: 3.0184803 ======\n",
      "=======Loss: 2.9898105 ======\n",
      "=======Loss: 2.9318252 ======\n",
      "=======Loss: 3.100718 ======\n",
      "=======Loss: 3.2613778 ======\n",
      "=======Loss: 3.1141515 ======\n",
      "=======Loss: 3.0778904 ======\n",
      "=======Loss: 3.1149027 ======\n",
      "=======Loss: 3.2019691 ======\n",
      "=======Loss: 2.906372 ======\n",
      "=======Loss: 2.999357 ======\n",
      "=======Loss: 3.0610516 ======\n",
      "=======Loss: 3.114461 ======\n",
      "=======Loss: 2.976972 ======\n",
      "=======Loss: 3.0719585 ======\n",
      "=======Loss: 3.0285337 ======\n",
      "=======Loss: 3.1381793 ======\n",
      "=======Loss: 2.9330063 ======\n",
      "=======Loss: 3.021577 ======\n",
      "=======Loss: 2.9157577 ======\n",
      "=======Loss: 3.088312 ======\n",
      "=======Loss: 3.0224915 ======\n",
      "=======Loss: 3.1508482 ======\n",
      "=======Loss: 2.8598783 ======\n",
      "=======Loss: 3.0992417 ======\n",
      "=======Loss: 2.9138992 ======\n",
      "=======Loss: 2.928624 ======\n",
      "=======Loss: 3.0159073 ======\n",
      "=======Loss: 2.8983903 ======\n",
      "=======Loss: 2.842536 ======\n",
      "=======Loss: 2.8362653 ======\n",
      "=======Loss: 2.9489522 ======\n",
      "=======Loss: 2.9925258 ======\n",
      "=======Loss: 2.9381318 ======\n",
      "=======Loss: 2.9056191 ======\n",
      "=======Loss: 3.105537 ======\n",
      "=======Loss: 3.0129042 ======\n",
      "=======Loss: 3.0271075 ======\n",
      "=======Loss: 2.8769865 ======\n",
      "=======Loss: 3.086687 ======\n",
      "=======Loss: 2.9116325 ======\n",
      "=======Loss: 2.9879303 ======\n",
      "=======Loss: 2.8285608 ======\n",
      "=======Loss: 2.910869 ======\n",
      "=======Loss: 2.926899 ======\n",
      "=======Loss: 2.7849932 ======\n",
      "=======Loss: 2.8822305 ======\n",
      "=======Loss: 2.865019 ======\n",
      "=======Loss: 2.7652016 ======\n",
      "=======Loss: 2.7984052 ======\n",
      "=======Loss: 2.9289865 ======\n",
      "=======Loss: 2.8716826 ======\n",
      "=======Loss: 2.7331579 ======\n",
      "=======Loss: 2.8029265 ======\n",
      "=======Loss: 2.8962355 ======\n",
      "=======Loss: 2.8207345 ======\n",
      "=======Loss: 2.9565063 ======\n",
      "=======Loss: 2.8285294 ======\n",
      "=======Loss: 2.8869264 ======\n",
      "=======Loss: 2.777144 ======\n",
      "=======Loss: 2.8340473 ======\n",
      "=======Loss: 2.8069894 ======\n",
      "=======Loss: 2.8395548 ======\n",
      "=======Loss: 2.8526416 ======\n",
      "=======Loss: 2.7716773 ======\n",
      "=======Loss: 2.7091103 ======\n",
      "=======Loss: 2.8300672 ======\n",
      "=======Loss: 2.671382 ======\n",
      "=======Loss: 2.8985534 ======\n",
      "=======Loss: 2.8240004 ======\n",
      "=======Loss: 2.6399105 ======\n",
      "=======Loss: 2.875382 ======\n",
      "=======Loss: 2.6776052 ======\n",
      "=======Loss: 2.753529 ======\n",
      "=======Loss: 2.7860284 ======\n",
      "=======Loss: 2.8165445 ======\n",
      "=======Loss: 2.7417557 ======\n",
      "=======Loss: 2.7592664 ======\n",
      "=======Loss: 2.683171 ======\n",
      "=======Loss: 2.592197 ======\n",
      "=======Loss: 2.725243 ======\n",
      "=======Loss: 2.678396 ======\n",
      "=======Loss: 2.7776182 ======\n",
      "=======Loss: 2.7096536 ======\n",
      "=======Loss: 2.8591056 ======\n",
      "=======Loss: 2.5336552 ======\n",
      "=======Loss: 2.7191508 ======\n",
      "=======Loss: 2.6377618 ======\n",
      "=======Loss: 2.7250428 ======\n",
      "=======Loss: 2.7539392 ======\n",
      "=======Loss: 2.6308484 ======\n",
      "=======Loss: 2.673945 ======\n",
      "=======Loss: 2.677435 ======\n",
      "=======Loss: 2.6424727 ======\n",
      "=======Loss: 2.7364528 ======\n",
      "=======Loss: 2.6544132 ======\n",
      "=======Loss: 2.710801 ======\n",
      "=======Loss: 2.732275 ======\n",
      "=======Loss: 2.682394 ======\n",
      "=======Loss: 2.6286225 ======\n",
      "=======Loss: 2.8717647 ======\n",
      "=======Loss: 2.656154 ======\n",
      "=======Loss: 2.6861703 ======\n",
      "=======Loss: 2.7031674 ======\n",
      "=======Loss: 2.5740228 ======\n",
      "=======Loss: 2.553023 ======\n",
      "=======Loss: 2.5588882 ======\n",
      "=======Loss: 2.5409484 ======\n",
      "=======Loss: 2.5935364 ======\n",
      "=======Loss: 2.708774 ======\n",
      "=======Loss: 2.670543 ======\n",
      "=======Loss: 2.434281 ======\n",
      "=======Loss: 2.5450168 ======\n",
      "=======Loss: 2.5418434 ======\n",
      "=======Loss: 2.535305 ======\n",
      "=======Loss: 2.616517 ======\n",
      "=======Loss: 2.7152567 ======\n",
      "=======Loss: 2.6877644 ======\n",
      "=======Loss: 2.6289113 ======\n",
      "=======Loss: 2.5815194 ======\n",
      "=======Loss: 2.5846925 ======\n",
      "=======Loss: 2.4743776 ======\n",
      "=======Loss: 2.6419668 ======\n",
      "=======Loss: 2.6872683 ======\n",
      "=======Loss: 2.6544156 ======\n",
      "=======Loss: 2.494073 ======\n",
      "=======Loss: 2.6274757 ======\n",
      "=======Loss: 2.5406556 ======\n",
      "=======Loss: 2.5228539 ======\n",
      "=======Loss: 2.38875 ======\n",
      "=======Loss: 2.537508 ======\n",
      "=======Loss: 2.6831567 ======\n",
      "=======Loss: 2.3642833 ======\n",
      "=======Loss: 2.5683875 ======\n",
      "=======Loss: 2.5517755 ======\n",
      "=======Loss: 2.6021678 ======\n",
      "=======Loss: 2.3050463 ======\n",
      "=======Loss: 2.5919468 ======\n",
      "=======Loss: 2.5095186 ======\n",
      "=======Loss: 2.5106854 ======\n",
      "=======Loss: 2.4053311 ======\n",
      "=======Loss: 2.6801708 ======\n",
      "=======Loss: 2.4393358 ======\n",
      "=======Loss: 2.451429 ======\n",
      "=======Loss: 2.5439925 ======\n",
      "=======Loss: 2.3837419 ======\n",
      "=======Loss: 2.552569 ======\n",
      "=======Loss: 2.4994688 ======\n",
      "=======Loss: 2.4599829 ======\n",
      "=======Loss: 2.5301604 ======\n",
      "=======Loss: 2.505577 ======\n",
      "=======Loss: 2.4572146 ======\n",
      "=======Loss: 2.5085092 ======\n",
      "=======Loss: 2.5199904 ======\n",
      "=======Loss: 2.5996513 ======\n",
      "=======Loss: 2.5032356 ======\n",
      "=======Loss: 2.3791287 ======\n",
      "=======Loss: 2.587036 ======\n",
      "=======Loss: 2.3657868 ======\n",
      "=======Loss: 2.5350728 ======\n",
      "=======Loss: 2.424963 ======\n",
      "=======Loss: 2.4098234 ======\n",
      "=======Loss: 2.6212564 ======\n",
      "=======Loss: 2.4023335 ======\n",
      "=======Loss: 2.3881402 ======\n",
      "=======Loss: 2.3598433 ======\n",
      "=======Loss: 2.3761928 ======\n",
      "=======Loss: 2.3539543 ======\n",
      "=======Loss: 2.471449 ======\n",
      "=======Loss: 2.5402172 ======\n",
      "=======Loss: 2.485772 ======\n",
      "=======Loss: 2.431185 ======\n",
      "=======Loss: 2.2880864 ======\n",
      "=======Loss: 2.385586 ======\n",
      "=======Loss: 2.3866904 ======\n",
      "=======Loss: 2.4790297 ======\n",
      "=======Loss: 2.4334862 ======\n",
      "=======Loss: 2.5226393 ======\n",
      "=======Loss: 2.3278482 ======\n",
      "=======Loss: 2.527075 ======\n",
      "=======Loss: 2.5456958 ======\n",
      "=======Loss: 2.482574 ======\n",
      "=======Loss: 2.3767161 ======\n",
      "=======Loss: 2.3028936 ======\n",
      "=======Loss: 2.4659553 ======\n",
      "=======Loss: 2.486425 ======\n",
      "=======Loss: 2.4274998 ======\n",
      "=======Loss: 2.2568462 ======\n",
      "=======Loss: 2.2978365 ======\n",
      "=======Loss: 2.3030562 ======\n",
      "=======Loss: 2.3191087 ======\n",
      "=======Loss: 2.243249 ======\n",
      "=======Loss: 2.4905572 ======\n",
      "=======Loss: 2.4543214 ======\n",
      "=======Loss: 2.3995347 ======\n",
      "=======Loss: 2.4064944 ======\n",
      "=======Loss: 2.4741366 ======\n",
      "=======Loss: 2.3717659 ======\n",
      "=======Loss: 2.2318106 ======\n",
      "=======Loss: 2.26877 ======\n",
      "=======Loss: 2.426348 ======\n",
      "=======Loss: 2.3457599 ======\n",
      "=======Loss: 2.3775938 ======\n",
      "=======Loss: 2.377056 ======\n",
      "=======Loss: 2.292933 ======\n",
      "=======Loss: 2.3385642 ======\n",
      "=======Loss: 2.444034 ======\n",
      "=======Loss: 2.3429394 ======\n",
      "=======Loss: 2.251009 ======\n",
      "=======Loss: 2.4129121 ======\n",
      "=======Loss: 2.339861 ======\n",
      "=======Loss: 2.421699 ======\n",
      "=======Loss: 2.2504427 ======\n",
      "=======Loss: 2.3662376 ======\n",
      "=======Loss: 2.4093835 ======\n",
      "=======Loss: 2.2826583 ======\n",
      "=======Loss: 2.3766356 ======\n",
      "=======Loss: 2.2521274 ======\n",
      "=======Loss: 2.3265145 ======\n",
      "=======Loss: 2.3074214 ======\n",
      "=======Loss: 2.4019156 ======\n",
      "=======Loss: 2.2295797 ======\n",
      "=======Loss: 2.2901802 ======\n",
      "=======Loss: 2.382295 ======\n",
      "=======Loss: 2.2523236 ======\n",
      "=======Loss: 2.1303067 ======\n",
      "=======Loss: 2.2825108 ======\n",
      "=======Loss: 2.3667932 ======\n",
      "=======Loss: 2.3375874 ======\n",
      "=======Loss: 2.4430332 ======\n",
      "=======Loss: 2.270049 ======\n",
      "=======Loss: 2.1803575 ======\n",
      "=======Loss: 2.3083735 ======\n",
      "=======Loss: 2.2615273 ======\n",
      "=======Loss: 2.248879 ======\n",
      "=======Loss: 2.3351083 ======\n",
      "=======Loss: 2.188121 ======\n",
      "=======Loss: 2.2569432 ======\n",
      "=======Loss: 2.2925687 ======\n",
      "=======Loss: 2.2002861 ======\n",
      "=======Loss: 2.1343129 ======\n",
      "=======Loss: 2.3900907 ======\n",
      "=======Loss: 2.1903338 ======\n",
      "=======Loss: 2.287762 ======\n",
      "=======Loss: 2.4151733 ======\n",
      "=======Loss: 2.130826 ======\n",
      "=======Loss: 2.189371 ======\n",
      "=======Loss: 2.4385762 ======\n",
      "=======Loss: 2.1075325 ======\n",
      "=======Loss: 2.1898024 ======\n",
      "=======Loss: 2.200574 ======\n",
      "=======Loss: 2.170228 ======\n",
      "=======Loss: 2.1155133 ======\n",
      "=======Loss: 2.233389 ======\n",
      "=======Loss: 2.257739 ======\n",
      "=======Loss: 2.209073 ======\n",
      "=======Loss: 2.1997542 ======\n",
      "=======Loss: 2.2175536 ======\n",
      "=======Loss: 2.2182498 ======\n",
      "=======Loss: 2.2384403 ======\n",
      "=======Loss: 2.1652422 ======\n",
      "=======Loss: 2.1087344 ======\n",
      "=======Loss: 2.1572082 ======\n",
      "=======Loss: 2.2098565 ======\n",
      "=======Loss: 2.190379 ======\n",
      "=======Loss: 2.142529 ======\n",
      "=======Loss: 2.2588844 ======\n",
      "=======Loss: 2.348822 ======\n",
      "=======Loss: 2.148923 ======\n",
      "=======Loss: 2.2494683 ======\n",
      "=======Loss: 2.2729962 ======\n",
      "=======Loss: 2.1697702 ======\n",
      "=======Loss: 2.260266 ======\n",
      "=======Loss: 2.3280244 ======\n",
      "=======Loss: 2.2988563 ======\n",
      "=======Loss: 2.2936726 ======\n",
      "=======Loss: 2.1012726 ======\n",
      "=======Loss: 2.3175955 ======\n",
      "=======Loss: 2.307946 ======\n",
      "=======Loss: 2.1553915 ======\n",
      "=======Loss: 2.1569393 ======\n",
      "=======Loss: 2.1988132 ======\n",
      "=======Loss: 2.1645284 ======\n",
      "=======Loss: 2.1086571 ======\n",
      "=======Loss: 2.0833254 ======\n",
      "=======Loss: 2.190518 ======\n",
      "=======Loss: 2.128128 ======\n",
      "=======Loss: 2.0451388 ======\n",
      "=======Loss: 2.10631 ======\n",
      "=======Loss: 2.2223468 ======\n",
      "=======Loss: 2.2048202 ======\n",
      "=======Loss: 2.2172885 ======\n",
      "=======Loss: 2.1570966 ======\n",
      "=======Loss: 2.297141 ======\n",
      "=======Loss: 2.1200342 ======\n",
      "=======Loss: 2.0942285 ======\n",
      "=======Loss: 2.148954 ======\n",
      "=======Loss: 2.0631127 ======\n",
      "=======Loss: 2.079564 ======\n",
      "=======Loss: 2.2216148 ======\n",
      "=======Loss: 2.0955238 ======\n",
      "=======Loss: 2.117331 ======\n",
      "=======Loss: 2.046846 ======\n",
      "=======Loss: 2.15263 ======\n",
      "=======Loss: 2.1963594 ======\n",
      "=======Loss: 2.2019477 ======\n",
      "=======Loss: 2.0486898 ======\n",
      "=======Loss: 2.096787 ======\n",
      "=======Loss: 2.134459 ======\n",
      "=======Loss: 2.0930047 ======\n",
      "=======Loss: 2.0512967 ======\n",
      "=======Loss: 2.188747 ======\n",
      "=======Loss: 1.9804924 ======\n",
      "=======Loss: 2.037488 ======\n",
      "=======Loss: 2.179189 ======\n",
      "=======Loss: 1.9909359 ======\n",
      "=======Loss: 2.1090167 ======\n",
      "=======Loss: 2.0762234 ======\n",
      "=======Loss: 2.0895054 ======\n",
      "=======Loss: 2.0284338 ======\n",
      "=======Loss: 2.1400678 ======\n",
      "=======Loss: 2.1117582 ======\n",
      "=======Loss: 2.197165 ======\n",
      "=======Loss: 2.042163 ======\n",
      "=======Loss: 2.0423682 ======\n",
      "=======Loss: 2.1294355 ======\n",
      "=======Loss: 2.0829687 ======\n",
      "=======Loss: 2.105066 ======\n",
      "=======Loss: 1.9673238 ======\n",
      "=======Loss: 2.0589793 ======\n",
      "=======Loss: 2.0428545 ======\n",
      "=======Loss: 2.0376303 ======\n",
      "=======Loss: 2.1723742 ======\n",
      "=======Loss: 2.028959 ======\n",
      "=======Loss: 2.00376 ======\n",
      "=======Loss: 2.1117454 ======\n",
      "=======Loss: 2.1017017 ======\n",
      "=======Loss: 2.0418258 ======\n",
      "=======Loss: 2.0356588 ======\n",
      "=======Loss: 2.059428 ======\n",
      "=======Loss: 2.082783 ======\n",
      "=======Loss: 2.021234 ======\n",
      "=======Loss: 2.119936 ======\n",
      "=======Loss: 2.154845 ======\n",
      "=======Loss: 2.0787141 ======\n",
      "=======Loss: 1.9804101 ======\n",
      "=======Loss: 2.1144462 ======\n",
      "=======Loss: 1.9608114 ======\n",
      "=======Loss: 2.0736613 ======\n",
      "=======Loss: 2.0432262 ======\n",
      "=======Loss: 2.0289414 ======\n",
      "=======Loss: 2.150034 ======\n",
      "=======Loss: 1.9643117 ======\n",
      "=======Loss: 2.1908386 ======\n",
      "=======Loss: 2.1120641 ======\n",
      "=======Loss: 2.0857906 ======\n",
      "=======Loss: 2.0091364 ======\n",
      "=======Loss: 1.953998 ======\n",
      "=======Loss: 2.0659103 ======\n",
      "=======Loss: 2.048272 ======\n",
      "=======Loss: 2.0522735 ======\n",
      "=======Loss: 1.9951658 ======\n",
      "=======Loss: 2.0570574 ======\n",
      "=======Loss: 1.9934227 ======\n",
      "=======Loss: 1.9641814 ======\n",
      "=======Loss: 1.9937816 ======\n",
      "=======Loss: 1.9543834 ======\n",
      "=======Loss: 2.017836 ======\n",
      "=======Loss: 1.9999342 ======\n",
      "=======Loss: 1.9689435 ======\n",
      "=======Loss: 2.0112174 ======\n",
      "=======Loss: 1.9047691 ======\n",
      "=======Loss: 1.920708 ======\n",
      "=======Loss: 1.9159954 ======\n",
      "=======Loss: 1.990084 ======\n",
      "=======Loss: 2.0417397 ======\n",
      "=======Loss: 2.0809321 ======\n",
      "=======Loss: 2.054785 ======\n",
      "=======Loss: 2.0315075 ======\n",
      "=======Loss: 2.2088332 ======\n",
      "=======Loss: 1.9755303 ======\n",
      "=======Loss: 1.9714763 ======\n",
      "=======Loss: 2.067271 ======\n",
      "=======Loss: 2.0177822 ======\n",
      "=======Loss: 2.0625477 ======\n",
      "=======Loss: 1.9622154 ======\n",
      "=======Loss: 2.0158775 ======\n",
      "=======Loss: 2.0056667 ======\n",
      "=======Loss: 1.917399 ======\n",
      "=======Loss: 1.9037025 ======\n",
      "=======Loss: 2.0044546 ======\n",
      "=======Loss: 2.0255866 ======\n",
      "=======Loss: 1.9432776 ======\n",
      "=======Loss: 2.0175457 ======\n",
      "=======Loss: 2.131499 ======\n",
      "=======Loss: 2.0279932 ======\n"
     ]
    }
   ],
   "source": [
    "n_params = count_model_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# %%\n",
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=8e-4, \n",
    "                                weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2, reduce_dims=(0,1))\n",
    "# h1loss = H1Loss(d=2, reduce_dims=(0,1))\n",
    "\n",
    "train_loss = l2loss\n",
    "eval_losses={'l2': l2loss} #'h1': h1loss, \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()\n",
    "\n",
    "step = 0\n",
    "\n",
    "with open('script/sfno_loss.txt', 'w') as f:\n",
    "    for epoch in range(20):\n",
    "        avg_loss = 0\n",
    "        train_err = 0.0\n",
    "        \n",
    "        # track number of training examples in batch\n",
    "        n_samples = 0\n",
    "        for idx, sample in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            sample = {\n",
    "                k: v.to(device)\n",
    "                for k, v in sample.items()\n",
    "                if torch.is_tensor(v)\n",
    "            }\n",
    "\n",
    "            n_samples += sample[\"y\"].shape[0]\n",
    "            out = model(sample[\"x\"])\n",
    "\n",
    "            loss = l2loss(out, **sample)\n",
    "\n",
    "            loss.backward()\n",
    "            del out\n",
    "\n",
    "            optimizer.step()\n",
    "            train_err += loss.item()\n",
    "            with torch.no_grad():\n",
    "                print(\"=======Loss:\",loss.detach().cpu().numpy(),\"======\")\n",
    "                f.write(f'Step {step + 1}, Loss: {loss.item()}\\n')\n",
    "                step += 1\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_err)\n",
    "        else:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=1\n",
      "Test Loss: 0.5304 ± 0.0249\n"
     ]
    }
   ],
   "source": [
    "resolution = (32, 64)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (64, 128)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (128, 256)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "outputs = model(inputs)\n",
    "print(\"test time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "    # Input x\n",
    "    x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "    # Ground-truth\n",
    "    y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "    # Model prediction\n",
    "    x_in = x.unsqueeze(0).to(device)\n",
    "    out = model(x_in).squeeze()[0, ...].detach().cpu().numpy()\n",
    "    x = x[0, ...].detach().numpy()\n",
    "\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"./script/output_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "#     # Input x\n",
    "#     x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "#     # Ground-truth\n",
    "#     y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "#     x = x[0, ...].detach().numpy()\n",
    "    \n",
    "#     plt.imshow(x)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/input_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    \n",
    "#     plt.imshow(y)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/label_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
