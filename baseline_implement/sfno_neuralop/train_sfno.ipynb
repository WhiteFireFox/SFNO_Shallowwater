{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Fourier Neural Operators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=10\n",
      "Loading test dataloader at resolution (64, 128) with 50 samples and batch-size=10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from neuralop.models import SFNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.datasets import load_spherical_swe\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %%\n",
    "# Loading the Navier-Stokes dataset in 128x128 resolution\n",
    "train_loader, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[(32, 64), (64, 128)], n_tests=[50, 50], test_batch_sizes=[10, 10])\n",
    "\n",
    "model = SFNO(n_modes=(32, 32), in_channels=3, out_channels=3, hidden_channels=32, projection_channels=64, factorization='dense').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 277923 parameters.\n",
      "\n",
      "### MODEL ###\n",
      " SFNO(\n",
      "  (fno_blocks): FNOBlocks(\n",
      "    (convs): SphericalConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-3): 4 x ComplexDenseTensor(shape=torch.Size([32, 32, 32]), rank=None)\n",
      "      )\n",
      "      (sht_handle): SHT(\n",
      "        (_SHT_cache): ModuleDict()\n",
      "        (_iSHT_cache): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-3): 4 x Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (lifting): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (projection): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0008\n",
      "    lr: 0.0008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001B208375C10>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.LpLoss object at 0x000001B2301E1EE0>\n",
      "\n",
      " * Test: {'l2': <neuralop.losses.data_losses.LpLoss object at 0x000001B2301E1EE0>}\n",
      "=======Loss: 12.0496235 ======\n",
      "=======Loss: 12.008968 ======\n",
      "=======Loss: 11.957178 ======\n",
      "=======Loss: 11.966283 ======\n",
      "=======Loss: 11.911501 ======\n",
      "=======Loss: 11.842937 ======\n",
      "=======Loss: 11.788853 ======\n",
      "=======Loss: 11.742503 ======\n",
      "=======Loss: 11.651684 ======\n",
      "=======Loss: 11.5341625 ======\n",
      "=======Loss: 11.439661 ======\n",
      "=======Loss: 11.347021 ======\n",
      "=======Loss: 11.211179 ======\n",
      "=======Loss: 10.881151 ======\n",
      "=======Loss: 10.673807 ======\n",
      "=======Loss: 10.358987 ======\n",
      "=======Loss: 10.101095 ======\n",
      "=======Loss: 9.895025 ======\n",
      "=======Loss: 9.639266 ======\n",
      "=======Loss: 9.400662 ======\n",
      "=======Loss: 9.381472 ======\n",
      "=======Loss: 8.907989 ======\n",
      "=======Loss: 9.016439 ======\n",
      "=======Loss: 8.814089 ======\n",
      "=======Loss: 8.791922 ======\n",
      "=======Loss: 8.634031 ======\n",
      "=======Loss: 8.524228 ======\n",
      "=======Loss: 8.142287 ======\n",
      "=======Loss: 8.139965 ======\n",
      "=======Loss: 7.9653893 ======\n",
      "=======Loss: 7.9237776 ======\n",
      "=======Loss: 8.071764 ======\n",
      "=======Loss: 7.9319296 ======\n",
      "=======Loss: 7.7756653 ======\n",
      "=======Loss: 7.731679 ======\n",
      "=======Loss: 7.391742 ======\n",
      "=======Loss: 7.417049 ======\n",
      "=======Loss: 7.143499 ======\n",
      "=======Loss: 7.1886716 ======\n",
      "=======Loss: 7.073616 ======\n",
      "=======Loss: 7.0873966 ======\n",
      "=======Loss: 6.6695585 ======\n",
      "=======Loss: 6.911895 ======\n",
      "=======Loss: 6.909007 ======\n",
      "=======Loss: 6.679108 ======\n",
      "=======Loss: 6.6477413 ======\n",
      "=======Loss: 6.6050367 ======\n",
      "=======Loss: 6.3016977 ======\n",
      "=======Loss: 6.202109 ======\n",
      "=======Loss: 6.2515545 ======\n",
      "=======Loss: 6.029173 ======\n",
      "=======Loss: 6.0512896 ======\n",
      "=======Loss: 5.688975 ======\n",
      "=======Loss: 5.711033 ======\n",
      "=======Loss: 5.5582848 ======\n",
      "=======Loss: 5.463629 ======\n",
      "=======Loss: 5.355995 ======\n",
      "=======Loss: 5.445815 ======\n",
      "=======Loss: 5.1464405 ======\n",
      "=======Loss: 5.3210096 ======\n",
      "=======Loss: 5.2679386 ======\n",
      "=======Loss: 5.264759 ======\n",
      "=======Loss: 4.7884045 ======\n",
      "=======Loss: 4.8168354 ======\n",
      "=======Loss: 4.85852 ======\n",
      "=======Loss: 4.9207234 ======\n",
      "=======Loss: 5.0147285 ======\n",
      "=======Loss: 4.896595 ======\n",
      "=======Loss: 4.996146 ======\n",
      "=======Loss: 4.7670493 ======\n",
      "=======Loss: 4.741551 ======\n",
      "=======Loss: 5.0164375 ======\n",
      "=======Loss: 4.6854124 ======\n",
      "=======Loss: 4.8214626 ======\n",
      "=======Loss: 4.560567 ======\n",
      "=======Loss: 4.9217787 ======\n",
      "=======Loss: 4.7739887 ======\n",
      "=======Loss: 4.778204 ======\n",
      "=======Loss: 4.3806214 ======\n",
      "=======Loss: 4.5916815 ======\n",
      "=======Loss: 4.6304913 ======\n",
      "=======Loss: 4.44417 ======\n",
      "=======Loss: 4.299034 ======\n",
      "=======Loss: 4.4560585 ======\n",
      "=======Loss: 4.5011263 ======\n",
      "=======Loss: 4.409359 ======\n",
      "=======Loss: 4.625782 ======\n",
      "=======Loss: 4.63609 ======\n",
      "=======Loss: 4.3811684 ======\n",
      "=======Loss: 4.261584 ======\n",
      "=======Loss: 4.663806 ======\n",
      "=======Loss: 4.652725 ======\n",
      "=======Loss: 4.3961277 ======\n",
      "=======Loss: 4.4303503 ======\n",
      "=======Loss: 4.4649124 ======\n",
      "=======Loss: 4.4437327 ======\n",
      "=======Loss: 4.6050005 ======\n",
      "=======Loss: 4.1920204 ======\n",
      "=======Loss: 4.2151384 ======\n",
      "=======Loss: 4.1830564 ======\n",
      "=======Loss: 4.156536 ======\n",
      "=======Loss: 4.437375 ======\n",
      "=======Loss: 4.2819266 ======\n",
      "=======Loss: 4.13952 ======\n",
      "=======Loss: 4.572434 ======\n",
      "=======Loss: 4.3409247 ======\n",
      "=======Loss: 4.317347 ======\n",
      "=======Loss: 4.4633694 ======\n",
      "=======Loss: 4.088728 ======\n",
      "=======Loss: 4.251913 ======\n",
      "=======Loss: 4.160108 ======\n",
      "=======Loss: 4.2810545 ======\n",
      "=======Loss: 4.2720785 ======\n",
      "=======Loss: 4.3322134 ======\n",
      "=======Loss: 4.294448 ======\n",
      "=======Loss: 3.9180326 ======\n",
      "=======Loss: 4.033564 ======\n",
      "=======Loss: 4.3613706 ======\n",
      "=======Loss: 4.0960937 ======\n",
      "=======Loss: 4.014388 ======\n",
      "=======Loss: 4.2471905 ======\n",
      "=======Loss: 4.491166 ======\n",
      "=======Loss: 3.9779189 ======\n",
      "=======Loss: 4.2800217 ======\n",
      "=======Loss: 4.1732554 ======\n",
      "=======Loss: 4.152336 ======\n",
      "=======Loss: 4.112937 ======\n",
      "=======Loss: 4.294831 ======\n",
      "=======Loss: 4.1673317 ======\n",
      "=======Loss: 4.2703633 ======\n",
      "=======Loss: 4.1971273 ======\n",
      "=======Loss: 4.148102 ======\n",
      "=======Loss: 4.288026 ======\n",
      "=======Loss: 4.273165 ======\n",
      "=======Loss: 4.2853174 ======\n",
      "=======Loss: 4.3302126 ======\n",
      "=======Loss: 3.9961855 ======\n",
      "=======Loss: 4.079551 ======\n",
      "=======Loss: 4.163859 ======\n",
      "=======Loss: 4.621664 ======\n",
      "=======Loss: 4.552107 ======\n",
      "=======Loss: 4.129139 ======\n",
      "=======Loss: 4.119693 ======\n",
      "=======Loss: 4.113363 ======\n",
      "=======Loss: 3.93751 ======\n",
      "=======Loss: 3.8623257 ======\n",
      "=======Loss: 4.2234087 ======\n",
      "=======Loss: 4.4940853 ======\n",
      "=======Loss: 4.253903 ======\n",
      "=======Loss: 4.2220488 ======\n",
      "=======Loss: 4.145513 ======\n",
      "=======Loss: 4.1887126 ======\n",
      "=======Loss: 4.1279044 ======\n",
      "=======Loss: 4.0649066 ======\n",
      "=======Loss: 4.1989193 ======\n",
      "=======Loss: 4.1812167 ======\n",
      "=======Loss: 3.9554045 ======\n",
      "=======Loss: 4.074852 ======\n",
      "=======Loss: 4.085747 ======\n",
      "=======Loss: 4.451734 ======\n",
      "=======Loss: 4.06151 ======\n",
      "=======Loss: 3.8445 ======\n",
      "=======Loss: 4.0083976 ======\n",
      "=======Loss: 3.891881 ======\n",
      "=======Loss: 4.038475 ======\n",
      "=======Loss: 4.078681 ======\n",
      "=======Loss: 4.1049857 ======\n",
      "=======Loss: 4.3238473 ======\n",
      "=======Loss: 3.9516215 ======\n",
      "=======Loss: 4.1298122 ======\n",
      "=======Loss: 4.019133 ======\n",
      "=======Loss: 4.064806 ======\n",
      "=======Loss: 4.199925 ======\n",
      "=======Loss: 4.1237373 ======\n",
      "=======Loss: 3.9495046 ======\n",
      "=======Loss: 3.829177 ======\n",
      "=======Loss: 3.9688349 ======\n",
      "=======Loss: 3.8809626 ======\n",
      "=======Loss: 4.0190406 ======\n",
      "=======Loss: 3.9788728 ======\n",
      "=======Loss: 4.0727253 ======\n",
      "=======Loss: 4.0792904 ======\n",
      "=======Loss: 3.8817863 ======\n",
      "=======Loss: 3.980709 ======\n",
      "=======Loss: 4.092083 ======\n",
      "=======Loss: 4.0415955 ======\n",
      "=======Loss: 4.1445518 ======\n",
      "=======Loss: 4.093696 ======\n",
      "=======Loss: 3.7831445 ======\n",
      "=======Loss: 3.998253 ======\n",
      "=======Loss: 3.8143482 ======\n",
      "=======Loss: 3.8864129 ======\n",
      "=======Loss: 4.044404 ======\n",
      "=======Loss: 3.7972326 ======\n",
      "=======Loss: 3.977441 ======\n",
      "=======Loss: 4.06632 ======\n",
      "=======Loss: 3.9268422 ======\n",
      "=======Loss: 3.8418741 ======\n",
      "=======Loss: 3.7064872 ======\n",
      "=======Loss: 3.9224 ======\n",
      "=======Loss: 3.9281387 ======\n",
      "=======Loss: 4.1092176 ======\n",
      "=======Loss: 4.009452 ======\n",
      "=======Loss: 4.0230646 ======\n",
      "=======Loss: 3.8845394 ======\n",
      "=======Loss: 3.8842144 ======\n",
      "=======Loss: 3.9500787 ======\n",
      "=======Loss: 3.8116975 ======\n",
      "=======Loss: 3.821085 ======\n",
      "=======Loss: 3.9690833 ======\n",
      "=======Loss: 3.7944772 ======\n",
      "=======Loss: 3.9913886 ======\n",
      "=======Loss: 3.9915934 ======\n",
      "=======Loss: 3.900794 ======\n",
      "=======Loss: 3.7649632 ======\n",
      "=======Loss: 3.901562 ======\n",
      "=======Loss: 4.025781 ======\n",
      "=======Loss: 3.9095519 ======\n",
      "=======Loss: 3.8187823 ======\n",
      "=======Loss: 4.148871 ======\n",
      "=======Loss: 4.1915293 ======\n",
      "=======Loss: 4.083819 ======\n",
      "=======Loss: 4.1554775 ======\n",
      "=======Loss: 3.8019085 ======\n",
      "=======Loss: 4.2239475 ======\n",
      "=======Loss: 4.172351 ======\n",
      "=======Loss: 3.9505863 ======\n",
      "=======Loss: 3.9666317 ======\n",
      "=======Loss: 3.9890876 ======\n",
      "=======Loss: 3.9895616 ======\n",
      "=======Loss: 3.8021545 ======\n",
      "=======Loss: 3.737507 ======\n",
      "=======Loss: 4.2448554 ======\n",
      "=======Loss: 3.9556959 ======\n",
      "=======Loss: 3.8779716 ======\n",
      "=======Loss: 3.8068938 ======\n",
      "=======Loss: 3.9253957 ======\n",
      "=======Loss: 3.8442345 ======\n",
      "=======Loss: 4.0577726 ======\n",
      "=======Loss: 3.994854 ======\n",
      "=======Loss: 3.6852512 ======\n",
      "=======Loss: 3.7709234 ======\n",
      "=======Loss: 3.9093819 ======\n",
      "=======Loss: 3.6440759 ======\n",
      "=======Loss: 4.0234494 ======\n",
      "=======Loss: 3.9879775 ======\n",
      "=======Loss: 4.025237 ======\n",
      "=======Loss: 3.8324468 ======\n",
      "=======Loss: 4.0796795 ======\n",
      "=======Loss: 3.7344298 ======\n",
      "=======Loss: 3.8255606 ======\n",
      "=======Loss: 3.803269 ======\n",
      "=======Loss: 3.7625933 ======\n",
      "=======Loss: 4.0205555 ======\n",
      "=======Loss: 3.8469987 ======\n",
      "=======Loss: 3.7284844 ======\n",
      "=======Loss: 4.0857487 ======\n",
      "=======Loss: 4.014744 ======\n",
      "=======Loss: 3.924876 ======\n",
      "=======Loss: 3.9758267 ======\n",
      "=======Loss: 3.8164911 ======\n",
      "=======Loss: 3.9385338 ======\n",
      "=======Loss: 3.9145136 ======\n",
      "=======Loss: 4.0235586 ======\n",
      "=======Loss: 4.0210156 ======\n",
      "=======Loss: 4.017494 ======\n",
      "=======Loss: 3.9679043 ======\n",
      "=======Loss: 4.194068 ======\n",
      "=======Loss: 3.8834317 ======\n",
      "=======Loss: 3.9875293 ======\n",
      "=======Loss: 4.009495 ======\n",
      "=======Loss: 3.852324 ======\n",
      "=======Loss: 3.5868294 ======\n",
      "=======Loss: 3.6465251 ======\n",
      "=======Loss: 3.7548866 ======\n",
      "=======Loss: 3.852487 ======\n",
      "=======Loss: 3.8506281 ======\n",
      "=======Loss: 4.063139 ======\n",
      "=======Loss: 3.723301 ======\n",
      "=======Loss: 3.791326 ======\n",
      "=======Loss: 4.054427 ======\n",
      "=======Loss: 3.9109116 ======\n",
      "=======Loss: 3.968533 ======\n",
      "=======Loss: 3.8431017 ======\n",
      "=======Loss: 3.8450718 ======\n",
      "=======Loss: 3.8260684 ======\n",
      "=======Loss: 3.9985752 ======\n",
      "=======Loss: 4.001585 ======\n",
      "=======Loss: 4.110922 ======\n",
      "=======Loss: 3.8498502 ======\n",
      "=======Loss: 3.7947414 ======\n",
      "=======Loss: 3.7530847 ======\n",
      "=======Loss: 3.9063392 ======\n",
      "=======Loss: 3.9271631 ======\n",
      "=======Loss: 3.8709116 ======\n",
      "=======Loss: 3.9876876 ======\n",
      "=======Loss: 3.8329203 ======\n",
      "=======Loss: 3.778944 ======\n",
      "=======Loss: 3.643113 ======\n",
      "=======Loss: 3.8929448 ======\n",
      "=======Loss: 3.9706817 ======\n",
      "=======Loss: 4.026963 ======\n",
      "=======Loss: 4.0894656 ======\n",
      "=======Loss: 3.9652214 ======\n",
      "=======Loss: 3.8901894 ======\n",
      "=======Loss: 3.857104 ======\n",
      "=======Loss: 3.969771 ======\n",
      "=======Loss: 3.7837553 ======\n",
      "=======Loss: 4.059869 ======\n",
      "=======Loss: 3.8181012 ======\n",
      "=======Loss: 3.8847075 ======\n",
      "=======Loss: 3.978467 ======\n",
      "=======Loss: 3.7374887 ======\n",
      "=======Loss: 3.8466165 ======\n",
      "=======Loss: 3.8501604 ======\n",
      "=======Loss: 3.8894029 ======\n",
      "=======Loss: 3.7734003 ======\n",
      "=======Loss: 3.7589962 ======\n",
      "=======Loss: 3.9939866 ======\n",
      "=======Loss: 3.780233 ======\n",
      "=======Loss: 3.9296427 ======\n",
      "=======Loss: 3.775367 ======\n",
      "=======Loss: 3.9249096 ======\n",
      "=======Loss: 3.7598429 ======\n",
      "=======Loss: 3.909524 ======\n",
      "=======Loss: 3.7717264 ======\n",
      "=======Loss: 3.8529303 ======\n",
      "=======Loss: 3.696095 ======\n",
      "=======Loss: 3.891701 ======\n",
      "=======Loss: 3.8290925 ======\n",
      "=======Loss: 3.7998602 ======\n",
      "=======Loss: 3.809712 ======\n",
      "=======Loss: 3.894077 ======\n",
      "=======Loss: 3.6914074 ======\n",
      "=======Loss: 3.9640899 ======\n",
      "=======Loss: 3.7735558 ======\n",
      "=======Loss: 3.757802 ======\n",
      "=======Loss: 3.8676636 ======\n",
      "=======Loss: 3.898918 ======\n",
      "=======Loss: 4.0409217 ======\n",
      "=======Loss: 3.7746482 ======\n",
      "=======Loss: 3.8146586 ======\n",
      "=======Loss: 4.0069737 ======\n",
      "=======Loss: 3.7301996 ======\n",
      "=======Loss: 3.7658472 ======\n",
      "=======Loss: 3.9577565 ======\n",
      "=======Loss: 4.0151563 ======\n",
      "=======Loss: 3.8774238 ======\n",
      "=======Loss: 3.7484045 ======\n",
      "=======Loss: 4.126945 ======\n",
      "=======Loss: 3.8448849 ======\n",
      "=======Loss: 3.7254524 ======\n",
      "=======Loss: 3.9099271 ======\n",
      "=======Loss: 3.7674732 ======\n",
      "=======Loss: 4.0326324 ======\n",
      "=======Loss: 4.2223887 ======\n",
      "=======Loss: 4.1515427 ======\n",
      "=======Loss: 3.844709 ======\n",
      "=======Loss: 3.8281689 ======\n",
      "=======Loss: 3.8171325 ======\n",
      "=======Loss: 3.955007 ======\n",
      "=======Loss: 3.7763348 ======\n",
      "=======Loss: 4.0942254 ======\n",
      "=======Loss: 3.8588254 ======\n",
      "=======Loss: 3.7361255 ======\n",
      "=======Loss: 3.8355265 ======\n",
      "=======Loss: 3.935414 ======\n",
      "=======Loss: 3.982236 ======\n",
      "=======Loss: 3.6997366 ======\n",
      "=======Loss: 3.6205668 ======\n",
      "=======Loss: 3.9342542 ======\n",
      "=======Loss: 3.589959 ======\n",
      "=======Loss: 3.6465695 ======\n",
      "=======Loss: 3.9204392 ======\n",
      "=======Loss: 4.101754 ======\n",
      "=======Loss: 3.880187 ======\n",
      "=======Loss: 3.9989667 ======\n",
      "=======Loss: 3.8950539 ======\n",
      "=======Loss: 3.7647514 ======\n",
      "=======Loss: 3.6184316 ======\n",
      "=======Loss: 3.7702832 ======\n",
      "=======Loss: 3.9013667 ======\n",
      "=======Loss: 3.8290257 ======\n",
      "=======Loss: 3.752153 ======\n",
      "=======Loss: 3.7907383 ======\n",
      "=======Loss: 3.519104 ======\n",
      "=======Loss: 3.7940516 ======\n",
      "=======Loss: 3.8929691 ======\n",
      "=======Loss: 3.7863631 ======\n",
      "=======Loss: 3.9482055 ======\n",
      "=======Loss: 3.8361335 ======\n",
      "=======Loss: 3.8416152 ======\n",
      "=======Loss: 3.8746607 ======\n",
      "=======Loss: 3.772574 ======\n",
      "=======Loss: 3.7705026 ======\n",
      "=======Loss: 3.9717467 ======\n",
      "=======Loss: 3.8810163 ======\n",
      "=======Loss: 3.7482543 ======\n",
      "=======Loss: 3.8856835 ======\n",
      "=======Loss: 3.921741 ======\n",
      "=======Loss: 3.6866403 ======\n",
      "=======Loss: 3.7577462 ======\n",
      "=======Loss: 3.7354307 ======\n",
      "=======Loss: 3.9610956 ======\n",
      "=======Loss: 3.6699646 ======\n",
      "=======Loss: 3.7537713 ======\n",
      "=======Loss: 3.5919337 ======\n",
      "=======Loss: 3.9352746 ======\n",
      "=======Loss: 3.6552076 ======\n",
      "=======Loss: 3.9651735 ======\n",
      "=======Loss: 3.8233867 ======\n",
      "=======Loss: 3.7413974 ======\n",
      "=======Loss: 3.8443112 ======\n",
      "=======Loss: 3.795723 ======\n",
      "=======Loss: 4.136576 ======\n",
      "=======Loss: 3.7481475 ======\n",
      "=======Loss: 3.6711965 ======\n",
      "=======Loss: 4.030738 ======\n",
      "=======Loss: 4.144558 ======\n",
      "=======Loss: 3.840057 ======\n",
      "=======Loss: 3.6801176 ======\n",
      "=======Loss: 3.7959485 ======\n",
      "=======Loss: 3.5569217 ======\n",
      "=======Loss: 3.8095708 ======\n",
      "=======Loss: 3.5044813 ======\n",
      "=======Loss: 3.7576904 ======\n",
      "=======Loss: 3.8124552 ======\n",
      "=======Loss: 4.0082912 ======\n",
      "=======Loss: 3.9973667 ======\n",
      "=======Loss: 3.942051 ======\n",
      "=======Loss: 3.709921 ======\n",
      "=======Loss: 3.8465748 ======\n",
      "=======Loss: 4.081522 ======\n",
      "=======Loss: 3.9530797 ======\n",
      "=======Loss: 3.6980639 ======\n",
      "=======Loss: 3.7788634 ======\n",
      "=======Loss: 3.881925 ======\n",
      "=======Loss: 3.949599 ======\n",
      "=======Loss: 3.9892235 ======\n",
      "=======Loss: 3.7326722 ======\n",
      "=======Loss: 3.8916883 ======\n",
      "=======Loss: 3.6272788 ======\n",
      "=======Loss: 3.7341979 ======\n",
      "=======Loss: 3.6862044 ======\n",
      "=======Loss: 3.6694467 ======\n",
      "=======Loss: 3.7658854 ======\n",
      "=======Loss: 3.6571167 ======\n",
      "=======Loss: 3.7425199 ======\n",
      "=======Loss: 4.02969 ======\n",
      "=======Loss: 3.804761 ======\n",
      "=======Loss: 3.8215113 ======\n",
      "=======Loss: 3.9621654 ======\n",
      "=======Loss: 3.7533872 ======\n",
      "=======Loss: 3.8339243 ======\n",
      "=======Loss: 3.6700988 ======\n",
      "=======Loss: 3.9264548 ======\n",
      "=======Loss: 3.922919 ======\n",
      "=======Loss: 3.6414351 ======\n",
      "=======Loss: 3.8377824 ======\n",
      "=======Loss: 3.9713228 ======\n",
      "=======Loss: 4.073184 ======\n",
      "=======Loss: 3.8089266 ======\n",
      "=======Loss: 3.7736225 ======\n",
      "=======Loss: 3.8939211 ======\n",
      "=======Loss: 3.997381 ======\n",
      "=======Loss: 3.7186427 ======\n",
      "=======Loss: 3.9491596 ======\n",
      "=======Loss: 3.8262239 ======\n",
      "=======Loss: 3.9048467 ======\n",
      "=======Loss: 3.8844864 ======\n",
      "=======Loss: 3.845902 ======\n",
      "=======Loss: 4.1377387 ======\n",
      "=======Loss: 3.9281096 ======\n",
      "=======Loss: 3.7373657 ======\n",
      "=======Loss: 3.7190576 ======\n",
      "=======Loss: 3.7261815 ======\n",
      "=======Loss: 3.7933774 ======\n",
      "=======Loss: 3.8048694 ======\n",
      "=======Loss: 3.8519864 ======\n",
      "=======Loss: 3.6844857 ======\n",
      "=======Loss: 3.6615925 ======\n",
      "=======Loss: 3.9157631 ======\n",
      "=======Loss: 3.9137914 ======\n",
      "=======Loss: 3.5840597 ======\n",
      "=======Loss: 3.7174375 ======\n",
      "=======Loss: 3.6886075 ======\n",
      "=======Loss: 3.7843912 ======\n",
      "=======Loss: 3.782421 ======\n",
      "=======Loss: 3.6527212 ======\n",
      "=======Loss: 3.6852088 ======\n",
      "=======Loss: 3.6156383 ======\n",
      "=======Loss: 3.67845 ======\n",
      "=======Loss: 3.670516 ======\n",
      "=======Loss: 4.0349245 ======\n",
      "=======Loss: 3.840178 ======\n",
      "=======Loss: 3.6343353 ======\n",
      "=======Loss: 3.7731748 ======\n",
      "=======Loss: 3.8315039 ======\n",
      "=======Loss: 3.8908796 ======\n",
      "=======Loss: 3.8842483 ======\n",
      "=======Loss: 3.8471346 ======\n",
      "=======Loss: 3.6258063 ======\n",
      "=======Loss: 3.8058853 ======\n",
      "=======Loss: 3.7620382 ======\n",
      "=======Loss: 3.6889105 ======\n",
      "=======Loss: 3.9661562 ======\n",
      "=======Loss: 3.795828 ======\n",
      "=======Loss: 3.806487 ======\n",
      "=======Loss: 3.9723942 ======\n",
      "=======Loss: 3.768952 ======\n",
      "=======Loss: 3.8338935 ======\n",
      "=======Loss: 3.7055926 ======\n",
      "=======Loss: 3.9540973 ======\n",
      "=======Loss: 3.5786295 ======\n",
      "=======Loss: 3.7766068 ======\n",
      "=======Loss: 3.6067286 ======\n",
      "=======Loss: 3.865617 ======\n",
      "=======Loss: 3.8997757 ======\n",
      "=======Loss: 3.7818577 ======\n",
      "=======Loss: 3.932744 ======\n",
      "=======Loss: 3.8159728 ======\n",
      "=======Loss: 3.7514918 ======\n",
      "=======Loss: 3.6887503 ======\n",
      "=======Loss: 3.8633327 ======\n",
      "=======Loss: 3.7190385 ======\n",
      "=======Loss: 3.7792294 ======\n",
      "=======Loss: 3.7326326 ======\n",
      "=======Loss: 3.7966099 ======\n",
      "=======Loss: 3.8535473 ======\n",
      "=======Loss: 3.7589452 ======\n",
      "=======Loss: 3.8970308 ======\n",
      "=======Loss: 3.7492366 ======\n",
      "=======Loss: 3.7720327 ======\n",
      "=======Loss: 3.5693908 ======\n",
      "=======Loss: 4.0013437 ======\n",
      "=======Loss: 3.8455033 ======\n",
      "=======Loss: 3.6945305 ======\n",
      "=======Loss: 3.5391603 ======\n",
      "=======Loss: 3.891499 ======\n",
      "=======Loss: 3.7376857 ======\n",
      "=======Loss: 3.912251 ======\n",
      "=======Loss: 3.9551716 ======\n",
      "=======Loss: 3.4662452 ======\n",
      "=======Loss: 3.60132 ======\n",
      "=======Loss: 3.7183838 ======\n",
      "=======Loss: 3.835659 ======\n",
      "=======Loss: 3.7771704 ======\n",
      "=======Loss: 3.8193092 ======\n",
      "=======Loss: 3.716361 ======\n",
      "=======Loss: 3.7251775 ======\n",
      "=======Loss: 3.6506748 ======\n",
      "=======Loss: 3.5462966 ======\n",
      "=======Loss: 3.5180552 ======\n",
      "=======Loss: 3.5741224 ======\n",
      "=======Loss: 3.76948 ======\n",
      "=======Loss: 3.9525728 ======\n",
      "=======Loss: 3.6265123 ======\n",
      "=======Loss: 3.7475343 ======\n",
      "=======Loss: 3.9718368 ======\n",
      "=======Loss: 3.8838196 ======\n",
      "=======Loss: 3.5501375 ======\n",
      "=======Loss: 3.694429 ======\n",
      "=======Loss: 3.7625952 ======\n",
      "=======Loss: 3.7584193 ======\n",
      "=======Loss: 3.8237948 ======\n",
      "=======Loss: 3.599966 ======\n",
      "=======Loss: 3.9941795 ======\n",
      "=======Loss: 3.8058414 ======\n",
      "=======Loss: 3.823255 ======\n",
      "=======Loss: 3.709303 ======\n",
      "=======Loss: 3.7157776 ======\n",
      "=======Loss: 3.5274591 ======\n",
      "=======Loss: 3.811603 ======\n",
      "=======Loss: 3.8202143 ======\n",
      "=======Loss: 3.5353637 ======\n",
      "=======Loss: 3.8510506 ======\n",
      "=======Loss: 3.6483197 ======\n",
      "=======Loss: 3.9139848 ======\n",
      "=======Loss: 3.8849645 ======\n",
      "=======Loss: 3.8730836 ======\n",
      "=======Loss: 3.6061788 ======\n",
      "=======Loss: 3.6144516 ======\n",
      "=======Loss: 3.677997 ======\n",
      "=======Loss: 3.7170856 ======\n",
      "=======Loss: 3.8139567 ======\n",
      "=======Loss: 3.7901213 ======\n",
      "=======Loss: 3.686751 ======\n",
      "=======Loss: 3.7131054 ======\n",
      "=======Loss: 3.8545976 ======\n",
      "=======Loss: 3.852479 ======\n",
      "=======Loss: 3.8204966 ======\n",
      "=======Loss: 3.5557165 ======\n",
      "=======Loss: 3.8296661 ======\n",
      "=======Loss: 3.7730594 ======\n",
      "=======Loss: 4.042388 ======\n",
      "=======Loss: 3.691812 ======\n",
      "=======Loss: 3.6987534 ======\n",
      "=======Loss: 3.6277885 ======\n",
      "=======Loss: 3.91794 ======\n",
      "=======Loss: 3.7898526 ======\n",
      "=======Loss: 3.7411587 ======\n",
      "=======Loss: 3.8587208 ======\n",
      "=======Loss: 3.8235714 ======\n",
      "=======Loss: 3.6085808 ======\n",
      "=======Loss: 3.6603851 ======\n",
      "=======Loss: 3.7382321 ======\n",
      "=======Loss: 3.9585981 ======\n",
      "=======Loss: 3.8162827 ======\n",
      "=======Loss: 3.5522265 ======\n",
      "=======Loss: 3.4654322 ======\n",
      "=======Loss: 3.8461652 ======\n",
      "=======Loss: 3.754187 ======\n",
      "=======Loss: 3.6435003 ======\n",
      "=======Loss: 3.5999436 ======\n",
      "=======Loss: 3.473913 ======\n",
      "=======Loss: 3.5056448 ======\n",
      "=======Loss: 3.7592494 ======\n",
      "=======Loss: 3.6459992 ======\n",
      "=======Loss: 3.4869955 ======\n",
      "=======Loss: 3.8061535 ======\n",
      "=======Loss: 3.5683317 ======\n",
      "=======Loss: 3.781427 ======\n",
      "=======Loss: 3.8591356 ======\n",
      "=======Loss: 3.6712484 ======\n",
      "=======Loss: 3.710064 ======\n",
      "=======Loss: 3.7105474 ======\n",
      "=======Loss: 3.5941598 ======\n",
      "=======Loss: 3.415991 ======\n",
      "=======Loss: 3.5952246 ======\n",
      "=======Loss: 3.610758 ======\n",
      "=======Loss: 3.5821881 ======\n",
      "=======Loss: 3.9863102 ======\n",
      "=======Loss: 3.7623067 ======\n",
      "=======Loss: 3.8022015 ======\n",
      "=======Loss: 3.593656 ======\n",
      "=======Loss: 4.017002 ======\n",
      "=======Loss: 3.7094202 ======\n",
      "=======Loss: 3.584451 ======\n",
      "=======Loss: 3.486336 ======\n",
      "=======Loss: 3.8171287 ======\n",
      "=======Loss: 3.726806 ======\n",
      "=======Loss: 3.8896227 ======\n",
      "=======Loss: 4.0456734 ======\n",
      "=======Loss: 3.7583423 ======\n",
      "=======Loss: 3.5811512 ======\n",
      "=======Loss: 3.6448224 ======\n",
      "=======Loss: 3.5845933 ======\n",
      "=======Loss: 3.6879478 ======\n",
      "=======Loss: 3.6704085 ======\n",
      "=======Loss: 3.8201354 ======\n",
      "=======Loss: 3.8038552 ======\n",
      "=======Loss: 3.3518314 ======\n",
      "=======Loss: 3.9057095 ======\n",
      "=======Loss: 3.8890944 ======\n",
      "=======Loss: 3.6347811 ======\n",
      "=======Loss: 3.9106772 ======\n",
      "=======Loss: 3.67127 ======\n",
      "=======Loss: 3.6954818 ======\n",
      "=======Loss: 3.629055 ======\n",
      "=======Loss: 3.5766382 ======\n",
      "=======Loss: 3.70649 ======\n",
      "=======Loss: 3.581146 ======\n",
      "=======Loss: 3.4949062 ======\n",
      "=======Loss: 3.7128878 ======\n",
      "=======Loss: 3.7222419 ======\n",
      "=======Loss: 3.797127 ======\n",
      "=======Loss: 3.4675713 ======\n",
      "=======Loss: 3.7734277 ======\n",
      "=======Loss: 3.5997584 ======\n",
      "=======Loss: 3.7693157 ======\n",
      "=======Loss: 3.5605965 ======\n",
      "=======Loss: 3.560688 ======\n",
      "=======Loss: 3.692833 ======\n",
      "=======Loss: 3.6476834 ======\n",
      "=======Loss: 3.908041 ======\n",
      "=======Loss: 3.7029686 ======\n",
      "=======Loss: 3.636084 ======\n",
      "=======Loss: 3.672861 ======\n",
      "=======Loss: 3.547123 ======\n",
      "=======Loss: 3.6281614 ======\n",
      "=======Loss: 3.8433235 ======\n",
      "=======Loss: 3.6386352 ======\n",
      "=======Loss: 3.5252197 ======\n",
      "=======Loss: 3.5867503 ======\n",
      "=======Loss: 3.614926 ======\n",
      "=======Loss: 3.6679144 ======\n",
      "=======Loss: 3.639543 ======\n",
      "=======Loss: 3.7235973 ======\n",
      "=======Loss: 3.8824303 ======\n",
      "=======Loss: 3.858603 ======\n",
      "=======Loss: 3.65666 ======\n",
      "=======Loss: 3.5294008 ======\n",
      "=======Loss: 3.6469293 ======\n",
      "=======Loss: 3.6079018 ======\n",
      "=======Loss: 3.6519575 ======\n",
      "=======Loss: 3.656051 ======\n",
      "=======Loss: 3.475697 ======\n",
      "=======Loss: 3.72684 ======\n",
      "=======Loss: 3.7432327 ======\n",
      "=======Loss: 3.6543808 ======\n",
      "=======Loss: 3.8373132 ======\n",
      "=======Loss: 3.8358655 ======\n",
      "=======Loss: 3.9153864 ======\n",
      "=======Loss: 3.9460354 ======\n",
      "=======Loss: 3.6525996 ======\n",
      "=======Loss: 3.8979597 ======\n",
      "=======Loss: 3.7984104 ======\n",
      "=======Loss: 3.583946 ======\n",
      "=======Loss: 3.963032 ======\n",
      "=======Loss: 3.6847093 ======\n",
      "=======Loss: 3.4714832 ======\n",
      "=======Loss: 3.750409 ======\n",
      "=======Loss: 3.78742 ======\n",
      "=======Loss: 3.7296097 ======\n",
      "=======Loss: 3.6277108 ======\n",
      "=======Loss: 3.6907105 ======\n",
      "=======Loss: 3.4928048 ======\n",
      "=======Loss: 3.6582701 ======\n",
      "=======Loss: 3.8298652 ======\n",
      "=======Loss: 3.8113108 ======\n",
      "=======Loss: 3.541139 ======\n",
      "=======Loss: 3.730887 ======\n",
      "=======Loss: 3.7400823 ======\n",
      "=======Loss: 3.9578996 ======\n",
      "=======Loss: 3.657373 ======\n",
      "=======Loss: 3.4572814 ======\n",
      "=======Loss: 3.6392174 ======\n",
      "=======Loss: 3.71692 ======\n",
      "=======Loss: 3.806674 ======\n",
      "=======Loss: 3.4794416 ======\n",
      "=======Loss: 3.687971 ======\n",
      "=======Loss: 3.691316 ======\n",
      "=======Loss: 3.8212242 ======\n",
      "=======Loss: 3.7577977 ======\n",
      "=======Loss: 3.4315217 ======\n",
      "=======Loss: 3.7217567 ======\n",
      "=======Loss: 3.4978676 ======\n",
      "=======Loss: 3.628139 ======\n",
      "=======Loss: 3.6521974 ======\n",
      "=======Loss: 3.6540146 ======\n",
      "=======Loss: 3.6944363 ======\n",
      "=======Loss: 3.6231542 ======\n",
      "=======Loss: 3.7019205 ======\n",
      "=======Loss: 3.553646 ======\n",
      "=======Loss: 3.417572 ======\n",
      "=======Loss: 3.8308601 ======\n",
      "=======Loss: 3.4580932 ======\n",
      "=======Loss: 3.7051187 ======\n",
      "=======Loss: 3.598524 ======\n",
      "=======Loss: 3.6158383 ======\n",
      "=======Loss: 3.6730874 ======\n",
      "=======Loss: 3.7254333 ======\n",
      "=======Loss: 3.4984694 ======\n",
      "=======Loss: 4.011816 ======\n",
      "=======Loss: 3.7868958 ======\n",
      "=======Loss: 3.6479132 ======\n",
      "=======Loss: 3.6721535 ======\n",
      "=======Loss: 3.542902 ======\n",
      "=======Loss: 3.5765405 ======\n",
      "=======Loss: 3.694579 ======\n",
      "=======Loss: 3.886035 ======\n",
      "=======Loss: 3.8551016 ======\n",
      "=======Loss: 3.5666752 ======\n",
      "=======Loss: 3.7476118 ======\n",
      "=======Loss: 3.5257864 ======\n",
      "=======Loss: 3.7026682 ======\n",
      "=======Loss: 3.5132272 ======\n",
      "=======Loss: 3.7741818 ======\n",
      "=======Loss: 3.739028 ======\n",
      "=======Loss: 3.7412255 ======\n",
      "=======Loss: 3.4761436 ======\n",
      "=======Loss: 3.655428 ======\n",
      "=======Loss: 3.8206875 ======\n",
      "=======Loss: 3.8827248 ======\n",
      "=======Loss: 3.5633073 ======\n",
      "=======Loss: 3.764741 ======\n",
      "=======Loss: 3.6236925 ======\n",
      "=======Loss: 3.893959 ======\n",
      "=======Loss: 3.6037917 ======\n",
      "=======Loss: 3.65154 ======\n",
      "=======Loss: 3.636516 ======\n",
      "=======Loss: 3.7764277 ======\n",
      "=======Loss: 3.631592 ======\n",
      "=======Loss: 3.6510377 ======\n",
      "=======Loss: 3.770084 ======\n",
      "=======Loss: 3.7281382 ======\n",
      "=======Loss: 3.956759 ======\n",
      "=======Loss: 3.8821096 ======\n",
      "=======Loss: 3.744532 ======\n",
      "=======Loss: 3.676952 ======\n",
      "=======Loss: 3.5147057 ======\n",
      "=======Loss: 3.9396956 ======\n",
      "=======Loss: 3.4577403 ======\n",
      "=======Loss: 3.5841708 ======\n",
      "=======Loss: 3.7318268 ======\n",
      "=======Loss: 3.7456703 ======\n",
      "=======Loss: 3.7587142 ======\n",
      "=======Loss: 3.7086246 ======\n",
      "=======Loss: 3.534696 ======\n",
      "=======Loss: 3.673796 ======\n",
      "=======Loss: 3.7376475 ======\n",
      "=======Loss: 3.6409867 ======\n",
      "=======Loss: 3.698148 ======\n",
      "=======Loss: 3.8446214 ======\n",
      "=======Loss: 3.7116556 ======\n",
      "=======Loss: 3.598908 ======\n",
      "=======Loss: 3.5918484 ======\n",
      "=======Loss: 3.5294032 ======\n",
      "=======Loss: 3.854773 ======\n",
      "=======Loss: 3.6418428 ======\n",
      "=======Loss: 3.5533595 ======\n",
      "=======Loss: 3.5662978 ======\n",
      "=======Loss: 3.6225314 ======\n",
      "=======Loss: 3.6965237 ======\n",
      "=======Loss: 3.7363968 ======\n",
      "=======Loss: 3.5524924 ======\n",
      "=======Loss: 3.6544306 ======\n",
      "=======Loss: 3.634121 ======\n",
      "=======Loss: 3.5392034 ======\n",
      "=======Loss: 3.375702 ======\n",
      "=======Loss: 3.858696 ======\n",
      "=======Loss: 3.6842768 ======\n",
      "=======Loss: 3.7222693 ======\n",
      "=======Loss: 3.849922 ======\n",
      "=======Loss: 3.5415564 ======\n",
      "=======Loss: 3.5652952 ======\n",
      "=======Loss: 3.8155885 ======\n",
      "=======Loss: 3.5207295 ======\n",
      "=======Loss: 3.7515707 ======\n",
      "=======Loss: 3.6321173 ======\n",
      "=======Loss: 3.806553 ======\n",
      "=======Loss: 3.5806363 ======\n",
      "=======Loss: 3.823051 ======\n",
      "=======Loss: 3.79747 ======\n",
      "=======Loss: 3.6487436 ======\n",
      "=======Loss: 3.6311555 ======\n",
      "=======Loss: 3.572754 ======\n",
      "=======Loss: 3.7019773 ======\n",
      "=======Loss: 3.664086 ======\n",
      "=======Loss: 3.9381435 ======\n",
      "=======Loss: 3.547041 ======\n",
      "=======Loss: 3.7092502 ======\n",
      "=======Loss: 3.7261033 ======\n",
      "=======Loss: 3.764369 ======\n",
      "=======Loss: 3.765973 ======\n",
      "=======Loss: 3.3767252 ======\n",
      "=======Loss: 3.5315778 ======\n",
      "=======Loss: 3.671988 ======\n",
      "=======Loss: 3.8706257 ======\n",
      "=======Loss: 3.7258737 ======\n",
      "=======Loss: 3.5762656 ======\n",
      "=======Loss: 3.450708 ======\n",
      "=======Loss: 3.6269102 ======\n",
      "=======Loss: 3.6468356 ======\n",
      "=======Loss: 3.5824785 ======\n",
      "=======Loss: 3.7312617 ======\n",
      "=======Loss: 3.3914094 ======\n",
      "=======Loss: 3.3467557 ======\n",
      "=======Loss: 3.6986737 ======\n",
      "=======Loss: 3.5311127 ======\n",
      "=======Loss: 3.9610293 ======\n",
      "=======Loss: 3.511053 ======\n",
      "=======Loss: 3.552868 ======\n",
      "=======Loss: 3.8916578 ======\n",
      "=======Loss: 3.7318268 ======\n",
      "=======Loss: 3.5546238 ======\n",
      "=======Loss: 3.6470292 ======\n",
      "=======Loss: 3.4914584 ======\n",
      "=======Loss: 3.61188 ======\n",
      "=======Loss: 3.6296945 ======\n",
      "=======Loss: 3.5211728 ======\n",
      "=======Loss: 3.8894806 ======\n",
      "=======Loss: 3.683601 ======\n",
      "=======Loss: 3.7812054 ======\n",
      "=======Loss: 3.809779 ======\n",
      "=======Loss: 3.8080041 ======\n",
      "=======Loss: 3.4533615 ======\n",
      "=======Loss: 3.6405354 ======\n",
      "=======Loss: 3.6911416 ======\n",
      "=======Loss: 3.4193144 ======\n",
      "=======Loss: 3.5559344 ======\n",
      "=======Loss: 3.7645907 ======\n",
      "=======Loss: 3.4232714 ======\n",
      "=======Loss: 3.590408 ======\n",
      "=======Loss: 3.714758 ======\n",
      "=======Loss: 3.591394 ======\n",
      "=======Loss: 3.432503 ======\n",
      "=======Loss: 3.6560822 ======\n",
      "=======Loss: 3.840291 ======\n",
      "=======Loss: 3.7160306 ======\n",
      "=======Loss: 3.7123733 ======\n",
      "=======Loss: 3.57026 ======\n",
      "=======Loss: 3.679609 ======\n",
      "=======Loss: 3.546346 ======\n",
      "=======Loss: 3.6435568 ======\n",
      "=======Loss: 3.7855275 ======\n",
      "=======Loss: 3.5802796 ======\n",
      "=======Loss: 3.5340445 ======\n",
      "=======Loss: 3.8110104 ======\n",
      "=======Loss: 3.5622187 ======\n",
      "=======Loss: 3.4129157 ======\n",
      "=======Loss: 3.7148762 ======\n",
      "=======Loss: 3.5387986 ======\n",
      "=======Loss: 3.6815612 ======\n",
      "=======Loss: 3.5801759 ======\n",
      "=======Loss: 3.5678697 ======\n",
      "=======Loss: 3.52255 ======\n",
      "=======Loss: 3.4961782 ======\n",
      "=======Loss: 3.5747075 ======\n",
      "=======Loss: 3.558929 ======\n",
      "=======Loss: 3.466447 ======\n",
      "=======Loss: 3.5305524 ======\n",
      "=======Loss: 3.4559948 ======\n",
      "=======Loss: 3.6320953 ======\n",
      "=======Loss: 3.8252354 ======\n",
      "=======Loss: 3.6872087 ======\n",
      "=======Loss: 3.6146379 ======\n",
      "=======Loss: 3.6591573 ======\n",
      "=======Loss: 3.683215 ======\n",
      "=======Loss: 3.7204313 ======\n",
      "=======Loss: 3.8978555 ======\n",
      "=======Loss: 3.7305942 ======\n",
      "=======Loss: 3.828161 ======\n",
      "=======Loss: 3.560245 ======\n",
      "=======Loss: 3.5410476 ======\n",
      "=======Loss: 3.5573869 ======\n",
      "=======Loss: 3.5995324 ======\n",
      "=======Loss: 3.7377656 ======\n",
      "=======Loss: 3.3758337 ======\n",
      "=======Loss: 3.491359 ======\n",
      "=======Loss: 3.624949 ======\n",
      "=======Loss: 3.677885 ======\n",
      "=======Loss: 3.4706216 ======\n",
      "=======Loss: 3.694746 ======\n",
      "=======Loss: 3.7930741 ======\n",
      "=======Loss: 3.5621498 ======\n",
      "=======Loss: 3.7479496 ======\n",
      "=======Loss: 3.6766248 ======\n",
      "=======Loss: 3.6873293 ======\n",
      "=======Loss: 3.6758108 ======\n",
      "=======Loss: 3.5796986 ======\n",
      "=======Loss: 3.767305 ======\n",
      "=======Loss: 3.5429578 ======\n",
      "=======Loss: 3.9090047 ======\n",
      "=======Loss: 3.5902367 ======\n",
      "=======Loss: 3.6024685 ======\n",
      "=======Loss: 3.7433715 ======\n",
      "=======Loss: 3.5973194 ======\n",
      "=======Loss: 3.5569663 ======\n",
      "=======Loss: 3.7065542 ======\n",
      "=======Loss: 3.4263477 ======\n",
      "=======Loss: 3.6899374 ======\n",
      "=======Loss: 3.854018 ======\n",
      "=======Loss: 3.7157927 ======\n",
      "=======Loss: 3.6917264 ======\n",
      "=======Loss: 3.6236382 ======\n",
      "=======Loss: 3.5733385 ======\n",
      "=======Loss: 3.6094491 ======\n",
      "=======Loss: 3.5607138 ======\n",
      "=======Loss: 3.6182752 ======\n",
      "=======Loss: 3.641584 ======\n",
      "=======Loss: 3.6997917 ======\n",
      "=======Loss: 3.8887963 ======\n",
      "=======Loss: 3.6182506 ======\n",
      "=======Loss: 3.4931033 ======\n",
      "=======Loss: 3.630783 ======\n",
      "=======Loss: 3.5549088 ======\n",
      "=======Loss: 3.4526634 ======\n",
      "=======Loss: 3.764326 ======\n",
      "=======Loss: 3.7774193 ======\n",
      "=======Loss: 3.5575142 ======\n",
      "=======Loss: 3.7096746 ======\n",
      "=======Loss: 3.6233606 ======\n",
      "=======Loss: 3.8832092 ======\n",
      "=======Loss: 3.7287407 ======\n",
      "=======Loss: 3.5149217 ======\n",
      "=======Loss: 3.518812 ======\n",
      "=======Loss: 3.454916 ======\n",
      "=======Loss: 3.6137295 ======\n",
      "=======Loss: 3.6280525 ======\n",
      "=======Loss: 3.6011739 ======\n",
      "=======Loss: 3.7655733 ======\n",
      "=======Loss: 3.7122235 ======\n",
      "=======Loss: 3.7380404 ======\n",
      "=======Loss: 3.5821092 ======\n",
      "=======Loss: 3.7093115 ======\n",
      "=======Loss: 3.5974216 ======\n",
      "=======Loss: 3.7075496 ======\n",
      "=======Loss: 3.5278602 ======\n",
      "=======Loss: 3.5700874 ======\n",
      "=======Loss: 3.692321 ======\n",
      "=======Loss: 4.015019 ======\n",
      "=======Loss: 3.6138453 ======\n",
      "=======Loss: 3.4837294 ======\n",
      "=======Loss: 3.642075 ======\n",
      "=======Loss: 3.4923263 ======\n",
      "=======Loss: 3.6339087 ======\n",
      "=======Loss: 3.610449 ======\n",
      "=======Loss: 3.7058394 ======\n",
      "=======Loss: 3.7145422 ======\n"
     ]
    }
   ],
   "source": [
    "n_params = count_model_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# %%\n",
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=8e-4, \n",
    "                                weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2, reduce_dims=(0,1))\n",
    "# h1loss = H1Loss(d=2, reduce_dims=(0,1))\n",
    "\n",
    "train_loss = l2loss\n",
    "eval_losses={'l2': l2loss} #'h1': h1loss, \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()\n",
    "\n",
    "step = 0\n",
    "\n",
    "with open('script/sfno_loss.txt', 'w') as f:\n",
    "    for epoch in range(20):\n",
    "        avg_loss = 0\n",
    "        train_err = 0.0\n",
    "        \n",
    "        # track number of training examples in batch\n",
    "        n_samples = 0\n",
    "        for idx, sample in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            sample = {\n",
    "                k: v.to(device)\n",
    "                for k, v in sample.items()\n",
    "                if torch.is_tensor(v)\n",
    "            }\n",
    "\n",
    "            n_samples += sample[\"y\"].shape[0]\n",
    "            out = model(sample[\"x\"])\n",
    "\n",
    "            loss = l2loss(out, **sample)\n",
    "\n",
    "            loss.backward()\n",
    "            del out\n",
    "\n",
    "            optimizer.step()\n",
    "            train_err += loss.item()\n",
    "            with torch.no_grad():\n",
    "                print(\"=======Loss:\",loss.detach().cpu().numpy(),\"======\")\n",
    "                f.write(f'Step {step + 1}, Loss: {loss.item()}\\n')\n",
    "                step += 1\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_err)\n",
    "        else:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=1\n",
      "Test Loss: 0.9659 ± 0.0563\n"
     ]
    }
   ],
   "source": [
    "resolution = (32, 64)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (64, 128) with 50 samples and batch-size=1\n",
      "Test Loss: 2.4205 ± 0.0495\n"
     ]
    }
   ],
   "source": [
    "resolution = (64, 128)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (128, 256) with 50 samples and batch-size=1\n",
      "Test Loss: 2.9528 ± 0.0359\n"
     ]
    }
   ],
   "source": [
    "resolution = (128, 256)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f} ± {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time: 0.005950450897216797\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "outputs = model(inputs)\n",
    "print(\"test time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "    # Input x\n",
    "    x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "    # Ground-truth\n",
    "    y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "    # Model prediction\n",
    "    x_in = x.unsqueeze(0).to(device)\n",
    "    out = model(x_in).squeeze()[0, ...].detach().cpu().numpy()\n",
    "    x = x[0, ...].detach().numpy()\n",
    "\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"./script/output_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "#     # Input x\n",
    "#     x = torch.tensor(np.load(\"../../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "#     # Ground-truth\n",
    "#     y = np.load(\"../../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "#     x = x[0, ...].detach().numpy()\n",
    "    \n",
    "#     plt.imshow(x)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/input_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    \n",
    "#     plt.imshow(y)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/label_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
