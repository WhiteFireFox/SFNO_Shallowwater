{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Fourier Neural Operators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=10\n",
      "Loading test dataloader at resolution (64, 128) with 50 samples and batch-size=10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sfno import SFNO, LpLoss, H1Loss\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from neuralop.datasets import load_spherical_swe\n",
    "from neuralop.utils import count_model_params\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# %%\n",
    "# Loading the Navier-Stokes dataset in 128x128 resolution\n",
    "train_loader, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=(32, 64),\n",
    "                                                test_resolutions=[(32, 64), (64, 128)], n_tests=[50, 50], test_batch_sizes=[10, 10])\n",
    "\n",
    "model = SFNO(img_size=(32, 64), grid=\"equiangular\", num_layers=4, scale_factor=3, embed_dim=32, \n",
    "             big_skip=True, use_embed=True, use_nonlinear=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 91753 parameters.\n",
      "\n",
      "### MODEL ###\n",
      " SFNO(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GELU(approximate='none')\n",
      "  )\n",
      "  (trans_down): RealSHT(\n",
      "    nlat=32, nlon=64,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=equiangular, csphase=True\n",
      "  )\n",
      "  (itrans_up): InverseRealSHT(\n",
      "    nlat=32, nlon=64,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=equiangular, csphase=True\n",
      "  )\n",
      "  (trans): RealSHT(\n",
      "    nlat=10, nlon=21,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=legendre-gauss, csphase=True\n",
      "  )\n",
      "  (itrans): InverseRealSHT(\n",
      "    nlat=10, nlon=21,\n",
      "     lmax=10, mmax=10,\n",
      "     grid=legendre-gauss, csphase=True\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): SFNO_Block(\n",
      "      (filter): SphericalConv(\n",
      "        (forward_transform): RealSHT(\n",
      "          nlat=32, nlon=64,\n",
      "           lmax=10, mmax=10,\n",
      "           grid=equiangular, csphase=True\n",
      "        )\n",
      "        (inverse_transform): InverseRealSHT(\n",
      "          nlat=10, nlon=21,\n",
      "           lmax=10, mmax=10,\n",
      "           grid=legendre-gauss, csphase=True\n",
      "        )\n",
      "        (first_forward_transform): RealSHT_(\n",
      "          (_SHT_cache): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (inner_skip): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act_layer): GELU(approximate='none')\n",
      "      (outer_skip): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1-2): 2 x SFNO_Block(\n",
      "      (filter): SphericalConv(\n",
      "        (forward_transform): RealSHT(\n",
      "          nlat=10, nlon=21,\n",
      "           lmax=10, mmax=10,\n",
      "           grid=legendre-gauss, csphase=True\n",
      "        )\n",
      "        (inverse_transform): InverseRealSHT(\n",
      "          nlat=10, nlon=21,\n",
      "           lmax=10, mmax=10,\n",
      "           grid=legendre-gauss, csphase=True\n",
      "        )\n",
      "      )\n",
      "      (inner_skip): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act_layer): GELU(approximate='none')\n",
      "      (outer_skip): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (3): SFNO_Block(\n",
      "      (filter): SphericalConv(\n",
      "        (forward_transform): RealSHT(\n",
      "          nlat=10, nlon=21,\n",
      "           lmax=10, mmax=10,\n",
      "           grid=legendre-gauss, csphase=True\n",
      "        )\n",
      "        (inverse_transform): InverseRealSHT(\n",
      "          nlat=32, nlon=64,\n",
      "           lmax=10, mmax=10,\n",
      "           grid=equiangular, csphase=True\n",
      "        )\n",
      "        (last_inverse_transform): IRealSHT_(\n",
      "          (_iSHT_cache): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "      (inner_skip): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (act_layer): GELU(approximate='none')\n",
      "      (outer_skip): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv2d(35, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0008\n",
      "    lr: 0.0008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001FE93DFB880>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <sfno.loss.LpLoss object at 0x000001FE93DFB610>\n",
      "\n",
      " * Test: {'l2': <sfno.loss.LpLoss object at 0x000001FE93DFB610>}\n",
      "=======Loss: 13.94716 ======\n",
      "=======Loss: 12.584829 ======\n",
      "=======Loss: 12.291861 ======\n",
      "=======Loss: 11.895454 ======\n",
      "=======Loss: 11.1801405 ======\n",
      "=======Loss: 11.153127 ======\n",
      "=======Loss: 10.89096 ======\n",
      "=======Loss: 10.735158 ======\n",
      "=======Loss: 10.460342 ======\n",
      "=======Loss: 10.190207 ======\n",
      "=======Loss: 10.061945 ======\n",
      "=======Loss: 10.062851 ======\n",
      "=======Loss: 9.786497 ======\n",
      "=======Loss: 9.565045 ======\n",
      "=======Loss: 9.3112755 ======\n",
      "=======Loss: 9.142548 ======\n",
      "=======Loss: 8.902312 ======\n",
      "=======Loss: 8.879603 ======\n",
      "=======Loss: 8.56468 ======\n",
      "=======Loss: 8.601754 ======\n",
      "=======Loss: 8.629976 ======\n",
      "=======Loss: 8.427205 ======\n",
      "=======Loss: 8.234848 ======\n",
      "=======Loss: 8.079577 ======\n",
      "=======Loss: 8.080354 ======\n",
      "=======Loss: 7.932983 ======\n",
      "=======Loss: 7.6806264 ======\n",
      "=======Loss: 7.9894743 ======\n",
      "=======Loss: 7.4753246 ======\n",
      "=======Loss: 7.611034 ======\n",
      "=======Loss: 7.4699926 ======\n",
      "=======Loss: 7.067052 ======\n",
      "=======Loss: 7.1870637 ======\n",
      "=======Loss: 7.1056585 ======\n",
      "=======Loss: 7.291922 ======\n",
      "=======Loss: 6.669305 ======\n",
      "=======Loss: 6.9363346 ======\n",
      "=======Loss: 6.8114934 ======\n",
      "=======Loss: 6.7284937 ======\n",
      "=======Loss: 6.7438803 ======\n",
      "=======Loss: 6.6244807 ======\n",
      "=======Loss: 5.970437 ======\n",
      "=======Loss: 6.6276684 ======\n",
      "=======Loss: 6.6320696 ======\n",
      "=======Loss: 6.3894978 ======\n",
      "=======Loss: 6.422467 ======\n",
      "=======Loss: 6.190117 ======\n",
      "=======Loss: 6.102192 ======\n",
      "=======Loss: 6.5348625 ======\n",
      "=======Loss: 6.184737 ======\n",
      "=======Loss: 6.2648964 ======\n",
      "=======Loss: 6.210012 ======\n",
      "=======Loss: 6.2216644 ======\n",
      "=======Loss: 6.142255 ======\n",
      "=======Loss: 5.843896 ======\n",
      "=======Loss: 6.2328186 ======\n",
      "=======Loss: 5.98933 ======\n",
      "=======Loss: 5.7038345 ======\n",
      "=======Loss: 5.8044252 ======\n",
      "=======Loss: 5.86757 ======\n",
      "=======Loss: 5.9257727 ======\n",
      "=======Loss: 6.0476885 ======\n",
      "=======Loss: 5.635678 ======\n",
      "=======Loss: 5.492237 ======\n",
      "=======Loss: 5.8163013 ======\n",
      "=======Loss: 5.49763 ======\n",
      "=======Loss: 5.816466 ======\n",
      "=======Loss: 5.639346 ======\n",
      "=======Loss: 5.7082634 ======\n",
      "=======Loss: 5.7050257 ======\n",
      "=======Loss: 5.54666 ======\n",
      "=======Loss: 5.83225 ======\n",
      "=======Loss: 5.4816155 ======\n",
      "=======Loss: 5.733206 ======\n",
      "=======Loss: 5.754128 ======\n",
      "=======Loss: 5.637345 ======\n",
      "=======Loss: 5.818769 ======\n",
      "=======Loss: 5.5970316 ======\n",
      "=======Loss: 5.493475 ======\n",
      "=======Loss: 5.293207 ======\n",
      "=======Loss: 5.5569925 ======\n",
      "=======Loss: 5.515567 ======\n",
      "=======Loss: 5.5187845 ======\n",
      "=======Loss: 5.6403313 ======\n",
      "=======Loss: 5.3787165 ======\n",
      "=======Loss: 5.5519547 ======\n",
      "=======Loss: 5.6507783 ======\n",
      "=======Loss: 5.5099554 ======\n",
      "=======Loss: 5.531963 ======\n",
      "=======Loss: 5.063956 ======\n",
      "=======Loss: 5.0662603 ======\n",
      "=======Loss: 5.294696 ======\n",
      "=======Loss: 5.2648726 ======\n",
      "=======Loss: 5.281775 ======\n",
      "=======Loss: 5.3335104 ======\n",
      "=======Loss: 5.3473334 ======\n",
      "=======Loss: 5.529675 ======\n",
      "=======Loss: 5.2880845 ======\n",
      "=======Loss: 5.1534433 ======\n",
      "=======Loss: 5.473871 ======\n",
      "=======Loss: 5.411493 ======\n",
      "=======Loss: 5.041252 ======\n",
      "=======Loss: 5.364565 ======\n",
      "=======Loss: 5.1959267 ======\n",
      "=======Loss: 5.2274504 ======\n",
      "=======Loss: 5.250087 ======\n",
      "=======Loss: 5.1289606 ======\n",
      "=======Loss: 4.9918957 ======\n",
      "=======Loss: 5.0658526 ======\n",
      "=======Loss: 5.036744 ======\n",
      "=======Loss: 4.751439 ======\n",
      "=======Loss: 4.9950285 ======\n",
      "=======Loss: 4.9858866 ======\n",
      "=======Loss: 5.129549 ======\n",
      "=======Loss: 5.1067553 ======\n",
      "=======Loss: 4.8564463 ======\n",
      "=======Loss: 5.155091 ======\n",
      "=======Loss: 5.155725 ======\n",
      "=======Loss: 5.114302 ======\n",
      "=======Loss: 5.1034856 ======\n",
      "=======Loss: 4.970542 ======\n",
      "=======Loss: 5.1642966 ======\n",
      "=======Loss: 5.1386147 ======\n",
      "=======Loss: 4.7928905 ======\n",
      "=======Loss: 4.853018 ======\n",
      "=======Loss: 4.9635096 ======\n",
      "=======Loss: 4.7422137 ======\n",
      "=======Loss: 5.1566753 ======\n",
      "=======Loss: 4.7928514 ======\n",
      "=======Loss: 4.844901 ======\n",
      "=======Loss: 4.8153863 ======\n",
      "=======Loss: 4.772379 ======\n",
      "=======Loss: 4.885861 ======\n",
      "=======Loss: 4.7770844 ======\n",
      "=======Loss: 4.7134814 ======\n",
      "=======Loss: 4.535372 ======\n",
      "=======Loss: 4.788382 ======\n",
      "=======Loss: 4.81617 ======\n",
      "=======Loss: 4.681534 ======\n",
      "=======Loss: 4.6235256 ======\n",
      "=======Loss: 4.5999947 ======\n",
      "=======Loss: 4.3947153 ======\n",
      "=======Loss: 4.4353695 ======\n",
      "=======Loss: 4.560362 ======\n",
      "=======Loss: 4.509297 ======\n",
      "=======Loss: 4.3909907 ======\n",
      "=======Loss: 4.558323 ======\n",
      "=======Loss: 4.5362105 ======\n",
      "=======Loss: 4.7411404 ======\n",
      "=======Loss: 4.5249977 ======\n",
      "=======Loss: 4.3056345 ======\n",
      "=======Loss: 4.196748 ======\n",
      "=======Loss: 4.4130306 ======\n",
      "=======Loss: 4.5152063 ======\n",
      "=======Loss: 4.457103 ======\n",
      "=======Loss: 4.2734814 ======\n",
      "=======Loss: 4.395715 ======\n",
      "=======Loss: 4.139781 ======\n",
      "=======Loss: 4.3158116 ======\n",
      "=======Loss: 4.0658855 ======\n",
      "=======Loss: 4.099651 ======\n",
      "=======Loss: 4.1531353 ======\n",
      "=======Loss: 4.2186685 ======\n",
      "=======Loss: 4.3706026 ======\n",
      "=======Loss: 3.9953897 ======\n",
      "=======Loss: 4.0672283 ======\n",
      "=======Loss: 4.0708337 ======\n",
      "=======Loss: 4.16646 ======\n",
      "=======Loss: 4.026943 ======\n",
      "=======Loss: 4.0923223 ======\n",
      "=======Loss: 4.134647 ======\n",
      "=======Loss: 3.9543557 ======\n",
      "=======Loss: 4.1015944 ======\n",
      "=======Loss: 4.158909 ======\n",
      "=======Loss: 4.1066036 ======\n",
      "=======Loss: 3.9843974 ======\n",
      "=======Loss: 3.9203112 ======\n",
      "=======Loss: 3.629633 ======\n",
      "=======Loss: 4.270729 ======\n",
      "=======Loss: 4.1105742 ======\n",
      "=======Loss: 3.8231711 ======\n",
      "=======Loss: 4.067378 ======\n",
      "=======Loss: 4.1202765 ======\n",
      "=======Loss: 3.9401536 ======\n",
      "=======Loss: 3.8245833 ======\n",
      "=======Loss: 3.77067 ======\n",
      "=======Loss: 3.9401228 ======\n",
      "=======Loss: 3.8988292 ======\n",
      "=======Loss: 3.8377805 ======\n",
      "=======Loss: 3.5664473 ======\n",
      "=======Loss: 4.0648766 ======\n",
      "=======Loss: 4.010871 ======\n",
      "=======Loss: 3.7300825 ======\n",
      "=======Loss: 3.9116864 ======\n",
      "=======Loss: 3.6613803 ======\n",
      "=======Loss: 3.532217 ======\n",
      "=======Loss: 3.6861513 ======\n",
      "=======Loss: 3.6313727 ======\n",
      "=======Loss: 3.9679213 ======\n",
      "=======Loss: 3.8224738 ======\n",
      "=======Loss: 3.735386 ======\n",
      "=======Loss: 3.6869326 ======\n",
      "=======Loss: 3.7639246 ======\n",
      "=======Loss: 3.591055 ======\n",
      "=======Loss: 3.7606962 ======\n",
      "=======Loss: 3.56961 ======\n",
      "=======Loss: 3.7446814 ======\n",
      "=======Loss: 3.8655548 ======\n",
      "=======Loss: 3.752078 ======\n",
      "=======Loss: 3.485521 ======\n",
      "=======Loss: 3.5285032 ======\n",
      "=======Loss: 3.5192378 ======\n",
      "=======Loss: 3.6321082 ======\n",
      "=======Loss: 3.6029017 ======\n",
      "=======Loss: 3.6378663 ======\n",
      "=======Loss: 3.482552 ======\n",
      "=======Loss: 3.5386252 ======\n",
      "=======Loss: 3.6178238 ======\n",
      "=======Loss: 3.6592343 ======\n",
      "=======Loss: 3.4065456 ======\n",
      "=======Loss: 3.5755875 ======\n",
      "=======Loss: 3.6314924 ======\n",
      "=======Loss: 3.287858 ======\n",
      "=======Loss: 3.6024806 ======\n",
      "=======Loss: 3.4900827 ======\n",
      "=======Loss: 3.3892052 ======\n",
      "=======Loss: 3.2799513 ======\n",
      "=======Loss: 3.4572988 ======\n",
      "=======Loss: 3.4430661 ======\n",
      "=======Loss: 3.5052528 ======\n",
      "=======Loss: 3.648956 ======\n",
      "=======Loss: 3.435298 ======\n",
      "=======Loss: 3.595313 ======\n",
      "=======Loss: 3.2942939 ======\n",
      "=======Loss: 3.6575131 ======\n",
      "=======Loss: 3.509794 ======\n",
      "=======Loss: 3.6040044 ======\n",
      "=======Loss: 3.3610084 ======\n",
      "=======Loss: 3.4096951 ======\n",
      "=======Loss: 3.3433847 ======\n",
      "=======Loss: 3.5372858 ======\n",
      "=======Loss: 3.5559998 ======\n",
      "=======Loss: 3.1958447 ======\n",
      "=======Loss: 3.5474668 ======\n",
      "=======Loss: 3.3892756 ======\n",
      "=======Loss: 3.3205717 ======\n",
      "=======Loss: 3.1706028 ======\n",
      "=======Loss: 3.4015625 ======\n",
      "=======Loss: 3.1729224 ======\n",
      "=======Loss: 3.2999854 ======\n",
      "=======Loss: 3.4766767 ======\n",
      "=======Loss: 3.238666 ======\n",
      "=======Loss: 3.1717813 ======\n",
      "=======Loss: 3.1868331 ======\n",
      "=======Loss: 3.1801608 ======\n",
      "=======Loss: 3.24417 ======\n",
      "=======Loss: 3.2847714 ======\n",
      "=======Loss: 3.1376746 ======\n",
      "=======Loss: 3.3735933 ======\n",
      "=======Loss: 3.1839623 ======\n",
      "=======Loss: 3.3112311 ======\n",
      "=======Loss: 3.2754903 ======\n",
      "=======Loss: 3.2090816 ======\n",
      "=======Loss: 3.3654206 ======\n",
      "=======Loss: 3.0843034 ======\n",
      "=======Loss: 3.0883856 ======\n",
      "=======Loss: 3.348744 ======\n",
      "=======Loss: 3.0731091 ======\n",
      "=======Loss: 3.311276 ======\n",
      "=======Loss: 3.1862679 ======\n",
      "=======Loss: 3.2182384 ======\n",
      "=======Loss: 3.1587868 ======\n",
      "=======Loss: 3.1807015 ======\n",
      "=======Loss: 3.1900728 ======\n",
      "=======Loss: 3.1295552 ======\n",
      "=======Loss: 3.1948428 ======\n",
      "=======Loss: 3.1501782 ======\n",
      "=======Loss: 3.0814033 ======\n",
      "=======Loss: 3.1454206 ======\n",
      "=======Loss: 3.1584008 ======\n",
      "=======Loss: 3.1350617 ======\n",
      "=======Loss: 3.101212 ======\n",
      "=======Loss: 3.2176242 ======\n",
      "=======Loss: 2.8455753 ======\n",
      "=======Loss: 2.8986816 ======\n",
      "=======Loss: 3.1450796 ======\n",
      "=======Loss: 3.1672626 ======\n",
      "=======Loss: 3.1296473 ======\n",
      "=======Loss: 3.1037235 ======\n",
      "=======Loss: 3.1028633 ======\n",
      "=======Loss: 2.993336 ======\n",
      "=======Loss: 2.9182956 ======\n",
      "=======Loss: 2.974833 ======\n",
      "=======Loss: 3.2221336 ======\n",
      "=======Loss: 3.0393376 ======\n",
      "=======Loss: 2.9747262 ======\n",
      "=======Loss: 3.2204998 ======\n",
      "=======Loss: 2.8508985 ======\n",
      "=======Loss: 2.966595 ======\n",
      "=======Loss: 2.8907585 ======\n",
      "=======Loss: 3.0745442 ======\n",
      "=======Loss: 2.9825242 ======\n",
      "=======Loss: 2.8363962 ======\n",
      "=======Loss: 2.8409328 ======\n",
      "=======Loss: 2.8638685 ======\n",
      "=======Loss: 2.8547678 ======\n",
      "=======Loss: 3.0693996 ======\n",
      "=======Loss: 2.7492087 ======\n",
      "=======Loss: 2.9266357 ======\n",
      "=======Loss: 2.943564 ======\n",
      "=======Loss: 2.8635592 ======\n",
      "=======Loss: 2.873045 ======\n",
      "=======Loss: 2.9549987 ======\n",
      "=======Loss: 2.9126697 ======\n",
      "=======Loss: 2.983792 ======\n",
      "=======Loss: 3.0063415 ======\n",
      "=======Loss: 2.9550838 ======\n",
      "=======Loss: 2.8573966 ======\n",
      "=======Loss: 2.9232578 ======\n",
      "=======Loss: 2.8992388 ======\n",
      "=======Loss: 2.813045 ======\n",
      "=======Loss: 2.9203136 ======\n",
      "=======Loss: 2.7918086 ======\n",
      "=======Loss: 2.8416662 ======\n",
      "=======Loss: 2.9021244 ======\n",
      "=======Loss: 2.829264 ======\n",
      "=======Loss: 2.8261738 ======\n",
      "=======Loss: 2.7487001 ======\n",
      "=======Loss: 3.1022882 ======\n",
      "=======Loss: 2.6660466 ======\n",
      "=======Loss: 2.8436778 ======\n",
      "=======Loss: 2.8950124 ======\n",
      "=======Loss: 2.6917255 ======\n",
      "=======Loss: 2.8005593 ======\n",
      "=======Loss: 2.7810183 ======\n",
      "=======Loss: 2.7603002 ======\n",
      "=======Loss: 2.758378 ======\n",
      "=======Loss: 2.905324 ======\n",
      "=======Loss: 2.5749273 ======\n",
      "=======Loss: 2.8416853 ======\n",
      "=======Loss: 2.7171655 ======\n",
      "=======Loss: 2.711045 ======\n",
      "=======Loss: 2.8802755 ======\n",
      "=======Loss: 2.6790602 ======\n",
      "=======Loss: 2.868867 ======\n",
      "=======Loss: 2.6784706 ======\n",
      "=======Loss: 2.8172772 ======\n",
      "=======Loss: 2.7559638 ======\n",
      "=======Loss: 2.7270136 ======\n",
      "=======Loss: 2.9640918 ======\n",
      "=======Loss: 2.6034558 ======\n",
      "=======Loss: 2.672412 ======\n",
      "=======Loss: 2.84128 ======\n",
      "=======Loss: 2.7838504 ======\n",
      "=======Loss: 2.7736397 ======\n",
      "=======Loss: 2.6843135 ======\n",
      "=======Loss: 2.6106486 ======\n",
      "=======Loss: 2.623949 ======\n",
      "=======Loss: 2.7141695 ======\n",
      "=======Loss: 2.657742 ======\n",
      "=======Loss: 2.8364422 ======\n",
      "=======Loss: 2.5585814 ======\n",
      "=======Loss: 2.65876 ======\n",
      "=======Loss: 2.5251596 ======\n",
      "=======Loss: 2.568386 ======\n",
      "=======Loss: 2.8023612 ======\n",
      "=======Loss: 2.6393518 ======\n",
      "=======Loss: 2.4292154 ======\n",
      "=======Loss: 2.7647402 ======\n",
      "=======Loss: 2.5438752 ======\n",
      "=======Loss: 2.603006 ======\n",
      "=======Loss: 2.6176038 ======\n",
      "=======Loss: 2.5556667 ======\n",
      "=======Loss: 2.5999713 ======\n",
      "=======Loss: 2.58575 ======\n",
      "=======Loss: 2.6520247 ======\n",
      "=======Loss: 2.4860506 ======\n",
      "=======Loss: 2.7034936 ======\n",
      "=======Loss: 2.5881371 ======\n",
      "=======Loss: 2.6461582 ======\n",
      "=======Loss: 2.6721835 ======\n",
      "=======Loss: 2.5042992 ======\n",
      "=======Loss: 2.711038 ======\n",
      "=======Loss: 2.5865836 ======\n",
      "=======Loss: 2.5215354 ======\n",
      "=======Loss: 2.5708363 ======\n",
      "=======Loss: 2.6063068 ======\n",
      "=======Loss: 2.4752443 ======\n",
      "=======Loss: 2.7267575 ======\n",
      "=======Loss: 2.524901 ======\n",
      "=======Loss: 2.577856 ======\n",
      "=======Loss: 2.610041 ======\n",
      "=======Loss: 2.4409149 ======\n",
      "=======Loss: 2.5460482 ======\n",
      "=======Loss: 2.4699488 ======\n",
      "=======Loss: 2.5530186 ======\n",
      "=======Loss: 2.517966 ======\n",
      "=======Loss: 2.4730847 ======\n",
      "=======Loss: 2.5626273 ======\n",
      "=======Loss: 2.6222978 ======\n",
      "=======Loss: 2.4907007 ======\n",
      "=======Loss: 2.475804 ======\n",
      "=======Loss: 2.436048 ======\n",
      "=======Loss: 2.6049433 ======\n",
      "=======Loss: 2.6019738 ======\n",
      "=======Loss: 2.4900165 ======\n",
      "=======Loss: 2.487102 ======\n",
      "=======Loss: 2.4242635 ======\n",
      "=======Loss: 2.5368416 ======\n",
      "=======Loss: 2.460887 ======\n",
      "=======Loss: 2.373706 ======\n",
      "=======Loss: 2.314692 ======\n",
      "=======Loss: 2.476726 ======\n",
      "=======Loss: 2.3901782 ======\n",
      "=======Loss: 2.2484827 ======\n",
      "=======Loss: 2.5194077 ======\n",
      "=======Loss: 2.521852 ======\n",
      "=======Loss: 2.5383093 ======\n",
      "=======Loss: 2.4293594 ======\n",
      "=======Loss: 2.4666073 ======\n",
      "=======Loss: 2.2983923 ======\n",
      "=======Loss: 2.3164725 ======\n",
      "=======Loss: 2.598534 ======\n",
      "=======Loss: 2.581685 ======\n",
      "=======Loss: 2.3476152 ======\n",
      "=======Loss: 2.5724943 ======\n",
      "=======Loss: 2.509346 ======\n",
      "=======Loss: 2.466775 ======\n",
      "=======Loss: 2.2978013 ======\n",
      "=======Loss: 2.442042 ======\n",
      "=======Loss: 2.386859 ======\n",
      "=======Loss: 2.3931751 ======\n",
      "=======Loss: 2.4063654 ======\n",
      "=======Loss: 2.3937678 ======\n",
      "=======Loss: 2.4354942 ======\n",
      "=======Loss: 2.551335 ======\n",
      "=======Loss: 2.2223907 ======\n",
      "=======Loss: 2.4343228 ======\n",
      "=======Loss: 2.3364027 ======\n",
      "=======Loss: 2.2408223 ======\n",
      "=======Loss: 2.3603792 ======\n",
      "=======Loss: 2.3549204 ======\n",
      "=======Loss: 2.32485 ======\n",
      "=======Loss: 2.2533538 ======\n",
      "=======Loss: 2.516067 ======\n",
      "=======Loss: 2.1695206 ======\n",
      "=======Loss: 2.4080055 ======\n",
      "=======Loss: 2.205289 ======\n",
      "=======Loss: 2.2280433 ======\n",
      "=======Loss: 2.386216 ======\n",
      "=======Loss: 2.3481169 ======\n",
      "=======Loss: 2.2719774 ======\n",
      "=======Loss: 2.3031032 ======\n",
      "=======Loss: 2.380405 ======\n",
      "=======Loss: 2.2129848 ======\n",
      "=======Loss: 2.2125275 ======\n",
      "=======Loss: 2.1840792 ======\n",
      "=======Loss: 2.281372 ======\n",
      "=======Loss: 2.248518 ======\n",
      "=======Loss: 2.2235692 ======\n",
      "=======Loss: 2.2666945 ======\n",
      "=======Loss: 2.1760123 ======\n",
      "=======Loss: 2.2784348 ======\n",
      "=======Loss: 2.2213821 ======\n",
      "=======Loss: 2.3502095 ======\n",
      "=======Loss: 2.415935 ======\n",
      "=======Loss: 2.1767876 ======\n",
      "=======Loss: 2.2922559 ======\n",
      "=======Loss: 2.0902534 ======\n",
      "=======Loss: 2.2972093 ======\n",
      "=======Loss: 2.2251487 ======\n",
      "=======Loss: 2.2025542 ======\n",
      "=======Loss: 2.236628 ======\n",
      "=======Loss: 2.2285595 ======\n",
      "=======Loss: 2.0255308 ======\n",
      "=======Loss: 2.1561356 ======\n",
      "=======Loss: 2.2883718 ======\n",
      "=======Loss: 2.2247694 ======\n",
      "=======Loss: 2.1579204 ======\n",
      "=======Loss: 2.2019007 ======\n",
      "=======Loss: 2.187332 ======\n",
      "=======Loss: 2.1089015 ======\n",
      "=======Loss: 2.1854758 ======\n",
      "=======Loss: 2.1288912 ======\n",
      "=======Loss: 2.3502932 ======\n",
      "=======Loss: 2.2439833 ======\n",
      "=======Loss: 2.0204878 ======\n",
      "=======Loss: 2.2981398 ======\n",
      "=======Loss: 2.2164512 ======\n",
      "=======Loss: 2.076584 ======\n",
      "=======Loss: 2.2027664 ======\n",
      "=======Loss: 2.1358309 ======\n",
      "=======Loss: 2.1639698 ======\n",
      "=======Loss: 2.2949133 ======\n",
      "=======Loss: 2.0738075 ======\n",
      "=======Loss: 2.2026927 ======\n",
      "=======Loss: 2.1485019 ======\n",
      "=======Loss: 1.8951926 ======\n",
      "=======Loss: 2.2517965 ======\n",
      "=======Loss: 2.1643264 ======\n",
      "=======Loss: 2.2528672 ======\n",
      "=======Loss: 2.1383033 ======\n",
      "=======Loss: 2.13593 ======\n",
      "=======Loss: 2.1298742 ======\n",
      "=======Loss: 1.9766378 ======\n",
      "=======Loss: 2.1856375 ======\n",
      "=======Loss: 2.1612787 ======\n",
      "=======Loss: 2.1571004 ======\n",
      "=======Loss: 2.2213366 ======\n",
      "=======Loss: 2.0768406 ======\n",
      "=======Loss: 2.070644 ======\n",
      "=======Loss: 2.0011559 ======\n",
      "=======Loss: 1.9408923 ======\n",
      "=======Loss: 2.0500007 ======\n",
      "=======Loss: 2.2058263 ======\n",
      "=======Loss: 2.1498425 ======\n",
      "=======Loss: 1.9882963 ======\n",
      "=======Loss: 2.059914 ======\n",
      "=======Loss: 2.2104867 ======\n",
      "=======Loss: 1.893149 ======\n",
      "=======Loss: 2.0177138 ======\n",
      "=======Loss: 2.018611 ======\n",
      "=======Loss: 1.9770123 ======\n",
      "=======Loss: 2.194746 ======\n",
      "=======Loss: 2.0340421 ======\n",
      "=======Loss: 2.0521863 ======\n",
      "=======Loss: 2.0359437 ======\n",
      "=======Loss: 2.000085 ======\n",
      "=======Loss: 2.0585036 ======\n",
      "=======Loss: 1.9609084 ======\n",
      "=======Loss: 2.083856 ======\n",
      "=======Loss: 1.9533772 ======\n",
      "=======Loss: 2.0187225 ======\n",
      "=======Loss: 2.1185098 ======\n",
      "=======Loss: 2.0137491 ======\n",
      "=======Loss: 1.9704984 ======\n",
      "=======Loss: 1.9886193 ======\n",
      "=======Loss: 1.8436648 ======\n",
      "=======Loss: 2.1596794 ======\n",
      "=======Loss: 1.9916143 ======\n",
      "=======Loss: 1.9388974 ======\n",
      "=======Loss: 2.0760174 ======\n",
      "=======Loss: 1.997163 ======\n",
      "=======Loss: 1.7740085 ======\n",
      "=======Loss: 1.8567076 ======\n",
      "=======Loss: 1.9001372 ======\n",
      "=======Loss: 1.8341277 ======\n",
      "=======Loss: 1.8848426 ======\n",
      "=======Loss: 1.9064486 ======\n",
      "=======Loss: 2.0684667 ======\n",
      "=======Loss: 2.0813787 ======\n",
      "=======Loss: 1.877207 ======\n",
      "=======Loss: 1.8976946 ======\n",
      "=======Loss: 1.8620126 ======\n",
      "=======Loss: 1.9049115 ======\n",
      "=======Loss: 2.2000244 ======\n",
      "=======Loss: 1.9519743 ======\n",
      "=======Loss: 2.125009 ======\n",
      "=======Loss: 1.9656316 ======\n",
      "=======Loss: 1.9783611 ======\n",
      "=======Loss: 2.052465 ======\n",
      "=======Loss: 1.9430671 ======\n",
      "=======Loss: 1.7556684 ======\n",
      "=======Loss: 1.8071452 ======\n",
      "=======Loss: 2.111008 ======\n",
      "=======Loss: 1.9873061 ======\n",
      "=======Loss: 1.8557233 ======\n",
      "=======Loss: 1.9318256 ======\n",
      "=======Loss: 1.788744 ======\n",
      "=======Loss: 2.0121486 ======\n",
      "=======Loss: 1.943444 ======\n",
      "=======Loss: 1.9447386 ======\n",
      "=======Loss: 1.8865933 ======\n",
      "=======Loss: 2.0364714 ======\n",
      "=======Loss: 1.8959475 ======\n",
      "=======Loss: 1.9363153 ======\n",
      "=======Loss: 1.7633291 ======\n",
      "=======Loss: 1.946197 ======\n",
      "=======Loss: 1.7658472 ======\n",
      "=======Loss: 1.855706 ======\n",
      "=======Loss: 2.0086057 ======\n",
      "=======Loss: 1.8423262 ======\n",
      "=======Loss: 1.9010477 ======\n",
      "=======Loss: 1.8668724 ======\n",
      "=======Loss: 1.8271648 ======\n",
      "=======Loss: 1.9213053 ======\n",
      "=======Loss: 1.9327793 ======\n",
      "=======Loss: 1.8463676 ======\n",
      "=======Loss: 1.7192903 ======\n",
      "=======Loss: 1.8525765 ======\n",
      "=======Loss: 1.7861099 ======\n",
      "=======Loss: 1.8992741 ======\n",
      "=======Loss: 1.8132598 ======\n",
      "=======Loss: 1.8387458 ======\n",
      "=======Loss: 1.7962226 ======\n",
      "=======Loss: 1.9014204 ======\n",
      "=======Loss: 1.7548445 ======\n",
      "=======Loss: 1.9430974 ======\n",
      "=======Loss: 1.8607088 ======\n",
      "=======Loss: 1.7832367 ======\n",
      "=======Loss: 1.8141173 ======\n",
      "=======Loss: 1.7604192 ======\n",
      "=======Loss: 1.7567961 ======\n",
      "=======Loss: 1.7845339 ======\n",
      "=======Loss: 1.8168776 ======\n",
      "=======Loss: 1.7229136 ======\n",
      "=======Loss: 1.797102 ======\n",
      "=======Loss: 1.8064113 ======\n",
      "=======Loss: 1.6868887 ======\n",
      "=======Loss: 1.6867326 ======\n",
      "=======Loss: 1.8343403 ======\n",
      "=======Loss: 1.7282469 ======\n",
      "=======Loss: 1.8040813 ======\n",
      "=======Loss: 1.7438834 ======\n",
      "=======Loss: 1.840837 ======\n",
      "=======Loss: 1.8982978 ======\n",
      "=======Loss: 1.8085374 ======\n",
      "=======Loss: 1.8286891 ======\n",
      "=======Loss: 1.72978 ======\n",
      "=======Loss: 1.794089 ======\n",
      "=======Loss: 1.8715789 ======\n",
      "=======Loss: 1.8351426 ======\n",
      "=======Loss: 1.7988688 ======\n",
      "=======Loss: 1.7457378 ======\n",
      "=======Loss: 1.9717224 ======\n",
      "=======Loss: 1.7266301 ======\n",
      "=======Loss: 1.598628 ======\n",
      "=======Loss: 1.6789995 ======\n",
      "=======Loss: 1.7840798 ======\n",
      "=======Loss: 1.7066112 ======\n",
      "=======Loss: 1.793237 ======\n",
      "=======Loss: 1.7418814 ======\n",
      "=======Loss: 1.7392024 ======\n",
      "=======Loss: 1.8033271 ======\n",
      "=======Loss: 1.7863293 ======\n",
      "=======Loss: 1.7178521 ======\n",
      "=======Loss: 1.7356334 ======\n",
      "=======Loss: 1.6664205 ======\n",
      "=======Loss: 1.7495177 ======\n",
      "=======Loss: 1.7990029 ======\n",
      "=======Loss: 1.8234379 ======\n",
      "=======Loss: 1.8387582 ======\n",
      "=======Loss: 1.7168359 ======\n",
      "=======Loss: 1.877067 ======\n",
      "=======Loss: 1.8536553 ======\n",
      "=======Loss: 1.7500434 ======\n",
      "=======Loss: 1.7147577 ======\n",
      "=======Loss: 1.7541823 ======\n",
      "=======Loss: 1.7370875 ======\n",
      "=======Loss: 1.8381451 ======\n",
      "=======Loss: 1.7535663 ======\n",
      "=======Loss: 1.7433302 ======\n",
      "=======Loss: 1.7839719 ======\n",
      "=======Loss: 1.7916393 ======\n",
      "=======Loss: 1.7732047 ======\n",
      "=======Loss: 1.7377896 ======\n",
      "=======Loss: 1.7299868 ======\n",
      "=======Loss: 1.7229905 ======\n",
      "=======Loss: 1.6750405 ======\n",
      "=======Loss: 1.7894967 ======\n",
      "=======Loss: 1.7589791 ======\n",
      "=======Loss: 1.6402858 ======\n",
      "=======Loss: 1.5876411 ======\n",
      "=======Loss: 1.6491095 ======\n",
      "=======Loss: 1.6443092 ======\n",
      "=======Loss: 1.6771328 ======\n",
      "=======Loss: 1.645678 ======\n",
      "=======Loss: 1.6559744 ======\n",
      "=======Loss: 1.7839291 ======\n",
      "=======Loss: 1.6905928 ======\n",
      "=======Loss: 1.7984805 ======\n",
      "=======Loss: 1.6451893 ======\n",
      "=======Loss: 1.6762705 ======\n",
      "=======Loss: 1.6343379 ======\n",
      "=======Loss: 1.6408924 ======\n",
      "=======Loss: 1.7091067 ======\n",
      "=======Loss: 1.7257385 ======\n",
      "=======Loss: 1.6066394 ======\n",
      "=======Loss: 1.6044247 ======\n",
      "=======Loss: 1.610781 ======\n",
      "=======Loss: 1.6179858 ======\n",
      "=======Loss: 1.6112056 ======\n",
      "=======Loss: 1.7157025 ======\n",
      "=======Loss: 1.6405859 ======\n",
      "=======Loss: 1.6115495 ======\n",
      "=======Loss: 1.623359 ======\n",
      "=======Loss: 1.5648711 ======\n",
      "=======Loss: 1.6504066 ======\n",
      "=======Loss: 1.5512996 ======\n",
      "=======Loss: 1.6349756 ======\n",
      "=======Loss: 1.6845825 ======\n",
      "=======Loss: 1.556538 ======\n",
      "=======Loss: 1.5785573 ======\n",
      "=======Loss: 1.6231074 ======\n",
      "=======Loss: 1.6187774 ======\n",
      "=======Loss: 1.5987656 ======\n",
      "=======Loss: 1.5883138 ======\n",
      "=======Loss: 1.601578 ======\n",
      "=======Loss: 1.615277 ======\n",
      "=======Loss: 1.5910863 ======\n",
      "=======Loss: 1.5479435 ======\n",
      "=======Loss: 1.6468294 ======\n",
      "=======Loss: 1.7025322 ======\n",
      "=======Loss: 1.5987525 ======\n",
      "=======Loss: 1.6266706 ======\n",
      "=======Loss: 1.5093358 ======\n",
      "=======Loss: 1.5865792 ======\n",
      "=======Loss: 1.5302546 ======\n",
      "=======Loss: 1.6014426 ======\n",
      "=======Loss: 1.7058733 ======\n",
      "=======Loss: 1.6409764 ======\n",
      "=======Loss: 1.5733824 ======\n",
      "=======Loss: 1.6004496 ======\n",
      "=======Loss: 1.5827266 ======\n",
      "=======Loss: 1.5667467 ======\n",
      "=======Loss: 1.4811788 ======\n",
      "=======Loss: 1.61247 ======\n",
      "=======Loss: 1.6098351 ======\n",
      "=======Loss: 1.5544597 ======\n",
      "=======Loss: 1.4894819 ======\n",
      "=======Loss: 1.5571954 ======\n",
      "=======Loss: 1.5691669 ======\n",
      "=======Loss: 1.5545077 ======\n",
      "=======Loss: 1.6249616 ======\n",
      "=======Loss: 1.4428014 ======\n",
      "=======Loss: 1.6675229 ======\n",
      "=======Loss: 1.6151412 ======\n",
      "=======Loss: 1.5026691 ======\n",
      "=======Loss: 1.5717038 ======\n",
      "=======Loss: 1.4924309 ======\n",
      "=======Loss: 1.6123164 ======\n",
      "=======Loss: 1.4978511 ======\n",
      "=======Loss: 1.5137959 ======\n",
      "=======Loss: 1.5346755 ======\n",
      "=======Loss: 1.5153947 ======\n",
      "=======Loss: 1.625258 ======\n",
      "=======Loss: 1.5742066 ======\n",
      "=======Loss: 1.6055722 ======\n",
      "=======Loss: 1.3972361 ======\n",
      "=======Loss: 1.5439221 ======\n",
      "=======Loss: 1.5860775 ======\n",
      "=======Loss: 1.4877137 ======\n",
      "=======Loss: 1.526187 ======\n",
      "=======Loss: 1.4759271 ======\n",
      "=======Loss: 1.4255159 ======\n",
      "=======Loss: 1.5393357 ======\n",
      "=======Loss: 1.4938588 ======\n",
      "=======Loss: 1.5299873 ======\n",
      "=======Loss: 1.6637182 ======\n",
      "=======Loss: 1.4230301 ======\n",
      "=======Loss: 1.5851562 ======\n",
      "=======Loss: 1.5627156 ======\n",
      "=======Loss: 1.4428818 ======\n",
      "=======Loss: 1.5354683 ======\n",
      "=======Loss: 1.635714 ======\n",
      "=======Loss: 1.5534568 ======\n",
      "=======Loss: 1.5259745 ======\n",
      "=======Loss: 1.6571928 ======\n",
      "=======Loss: 1.4835919 ======\n",
      "=======Loss: 1.349344 ======\n",
      "=======Loss: 1.6450696 ======\n",
      "=======Loss: 1.516185 ======\n",
      "=======Loss: 1.4917307 ======\n",
      "=======Loss: 1.4334365 ======\n",
      "=======Loss: 1.5486326 ======\n",
      "=======Loss: 1.536391 ======\n",
      "=======Loss: 1.5270922 ======\n",
      "=======Loss: 1.4028467 ======\n",
      "=======Loss: 1.6039675 ======\n",
      "=======Loss: 1.4038875 ======\n",
      "=======Loss: 1.526511 ======\n",
      "=======Loss: 1.4701197 ======\n",
      "=======Loss: 1.4154975 ======\n",
      "=======Loss: 1.6432378 ======\n",
      "=======Loss: 1.4601458 ======\n",
      "=======Loss: 1.4633591 ======\n",
      "=======Loss: 1.4881811 ======\n",
      "=======Loss: 1.480097 ======\n",
      "=======Loss: 1.5080868 ======\n",
      "=======Loss: 1.4512913 ======\n",
      "=======Loss: 1.4429276 ======\n",
      "=======Loss: 1.47837 ======\n",
      "=======Loss: 1.5557632 ======\n",
      "=======Loss: 1.5500424 ======\n",
      "=======Loss: 1.4163785 ======\n",
      "=======Loss: 1.5336297 ======\n",
      "=======Loss: 1.4357982 ======\n",
      "=======Loss: 1.2982278 ======\n",
      "=======Loss: 1.4586589 ======\n",
      "=======Loss: 1.4198993 ======\n",
      "=======Loss: 1.4618764 ======\n",
      "=======Loss: 1.4864337 ======\n",
      "=======Loss: 1.4965725 ======\n",
      "=======Loss: 1.4451035 ======\n",
      "=======Loss: 1.4589741 ======\n",
      "=======Loss: 1.5412982 ======\n",
      "=======Loss: 1.3859797 ======\n",
      "=======Loss: 1.3933506 ======\n",
      "=======Loss: 1.5532465 ======\n",
      "=======Loss: 1.5221729 ======\n",
      "=======Loss: 1.502112 ======\n",
      "=======Loss: 1.572297 ======\n",
      "=======Loss: 1.5065647 ======\n",
      "=======Loss: 1.4838117 ======\n",
      "=======Loss: 1.49295 ======\n",
      "=======Loss: 1.507876 ======\n",
      "=======Loss: 1.3928671 ======\n",
      "=======Loss: 1.4928257 ======\n",
      "=======Loss: 1.5263126 ======\n",
      "=======Loss: 1.4312205 ======\n",
      "=======Loss: 1.4451156 ======\n",
      "=======Loss: 1.4419962 ======\n",
      "=======Loss: 1.362021 ======\n",
      "=======Loss: 1.4266396 ======\n",
      "=======Loss: 1.4479203 ======\n",
      "=======Loss: 1.4861618 ======\n",
      "=======Loss: 1.3634951 ======\n",
      "=======Loss: 1.3942573 ======\n",
      "=======Loss: 1.4207428 ======\n",
      "=======Loss: 1.4353311 ======\n",
      "=======Loss: 1.3991009 ======\n",
      "=======Loss: 1.4816005 ======\n",
      "=======Loss: 1.4645056 ======\n",
      "=======Loss: 1.4505681 ======\n",
      "=======Loss: 1.4779692 ======\n",
      "=======Loss: 1.4468474 ======\n",
      "=======Loss: 1.4289486 ======\n",
      "=======Loss: 1.3702695 ======\n",
      "=======Loss: 1.4060156 ======\n",
      "=======Loss: 1.4491165 ======\n",
      "=======Loss: 1.4386272 ======\n",
      "=======Loss: 1.4016995 ======\n",
      "=======Loss: 1.3805916 ======\n",
      "=======Loss: 1.4053235 ======\n",
      "=======Loss: 1.3402293 ======\n",
      "=======Loss: 1.427494 ======\n",
      "=======Loss: 1.3871486 ======\n",
      "=======Loss: 1.401173 ======\n",
      "=======Loss: 1.4745903 ======\n",
      "=======Loss: 1.3977281 ======\n",
      "=======Loss: 1.4056511 ======\n",
      "=======Loss: 1.4180899 ======\n",
      "=======Loss: 1.4408941 ======\n",
      "=======Loss: 1.359042 ======\n",
      "=======Loss: 1.3496097 ======\n",
      "=======Loss: 1.4367039 ======\n",
      "=======Loss: 1.2999988 ======\n",
      "=======Loss: 1.2861683 ======\n",
      "=======Loss: 1.50183 ======\n",
      "=======Loss: 1.4548497 ======\n",
      "=======Loss: 1.3328068 ======\n",
      "=======Loss: 1.3103623 ======\n",
      "=======Loss: 1.2711512 ======\n",
      "=======Loss: 1.4121416 ======\n",
      "=======Loss: 1.3872855 ======\n",
      "=======Loss: 1.3896168 ======\n",
      "=======Loss: 1.4068114 ======\n",
      "=======Loss: 1.3761024 ======\n",
      "=======Loss: 1.3933318 ======\n",
      "=======Loss: 1.3147128 ======\n",
      "=======Loss: 1.3639518 ======\n",
      "=======Loss: 1.3180575 ======\n",
      "=======Loss: 1.310762 ======\n",
      "=======Loss: 1.3176228 ======\n",
      "=======Loss: 1.3408124 ======\n",
      "=======Loss: 1.4231513 ======\n",
      "=======Loss: 1.2831466 ======\n",
      "=======Loss: 1.4127877 ======\n",
      "=======Loss: 1.3957891 ======\n",
      "=======Loss: 1.3349754 ======\n",
      "=======Loss: 1.503476 ======\n",
      "=======Loss: 1.4297323 ======\n",
      "=======Loss: 1.4066397 ======\n",
      "=======Loss: 1.2543198 ======\n",
      "=======Loss: 1.3805771 ======\n",
      "=======Loss: 1.3092794 ======\n",
      "=======Loss: 1.425636 ======\n",
      "=======Loss: 1.5000393 ======\n",
      "=======Loss: 1.4218318 ======\n",
      "=======Loss: 1.3879464 ======\n",
      "=======Loss: 1.3479846 ======\n",
      "=======Loss: 1.340091 ======\n",
      "=======Loss: 1.2641028 ======\n",
      "=======Loss: 1.3482144 ======\n",
      "=======Loss: 1.3491654 ======\n",
      "=======Loss: 1.3289704 ======\n",
      "=======Loss: 1.302638 ======\n",
      "=======Loss: 1.4074426 ======\n",
      "=======Loss: 1.423953 ======\n",
      "=======Loss: 1.3554647 ======\n",
      "=======Loss: 1.3723401 ======\n",
      "=======Loss: 1.412364 ======\n",
      "=======Loss: 1.3791914 ======\n",
      "=======Loss: 1.2820461 ======\n",
      "=======Loss: 1.2643126 ======\n",
      "=======Loss: 1.2772195 ======\n",
      "=======Loss: 1.2894577 ======\n",
      "=======Loss: 1.4373907 ======\n",
      "=======Loss: 1.3893478 ======\n",
      "=======Loss: 1.3371238 ======\n",
      "=======Loss: 1.4157088 ======\n",
      "=======Loss: 1.2852418 ======\n",
      "=======Loss: 1.3345166 ======\n",
      "=======Loss: 1.286636 ======\n",
      "=======Loss: 1.4136312 ======\n",
      "=======Loss: 1.2809879 ======\n",
      "=======Loss: 1.233533 ======\n",
      "=======Loss: 1.3086187 ======\n",
      "=======Loss: 1.359009 ======\n",
      "=======Loss: 1.2853479 ======\n",
      "=======Loss: 1.3570648 ======\n",
      "=======Loss: 1.2768308 ======\n",
      "=======Loss: 1.3086596 ======\n",
      "=======Loss: 1.2507997 ======\n",
      "=======Loss: 1.3459587 ======\n",
      "=======Loss: 1.3453324 ======\n",
      "=======Loss: 1.4020307 ======\n",
      "=======Loss: 1.3746895 ======\n",
      "=======Loss: 1.2774713 ======\n",
      "=======Loss: 1.3212113 ======\n",
      "=======Loss: 1.282445 ======\n",
      "=======Loss: 1.4000766 ======\n",
      "=======Loss: 1.2786176 ======\n",
      "=======Loss: 1.2348707 ======\n",
      "=======Loss: 1.4638867 ======\n",
      "=======Loss: 1.2917686 ======\n",
      "=======Loss: 1.2928818 ======\n",
      "=======Loss: 1.3604928 ======\n",
      "=======Loss: 1.3937304 ======\n",
      "=======Loss: 1.2429082 ======\n",
      "=======Loss: 1.297101 ======\n",
      "=======Loss: 1.2644558 ======\n",
      "=======Loss: 1.3156508 ======\n",
      "=======Loss: 1.2894604 ======\n",
      "=======Loss: 1.3315213 ======\n",
      "=======Loss: 1.432501 ======\n",
      "=======Loss: 1.3204486 ======\n",
      "=======Loss: 1.2818649 ======\n",
      "=======Loss: 1.3295269 ======\n",
      "=======Loss: 1.3207226 ======\n",
      "=======Loss: 1.2066042 ======\n",
      "=======Loss: 1.2808117 ======\n",
      "=======Loss: 1.3478923 ======\n",
      "=======Loss: 1.2749878 ======\n",
      "=======Loss: 1.3116554 ======\n",
      "=======Loss: 1.2184491 ======\n",
      "=======Loss: 1.2367604 ======\n",
      "=======Loss: 1.2364144 ======\n",
      "=======Loss: 1.224767 ======\n",
      "=======Loss: 1.308835 ======\n",
      "=======Loss: 1.2333395 ======\n",
      "=======Loss: 1.2680359 ======\n",
      "=======Loss: 1.3222253 ======\n",
      "=======Loss: 1.2954619 ======\n",
      "=======Loss: 1.3364389 ======\n",
      "=======Loss: 1.277117 ======\n",
      "=======Loss: 1.2851816 ======\n",
      "=======Loss: 1.346033 ======\n",
      "=======Loss: 1.3134089 ======\n",
      "=======Loss: 1.2503076 ======\n",
      "=======Loss: 1.3314949 ======\n",
      "=======Loss: 1.2798768 ======\n",
      "=======Loss: 1.266739 ======\n",
      "=======Loss: 1.3006241 ======\n",
      "=======Loss: 1.2971051 ======\n",
      "=======Loss: 1.2326638 ======\n",
      "=======Loss: 1.2954876 ======\n",
      "=======Loss: 1.3278674 ======\n",
      "=======Loss: 1.3606496 ======\n",
      "=======Loss: 1.2261969 ======\n",
      "=======Loss: 1.3053155 ======\n",
      "=======Loss: 1.3019384 ======\n",
      "=======Loss: 1.29735 ======\n",
      "=======Loss: 1.2521045 ======\n",
      "=======Loss: 1.2843723 ======\n",
      "=======Loss: 1.3104984 ======\n",
      "=======Loss: 1.2996935 ======\n",
      "=======Loss: 1.271425 ======\n",
      "=======Loss: 1.2289355 ======\n",
      "=======Loss: 1.2254322 ======\n",
      "=======Loss: 1.3264678 ======\n",
      "=======Loss: 1.3174219 ======\n",
      "=======Loss: 1.2181021 ======\n",
      "=======Loss: 1.2887143 ======\n",
      "=======Loss: 1.2373414 ======\n",
      "=======Loss: 1.1821125 ======\n",
      "=======Loss: 1.264627 ======\n",
      "=======Loss: 1.2681429 ======\n",
      "=======Loss: 1.3027809 ======\n",
      "=======Loss: 1.2198604 ======\n",
      "=======Loss: 1.2853203 ======\n",
      "=======Loss: 1.2299091 ======\n",
      "=======Loss: 1.2997224 ======\n",
      "=======Loss: 1.2696387 ======\n",
      "=======Loss: 1.23836 ======\n",
      "=======Loss: 1.3754983 ======\n",
      "=======Loss: 1.2201436 ======\n",
      "=======Loss: 1.2933515 ======\n",
      "=======Loss: 1.270514 ======\n",
      "=======Loss: 1.2408757 ======\n"
     ]
    }
   ],
   "source": [
    "n_params = count_model_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# %%\n",
    "#Create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=8e-4, \n",
    "                                weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2, reduce_dims=(0,1))\n",
    "# h1loss = H1Loss(d=2, reduce_dims=(0,1))\n",
    "\n",
    "train_loss = l2loss\n",
    "eval_losses={'l2': l2loss} #'h1': h1loss, \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()\n",
    "\n",
    "step = 0\n",
    "\n",
    "with open('script/sfno_loss.txt', 'w') as f:\n",
    "    for epoch in range(20):\n",
    "        avg_loss = 0\n",
    "        train_err = 0.0\n",
    "        \n",
    "        # track number of training examples in batch\n",
    "        n_samples = 0\n",
    "        for idx, sample in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            sample = {\n",
    "                k: v.to(device)\n",
    "                for k, v in sample.items()\n",
    "                if torch.is_tensor(v)\n",
    "            }\n",
    "\n",
    "            n_samples += sample[\"y\"].shape[0]\n",
    "            out = model(sample[\"x\"])\n",
    "\n",
    "            loss = l2loss(out, **sample)\n",
    "\n",
    "            loss.backward()\n",
    "            del out\n",
    "\n",
    "            optimizer.step()\n",
    "            train_err += loss.item()\n",
    "            with torch.no_grad():\n",
    "                print(\"=======Loss:\",loss.detach().cpu().numpy(),\"======\")\n",
    "                f.write(f'Step {step + 1}, Loss: {loss.item()}\\n')\n",
    "                step += 1\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_err)\n",
    "        else:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataloader at resolution (32, 64) with 200 samples and batch-size=4\n",
      "Loading test dataloader at resolution (32, 64) with 50 samples and batch-size=1\n",
      "Test Loss: 0.5053  0.0209\n"
     ]
    }
   ],
   "source": [
    "resolution = (32, 64)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=resolution,\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f}  {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (64, 128)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=resolution,\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f}  {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (128, 256)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=resolution,\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "mean_loss = torch.mean(torch.tensor(test_losses))\n",
    "std_loss = torch.std(torch.tensor(test_losses))\n",
    "\n",
    "print(f'Test Loss: {mean_loss:.4f}  {std_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "outputs = model(inputs)\n",
    "print(\"test time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "    # Input x\n",
    "    x = torch.tensor(np.load(\"../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "    # Ground-truth\n",
    "    y = np.load(\"../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "    # Model prediction\n",
    "    x_in = x.unsqueeze(0).to(device)\n",
    "    out = model(x_in).squeeze()[0, ...].detach().cpu().numpy()\n",
    "    x = x[0, ...].detach().numpy()\n",
    "\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"./script/output_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(4, 2))\n",
    "# for index, resolution in enumerate([(32, 64), (64, 128), (128, 256)]):\n",
    "#     # Input x\n",
    "#     x = torch.tensor(np.load(\"../test_dataset/input_\"+str(resolution[0])+\"_resolution.npy\"))\n",
    "#     # Ground-truth\n",
    "#     y = np.load(\"../test_dataset/label_\"+str(resolution[0])+\"_resolution.npy\")\n",
    "#     x = x[0, ...].detach().numpy()\n",
    "    \n",
    "#     plt.imshow(x)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/input_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    \n",
    "#     plt.imshow(y)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\"./script/label_\" + str(resolution[0]) + \"_resolution.png\", bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (128, 256)\n",
    "_, test_loaders = load_spherical_swe(n_train=200, batch_size=4, train_resolution=resolution,\n",
    "                                                test_resolutions=[resolution], n_tests=[50], test_batch_sizes=[1])\n",
    "\n",
    "loss_low = 1e5\n",
    "loss_high = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loaders[resolution]):\n",
    "        inputs = sample['x'].to(device)\n",
    "        targets = sample['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = l2loss(outputs, targets)\n",
    "        if loss > loss_high:\n",
    "            loss_high = loss\n",
    "            target_high = targets.squeeze()[0, ...].detach().cpu().numpy()\n",
    "            output_high = outputs.squeeze()[0, ...].detach().cpu().numpy()\n",
    "        if loss < loss_low:\n",
    "            loss_low = loss\n",
    "            target_low = targets.squeeze()[0, ...].detach().cpu().numpy()\n",
    "            output_low = outputs.squeeze()[0, ...].detach().cpu().numpy()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2))\n",
    "plt.imshow(target_high)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/target_high.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(output_high)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/output_high.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(target_low)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/target_low.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(output_low)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"./script/output_low.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
